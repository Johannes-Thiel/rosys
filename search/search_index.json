{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RoSys - The Robot System \u00b6 RoSys provides an easy-to-use robot system. Its purpose is similar to ROS . But RoSys is fully based on modern web technologies and focusses on mobile robotics. The full documentation is available at rosys.io . Principles \u00b6 All Python \u00b6 Python is great to write business logic. Computation-heavy tasks are wrapped in processes, accessed through WebSockets or called via C++ bindings. Like you would do in any other Python program. Modularity \u00b6 You can structure your code as you please. RoSys provides its magic without assuming a specific file structure, configuration files or enforced naming. Event Loop \u00b6 Thanks to asyncio you can write your business logic without locks and mutexes. The execution is parallel but not concurrent which makes it easier to read, write and debug. In real-case scenarios this is also much faster than ROS . Its multiprocessing architecture requires too much inter-process communication. Web UI \u00b6 Most machines need some kind of human interaction. RoSys is built from the ground up to make sure your robot can be operated fully off the grid with any web browser. This is done by incorporating NiceGUI , a wonderful all-Python UI web framework. It is also possible to proxy the user interface through a gateway for remote operation. Simulation \u00b6 Robot hardware is often slower than your own computer. To rapidly test out new behavior and algorithms, RoSys provides a simulation mode. Here, all hardware is mocked and can even be manipulated to test wheel blockages and similar. Testing \u00b6 You can use pytest to write high-level integration tests. It is based on the above-described simulation mode and accelerates the robot's time for super fast execution. Architecture and Features \u00b6 Modules \u00b6 RoSys modules are just Python modules which encapsulate certain functionality. They can hold their own state, register lifecycle hooks, run methods repeatedly and subscribe to or raise events . Modules can depend on other modules which is mostly implemented by passing them into the constructor. Lifecycle Hooks and Loops \u00b6 Modules can register functions for being called on_startup or on_shutdown as well as repeatedly with a given interval with on_repeat . Events \u00b6 Modules can provide events to allow connecting otherwise separated modules of the system. For example, one module might read sensor data and raise an event NEW_SENSOR_DATA , without knowing of any consumers. Another module can register on NEW_SENSOR_DATA and act accordingly when being called. Automations \u00b6 RoSys provides an Automator module for running \"automations\". Automations are coroutines that can not only be started and stopped, but also paused and resumed, e.g. using AutomationControls . Have a look at our Click-and-drive example. Persistence \u00b6 Modules can register backup and restore methods to read and write their state to disk. Time \u00b6 RoSys uses its own time which is accessible through rosys.time . This way the time can advance much faster in simulation and tests if no CPU-intensive operation is performed. To delay the execution of a coroutine, you should invoke await rosys.sleep(seconds: float) . This creates a delay until the provided amount of RoSys time has elapsed. Threading and Multiprocessing \u00b6 RoSys makes extensive use of async/await to achieve parallelism without threading or multiprocessing. But not every piece of code you want to integrate is offering an asyncio interface. Therefore RoSys provides two handy wrappers: IO-bound: If you need to read from an external device or use a non-async HTTP library like requests , you should wrap the code in a function and await it with await rosys.run.io_bound(...) . CPU-bound: If you need to do some heavy computation and want to spawn another process, you should wrap the code in a function and await it with await rosys.run.cpu_bound(...) . Safety \u00b6 Python (and Linux) is fast enough for most high-level logic, but has no realtime guarantees. Safety-relevant behavior should therefore be put on a suitable microcontroller. It governs the hardware of the robot and must be able to perform safety actions like triggering emergency hold etc. We suggest to use an industrial PC with an integrated controller like the Zauberzeug Robot Brain . It provides a Linux system to run RoSys, offers AI acceleration via NVidia Jetson, two integrated ESP32 microcontrollers and six I/O sockets with up to 24 GPIOs for digital I/Os, CAN, RS485, SPI, I2C, etc. It also has two hardware ENABLE switches and one which is controllable via software. To have flexible configuration for the microcontroller we created another open source project called Lizard . It is a domain-specific language interpreted by the microcontroller which enables you to write reactive hardware behavior without recompiling and flashing. User Interface \u00b6 RoSys builds upon the open source project NiceGUI and offers many robot-related UI elements. NiceGUI is a high-level UI framework for the web. This means you can write all UI code in Python and the state is automatically reflected in the browser through WebSockets. See any of our examples . RoSys can also be used with other user interfaces or interaction models if required, for example a completely app-based control through Bluetooth Low Energy with Flutter. Notifications \u00b6 Modules can notify the user through rosys.notify('message to the user') . When using NiceGUI, the notifications will show as snackbar messages. The history of notifications is stored in the list rosys.notifications .","title":"About"},{"location":"#rosys-the-robot-system","text":"RoSys provides an easy-to-use robot system. Its purpose is similar to ROS . But RoSys is fully based on modern web technologies and focusses on mobile robotics. The full documentation is available at rosys.io .","title":"RoSys - The Robot System"},{"location":"#principles","text":"","title":"Principles"},{"location":"#all-python","text":"Python is great to write business logic. Computation-heavy tasks are wrapped in processes, accessed through WebSockets or called via C++ bindings. Like you would do in any other Python program.","title":"All Python"},{"location":"#modularity","text":"You can structure your code as you please. RoSys provides its magic without assuming a specific file structure, configuration files or enforced naming.","title":"Modularity"},{"location":"#event-loop","text":"Thanks to asyncio you can write your business logic without locks and mutexes. The execution is parallel but not concurrent which makes it easier to read, write and debug. In real-case scenarios this is also much faster than ROS . Its multiprocessing architecture requires too much inter-process communication.","title":"Event Loop"},{"location":"#web-ui","text":"Most machines need some kind of human interaction. RoSys is built from the ground up to make sure your robot can be operated fully off the grid with any web browser. This is done by incorporating NiceGUI , a wonderful all-Python UI web framework. It is also possible to proxy the user interface through a gateway for remote operation.","title":"Web UI"},{"location":"#simulation","text":"Robot hardware is often slower than your own computer. To rapidly test out new behavior and algorithms, RoSys provides a simulation mode. Here, all hardware is mocked and can even be manipulated to test wheel blockages and similar.","title":"Simulation"},{"location":"#testing","text":"You can use pytest to write high-level integration tests. It is based on the above-described simulation mode and accelerates the robot's time for super fast execution.","title":"Testing"},{"location":"#architecture-and-features","text":"","title":"Architecture and Features"},{"location":"#modules","text":"RoSys modules are just Python modules which encapsulate certain functionality. They can hold their own state, register lifecycle hooks, run methods repeatedly and subscribe to or raise events . Modules can depend on other modules which is mostly implemented by passing them into the constructor.","title":"Modules"},{"location":"#lifecycle-hooks-and-loops","text":"Modules can register functions for being called on_startup or on_shutdown as well as repeatedly with a given interval with on_repeat .","title":"Lifecycle Hooks and Loops"},{"location":"#events","text":"Modules can provide events to allow connecting otherwise separated modules of the system. For example, one module might read sensor data and raise an event NEW_SENSOR_DATA , without knowing of any consumers. Another module can register on NEW_SENSOR_DATA and act accordingly when being called.","title":"Events"},{"location":"#automations","text":"RoSys provides an Automator module for running \"automations\". Automations are coroutines that can not only be started and stopped, but also paused and resumed, e.g. using AutomationControls . Have a look at our Click-and-drive example.","title":"Automations"},{"location":"#persistence","text":"Modules can register backup and restore methods to read and write their state to disk.","title":"Persistence"},{"location":"#time","text":"RoSys uses its own time which is accessible through rosys.time . This way the time can advance much faster in simulation and tests if no CPU-intensive operation is performed. To delay the execution of a coroutine, you should invoke await rosys.sleep(seconds: float) . This creates a delay until the provided amount of RoSys time has elapsed.","title":"Time"},{"location":"#threading-and-multiprocessing","text":"RoSys makes extensive use of async/await to achieve parallelism without threading or multiprocessing. But not every piece of code you want to integrate is offering an asyncio interface. Therefore RoSys provides two handy wrappers: IO-bound: If you need to read from an external device or use a non-async HTTP library like requests , you should wrap the code in a function and await it with await rosys.run.io_bound(...) . CPU-bound: If you need to do some heavy computation and want to spawn another process, you should wrap the code in a function and await it with await rosys.run.cpu_bound(...) .","title":"Threading and Multiprocessing"},{"location":"#safety","text":"Python (and Linux) is fast enough for most high-level logic, but has no realtime guarantees. Safety-relevant behavior should therefore be put on a suitable microcontroller. It governs the hardware of the robot and must be able to perform safety actions like triggering emergency hold etc. We suggest to use an industrial PC with an integrated controller like the Zauberzeug Robot Brain . It provides a Linux system to run RoSys, offers AI acceleration via NVidia Jetson, two integrated ESP32 microcontrollers and six I/O sockets with up to 24 GPIOs for digital I/Os, CAN, RS485, SPI, I2C, etc. It also has two hardware ENABLE switches and one which is controllable via software. To have flexible configuration for the microcontroller we created another open source project called Lizard . It is a domain-specific language interpreted by the microcontroller which enables you to write reactive hardware behavior without recompiling and flashing.","title":"Safety"},{"location":"#user-interface","text":"RoSys builds upon the open source project NiceGUI and offers many robot-related UI elements. NiceGUI is a high-level UI framework for the web. This means you can write all UI code in Python and the state is automatically reflected in the browser through WebSockets. See any of our examples . RoSys can also be used with other user interfaces or interaction models if required, for example a completely app-based control through Bluetooth Low Energy with Flutter.","title":"User Interface"},{"location":"#notifications","text":"Modules can notify the user through rosys.notify('message to the user') . When using NiceGUI, the notifications will show as snackbar messages. The history of notifications is stored in the list rosys.notifications .","title":"Notifications"},{"location":"development/","text":"Development \u00b6 Pushing Code to the Robot \u00b6 To get the code onto the robot you can simply pull your repository. But this requires you to have login credentials on an external machine. And editing files must be done on slow hardware compared to development workstations and laptops. If you use VS Code Remote Development or similar to do actual development on these slow systems, everything feels like jelly. Especially if you run powerful extensions like Pylance . That is why we at Zauberzeug created a small open source tool called LiveSync . It combines a local filesystem watcher with rsync to copy changes to a (slow) remote target whenever your local code changes. This approach has multiple advantages: work with your personal choice of IDE and tooling run tests (or simulate the production code) locally continuously deploy the development code to the target environment (where auto-reload ensures live preview) almost no overhead on the (slow) target Logging \u00b6 RoSys uses the Python logging package with namespaced loggers. For example, the steerer module writes its logs as rosys.steerer . This can be used for fine-granular control of what should show on the console. As a general starting point we suggest reading the Python Logging HOWTO . In the following examples we use Python's logging dictConfig for configuration, because it provides the most flexibility while having all configuration in one place. Show Info Messages \u00b6 To only print RoSys messages at the info level to the console we can use a configuration like this: #!/usr/bin/env python3 import logging import logging.config from nicegui import ui from rosys.driving import Odometer , Steerer , joystick from rosys.hardware import WheelsSimulation logging . config . dictConfig ({ 'version' : 1 , 'disable_existing_loggers' : True , # to make sure this config is used 'formatters' : { 'default' : { 'format' : ' %(asctime)s - %(levelname)s - %(message)s ' , 'datefmt' : '%Y-%m- %d %H:%M:%S' , }, }, 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, }, 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' ], 'level' : 'INFO' , 'propagate' : False , }, }, }) wheels = WheelsSimulation () steerer = Steerer ( wheels ) odometer = Odometer ( wheels ) joystick ( steerer ) ui . run ( title = 'RoSys' ) As you move the joystick, rosys.steerer messages will appear on the console: 2022-01-11 06:53:21 - INFO - start steering 2022-01-11 06:53:22 - INFO - stop steering 2022-01-11 06:53:23 - INFO - start steering 2022-01-11 06:53:23 - INFO - stop steering Adding Loggers \u00b6 You can easily add more loggers. For example, to see debug messages of the odometer you can add 'rosys.odometer' : { 'handlers' : [ 'console' ], 'level' : 'DEBUG' , 'propagate' : False , }, Most of the time we turn off log propagation to ensure the configuration we defined ourselves is really used. Logging to File \u00b6 Sometimes it is helpful to write intensive logging into a file and only show some messages on the console. For this you can add a file handler : 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, 'file' : { 'level' : 'DEBUG' , 'class' : 'logging.handlers.RotatingFileHandler' , 'formatter' : 'default' , 'filename' : os . path . expanduser ( '~/.rosys/example.log' ), 'maxBytes' : 1024 * 1000 , 'backupCount' : 3 } }, Then you can decide for each logger which handlers should be used: 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' , 'file' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' , 'file' ], 'level' : 'INFO' , 'propagate' : False , }, 'rosys.event' : { 'handlers' : [ 'file' ], 'level' : 'DEBUG' , 'propagate' : False , }, 'rosys.core' : { 'handlers' : [ 'file' ], 'level' : 'DEBUG' , 'propagate' : False , }, }, Note The above file logger writes to ~/.rosys . For development it is very helpful to have auto-reloading on file change activated . Therefore logging should always be stored outside of your project's source directory. Formatting \u00b6 It is quite useful to see from which file and line number a log entry was triggered. To keep the log lines from getting too long, you can create a log filter which computes the relative path: class PackagePathFilter ( logging . Filter ): '''Provides relative path for log formatter. Original code borrowed from https://stackoverflow.com/a/52582536/3419103 ''' def filter ( self , record : logging . LogRecord ) -> bool : pathname = record . pathname record . relative_path = None abs_sys_paths = map ( os . path . abspath , sys . path ) for path in sorted ( abs_sys_paths , key = len , reverse = True ): # longer paths first if not path . endswith ( os . sep ): path += os . sep if pathname . startswith ( path ): record . relative_path = os . path . relpath ( pathname , path ) break return True You need to register the filter and apply it in the handler. Then you can change the format for the formatter: 'filters' : { 'package_path_filter' : { '()' : PackagePathFilter , }, }, 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'filters' : [ 'package_path_filter' ], 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, }, 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' ], 'level' : 'INFO' , 'propagate' : False , }, }, Log output then looks like this: 2022-01-11 06:51:00.319 [DEBUG] rosys/runtime.py:78: startup completed Profiling \u00b6 Note The default RoSys installation via pip does not come with profiling packages. To install them, run python3 -m pip install rosys [ profiling ] Currently this does not work with Python 3.11 because yappy and line-profiler do not support 3.11 yet. You can add a profile decorator to expensive functions and add a profiler button to your UI: #!/usr/bin/env python3 import rosys from nicegui import ui from rosys.analysis import profile_button , profiling @profiling . profile def compute () -> None : s = 0 for i in range ( 1_000_000 ): s += i ** 2 ui . notify ( s ) rosys . on_repeat ( compute , 1.0 ) profile_button () ui . run () When the button is pressed, the profiler yappi will start recording data. When stopped, you will see its output on the console: Line # Hits Time Per Hit % Time Line Contents ============================================================== 7 @profiling.profile 8 def compute() -> None: 9 3 21.0 7.0 0.0 s = 0 10 3000003 433138.0 0.1 28.2 for i in range(1_000_000): 11 3000000 1098975.0 0.4 71.6 s += i**2 12 3 2151.0 717.0 0.1 ui.notify(s) Track async function calls \u00b6 RoSys provides a @track decorator that you can put above asynchronous functions that are called as part of automations. The UI element track.ui() will show the stack of functions that are currently awaited. #!/usr/bin/env python3 import asyncio from nicegui import ui from rosys.analysis import track @track async def do_A (): await asyncio . sleep ( 1 ) @track async def do_B (): await asyncio . sleep ( 1 ) @track async def do_something (): await asyncio . sleep ( 1 ) for _ in range ( 3 ): await do_A () await do_B () ui . button ( 'Do something' , on_click = do_something ) track . ui () ui . run () Continuous Build \u00b6 We run our continuous integration with GitHub Actions. For each commit the pytests are executed. Releases \u00b6 We publish releases by creating a new version on GitHub and describe the changes. A GitHub Action then performs the following steps: If the pytests are successful, a poetry build and deployment to pypi is issued. A multi-arch Docker image is built and pushed to Docker Hub .","title":"Development"},{"location":"development/#development","text":"","title":"Development"},{"location":"development/#pushing-code-to-the-robot","text":"To get the code onto the robot you can simply pull your repository. But this requires you to have login credentials on an external machine. And editing files must be done on slow hardware compared to development workstations and laptops. If you use VS Code Remote Development or similar to do actual development on these slow systems, everything feels like jelly. Especially if you run powerful extensions like Pylance . That is why we at Zauberzeug created a small open source tool called LiveSync . It combines a local filesystem watcher with rsync to copy changes to a (slow) remote target whenever your local code changes. This approach has multiple advantages: work with your personal choice of IDE and tooling run tests (or simulate the production code) locally continuously deploy the development code to the target environment (where auto-reload ensures live preview) almost no overhead on the (slow) target","title":"Pushing Code to the Robot"},{"location":"development/#logging","text":"RoSys uses the Python logging package with namespaced loggers. For example, the steerer module writes its logs as rosys.steerer . This can be used for fine-granular control of what should show on the console. As a general starting point we suggest reading the Python Logging HOWTO . In the following examples we use Python's logging dictConfig for configuration, because it provides the most flexibility while having all configuration in one place.","title":"Logging"},{"location":"development/#show-info-messages","text":"To only print RoSys messages at the info level to the console we can use a configuration like this: #!/usr/bin/env python3 import logging import logging.config from nicegui import ui from rosys.driving import Odometer , Steerer , joystick from rosys.hardware import WheelsSimulation logging . config . dictConfig ({ 'version' : 1 , 'disable_existing_loggers' : True , # to make sure this config is used 'formatters' : { 'default' : { 'format' : ' %(asctime)s - %(levelname)s - %(message)s ' , 'datefmt' : '%Y-%m- %d %H:%M:%S' , }, }, 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, }, 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' ], 'level' : 'INFO' , 'propagate' : False , }, }, }) wheels = WheelsSimulation () steerer = Steerer ( wheels ) odometer = Odometer ( wheels ) joystick ( steerer ) ui . run ( title = 'RoSys' ) As you move the joystick, rosys.steerer messages will appear on the console: 2022-01-11 06:53:21 - INFO - start steering 2022-01-11 06:53:22 - INFO - stop steering 2022-01-11 06:53:23 - INFO - start steering 2022-01-11 06:53:23 - INFO - stop steering","title":"Show Info Messages"},{"location":"development/#adding-loggers","text":"You can easily add more loggers. For example, to see debug messages of the odometer you can add 'rosys.odometer' : { 'handlers' : [ 'console' ], 'level' : 'DEBUG' , 'propagate' : False , }, Most of the time we turn off log propagation to ensure the configuration we defined ourselves is really used.","title":"Adding Loggers"},{"location":"development/#logging-to-file","text":"Sometimes it is helpful to write intensive logging into a file and only show some messages on the console. For this you can add a file handler : 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, 'file' : { 'level' : 'DEBUG' , 'class' : 'logging.handlers.RotatingFileHandler' , 'formatter' : 'default' , 'filename' : os . path . expanduser ( '~/.rosys/example.log' ), 'maxBytes' : 1024 * 1000 , 'backupCount' : 3 } }, Then you can decide for each logger which handlers should be used: 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' , 'file' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' , 'file' ], 'level' : 'INFO' , 'propagate' : False , }, 'rosys.event' : { 'handlers' : [ 'file' ], 'level' : 'DEBUG' , 'propagate' : False , }, 'rosys.core' : { 'handlers' : [ 'file' ], 'level' : 'DEBUG' , 'propagate' : False , }, }, Note The above file logger writes to ~/.rosys . For development it is very helpful to have auto-reloading on file change activated . Therefore logging should always be stored outside of your project's source directory.","title":"Logging to File"},{"location":"development/#formatting","text":"It is quite useful to see from which file and line number a log entry was triggered. To keep the log lines from getting too long, you can create a log filter which computes the relative path: class PackagePathFilter ( logging . Filter ): '''Provides relative path for log formatter. Original code borrowed from https://stackoverflow.com/a/52582536/3419103 ''' def filter ( self , record : logging . LogRecord ) -> bool : pathname = record . pathname record . relative_path = None abs_sys_paths = map ( os . path . abspath , sys . path ) for path in sorted ( abs_sys_paths , key = len , reverse = True ): # longer paths first if not path . endswith ( os . sep ): path += os . sep if pathname . startswith ( path ): record . relative_path = os . path . relpath ( pathname , path ) break return True You need to register the filter and apply it in the handler. Then you can change the format for the formatter: 'filters' : { 'package_path_filter' : { '()' : PackagePathFilter , }, }, 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'filters' : [ 'package_path_filter' ], 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, }, 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' ], 'level' : 'INFO' , 'propagate' : False , }, }, Log output then looks like this: 2022-01-11 06:51:00.319 [DEBUG] rosys/runtime.py:78: startup completed","title":"Formatting"},{"location":"development/#profiling","text":"Note The default RoSys installation via pip does not come with profiling packages. To install them, run python3 -m pip install rosys [ profiling ] Currently this does not work with Python 3.11 because yappy and line-profiler do not support 3.11 yet. You can add a profile decorator to expensive functions and add a profiler button to your UI: #!/usr/bin/env python3 import rosys from nicegui import ui from rosys.analysis import profile_button , profiling @profiling . profile def compute () -> None : s = 0 for i in range ( 1_000_000 ): s += i ** 2 ui . notify ( s ) rosys . on_repeat ( compute , 1.0 ) profile_button () ui . run () When the button is pressed, the profiler yappi will start recording data. When stopped, you will see its output on the console: Line # Hits Time Per Hit % Time Line Contents ============================================================== 7 @profiling.profile 8 def compute() -> None: 9 3 21.0 7.0 0.0 s = 0 10 3000003 433138.0 0.1 28.2 for i in range(1_000_000): 11 3000000 1098975.0 0.4 71.6 s += i**2 12 3 2151.0 717.0 0.1 ui.notify(s)","title":"Profiling"},{"location":"development/#track-async-function-calls","text":"RoSys provides a @track decorator that you can put above asynchronous functions that are called as part of automations. The UI element track.ui() will show the stack of functions that are currently awaited. #!/usr/bin/env python3 import asyncio from nicegui import ui from rosys.analysis import track @track async def do_A (): await asyncio . sleep ( 1 ) @track async def do_B (): await asyncio . sleep ( 1 ) @track async def do_something (): await asyncio . sleep ( 1 ) for _ in range ( 3 ): await do_A () await do_B () ui . button ( 'Do something' , on_click = do_something ) track . ui () ui . run ()","title":"Track async function calls"},{"location":"development/#continuous-build","text":"We run our continuous integration with GitHub Actions. For each commit the pytests are executed.","title":"Continuous Build"},{"location":"development/#releases","text":"We publish releases by creating a new version on GitHub and describe the changes. A GitHub Action then performs the following steps: If the pytests are successful, a poetry build and deployment to pypi is issued. A multi-arch Docker image is built and pushed to Docker Hub .","title":"Releases"},{"location":"getting_started/","text":"Getting Started \u00b6 First install RoSys with pip or Docker. Then create a directory to host your code and put it under version control. Name your entry file main.py and add the following content: #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Odometer , Steerer , keyboard_control , robot_object from rosys.geometry import Prism from rosys.hardware import WheelsSimulation # setup shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) # ui keyboard_control ( steerer ) with ui . scene (): robot_object ( shape , odometer ) ui . label ( 'hold SHIFT to steer with the keyboard arrow keys' ) # start ui . run ( title = 'RoSys' ) If you launch the program, your browser will open the url http://0.0.0.0:8080/ and present a 3d view: Explanation \u00b6 Imports \u00b6 The user interface is built with NiceGUI . The individual RoSys modules come in packages driving , geometry , hardware and others. Setup \u00b6 In this example we create a Steerer which needs an Odometer . Here we work without real hardware, so two wheels are simulated. Please see Hardware for an example which can actually be used on a mobile robot. For visualization purposes we also need the approximate robot shape. User Interface \u00b6 The user interface consists of keyboard control with access to the steerer as well as a 3D view of the scene. The latter only contains the RobotObject with the given shape. The robot pose is constantly updated from the odometer. See NiceGUI for more details about its API. Start \u00b6 NiceGUI provides a ui.run command which launches the web server and opens the corresponding web application. If you modify the code, a reload is triggered automatically. This is very convenient, but can be deactivated by passing reload=False .","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"First install RoSys with pip or Docker. Then create a directory to host your code and put it under version control. Name your entry file main.py and add the following content: #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Odometer , Steerer , keyboard_control , robot_object from rosys.geometry import Prism from rosys.hardware import WheelsSimulation # setup shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) # ui keyboard_control ( steerer ) with ui . scene (): robot_object ( shape , odometer ) ui . label ( 'hold SHIFT to steer with the keyboard arrow keys' ) # start ui . run ( title = 'RoSys' ) If you launch the program, your browser will open the url http://0.0.0.0:8080/ and present a 3d view:","title":"Getting Started"},{"location":"getting_started/#explanation","text":"","title":"Explanation"},{"location":"getting_started/#imports","text":"The user interface is built with NiceGUI . The individual RoSys modules come in packages driving , geometry , hardware and others.","title":"Imports"},{"location":"getting_started/#setup","text":"In this example we create a Steerer which needs an Odometer . Here we work without real hardware, so two wheels are simulated. Please see Hardware for an example which can actually be used on a mobile robot. For visualization purposes we also need the approximate robot shape.","title":"Setup"},{"location":"getting_started/#user-interface","text":"The user interface consists of keyboard control with access to the steerer as well as a 3D view of the scene. The latter only contains the RobotObject with the given shape. The robot pose is constantly updated from the odometer. See NiceGUI for more details about its API.","title":"User Interface"},{"location":"getting_started/#start","text":"NiceGUI provides a ui.run command which launches the web server and opens the corresponding web application. If you modify the code, a reload is triggered automatically. This is very convenient, but can be deactivated by passing reload=False .","title":"Start"},{"location":"installation/","text":"Installation \u00b6 On Your Computer \u00b6 python3 -m pip install rosys See Getting Started for what to do next. On The Robot \u00b6 While the above-mentioned installation command works perfectly well in local environments, on a robot it is often easier to run RoSys inside a Docker container. If you already have a main.py , it can be as simple as running docker run -it --rm -v `pwd`:/app zauberzeug/rosys from the same directory. See Pushing Code to Robot on how to get your project onto the remote system. More complex Docker setups benefit from a compose file. There are also some specialties needed to start RoSys in different environments (Mac, Linux, NVidia Jetson, ...). To simplify the usage we suggest to use a script called ./docker.sh which you can also copy and adapt in your own project. Have a look at the project examples to see how a setup of your own repository may look like.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#on-your-computer","text":"python3 -m pip install rosys See Getting Started for what to do next.","title":"On Your Computer"},{"location":"installation/#on-the-robot","text":"While the above-mentioned installation command works perfectly well in local environments, on a robot it is often easier to run RoSys inside a Docker container. If you already have a main.py , it can be as simple as running docker run -it --rm -v `pwd`:/app zauberzeug/rosys from the same directory. See Pushing Code to Robot on how to get your project onto the remote system. More complex Docker setups benefit from a compose file. There are also some specialties needed to start RoSys in different environments (Mac, Linux, NVidia Jetson, ...). To simplify the usage we suggest to use a script called ./docker.sh which you can also copy and adapt in your own project. Have a look at the project examples to see how a setup of your own repository may look like.","title":"On The Robot"},{"location":"troubleshooting/","text":"Troubleshooting \u00b6 Asyncio Warning \u00b6 While running RoSys you may see warnings similar to this one: 2021-10-31 15:08:04.040 [WARNING] asyncio: Executing <Task pending name='Task-255' coro=<handle_event() running at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:344> wait_for=<_GatheringFuture pending cb=[<TaskWakeupMethWrapper object at 0x7f7001f8e0>()] created at /usr/local/lib/python3.9/asyncio/tasks.py:705> created at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:261> took 0.238 seconds This means some coroutine is clogging the event loop for too long. In the above example it is a whopping 238 ms in which no other actor can do anything. This is an eternity when machine communication is expected to happen about every 10 ms. The warning also provides a (not so readable) hint where the time is consumed. The example above is one of the more frequent scenarios. It means some code inside a user interaction event handler (e.g. handle_event() in justpy.py ) is blocking. Try to figure out which UI event code is responsible by commenting out parts of your logic and try to reproduce the warning systematically.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"troubleshooting/#asyncio-warning","text":"While running RoSys you may see warnings similar to this one: 2021-10-31 15:08:04.040 [WARNING] asyncio: Executing <Task pending name='Task-255' coro=<handle_event() running at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:344> wait_for=<_GatheringFuture pending cb=[<TaskWakeupMethWrapper object at 0x7f7001f8e0>()] created at /usr/local/lib/python3.9/asyncio/tasks.py:705> created at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:261> took 0.238 seconds This means some coroutine is clogging the event loop for too long. In the above example it is a whopping 238 ms in which no other actor can do anything. This is an eternity when machine communication is expected to happen about every 10 ms. The warning also provides a (not so readable) hint where the time is consumed. The example above is one of the more frequent scenarios. It means some code inside a user interaction event handler (e.g. handle_event() in justpy.py ) is blocking. Try to figure out which UI event code is responsible by commenting out parts of your logic and try to reproduce the warning systematically.","title":"Asyncio Warning"},{"location":"examples/cameras/","text":"Cameras \u00b6 RoSys provides instant camera access for object detection, remote operation and similar use cases. Setup \u00b6 USB camera devices are discovered through video4linux (v4l) and accessed with openCV. Therefore the program v4l2ctl and openCV (including python bindings) must be available. We recommend to use the RoSys Docker image which provides the full required software stack. Make sure the container can access the USB devices by starting it with --privileged or explicitly passing the specific --device s. Show Captured Images \u00b6 Using rosys.ui you can show the latest captured images from each camera: #!/usr/bin/env python3 from nicegui import ui from rosys.vision import UsbCameraProviderSimulation , camera_provider camera_provider = UsbCameraProviderSimulation () camera_provider . add_camera ( camera_provider . create_calibrated ( 'test_cam' , width = 800 , height = 600 )) def refresh () -> None : for uid , camera in camera_provider . cameras . items (): if uid not in feeds : feeds [ uid ] = ui . image () feeds [ uid ] . set_source ( camera_provider . get_latest_image_url ( camera )) feeds = {} ui . timer ( 0.3 , refresh ) ui . run ( title = 'RoSys' ) The ui.timer regularly updates the source property of the ui.image . The cameras latest_image_uri property provides the URI to the latest captured image. This example uses a UsbCameraProviderSimulation with a single simulated test camera. But you can replace the provider with a UsbCameraProviderHardware . Remote Operation \u00b6 A fairly often required use case on real mobile robots is the remote operation. In a simple use case you may only need to visualize one camera and have some steering controls. Here we use the NEW_CAMERA event to display the first camera to control real Hardware : #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Odometer , Steerer , joystick , keyboard_control from rosys.hardware import RobotBrain , SerialCommunication , WheelsHardware , WheelsSimulation from rosys.vision import Camera , UsbCameraProviderHardware , UsbCameraProviderSimulation if SerialCommunication . is_possible (): communication = SerialCommunication () robot_brain = RobotBrain ( communication ) wheels = WheelsHardware ( robot_brain ) camera_provider = UsbCameraProviderHardware () else : wheels = WheelsSimulation () camera_provider = UsbCameraProviderSimulation () camera_provider . restore = lambda _ : None # NOTE: disable persistence test_cam = camera_provider . create_calibrated ( 'test_cam' , width = 800 , height = 600 ) ui . on_startup ( lambda : camera_provider . add_camera ( test_cam )) steerer = Steerer ( wheels ) odometer = Odometer ( wheels ) async def add_main_camera ( camera : Camera ) -> None : camera_card . clear () # remove \"seeking camera\" label with camera_card : maincam = ui . image () ui . timer ( 1 , lambda : maincam . set_source ( camera_provider . get_latest_image_url ( camera ))) camera_provider . CAMERA_ADDED . register ( add_main_camera ) with ui . card () . tight () . style ( 'width:30em' ) as camera_card : ui . label ( 'seeking main camera' ) . classes ( 'm-8 text-center' ) with ui . card () . tight () . style ( 'width:30em' ): with ui . row (): with ui . card () . tight (): joystick ( steerer ) keyboard_control ( steerer ) ui . markdown ( 'steer with joystick on the left or<br />SHIFT + arrow keys' ) . classes ( 'm-8 text-center' ) ui . run ( title = 'RoSys' ) By adding a Joystick and KeyboardControl the robot is ready to go for remote operation.","title":"Cameras"},{"location":"examples/cameras/#cameras","text":"RoSys provides instant camera access for object detection, remote operation and similar use cases.","title":"Cameras"},{"location":"examples/cameras/#setup","text":"USB camera devices are discovered through video4linux (v4l) and accessed with openCV. Therefore the program v4l2ctl and openCV (including python bindings) must be available. We recommend to use the RoSys Docker image which provides the full required software stack. Make sure the container can access the USB devices by starting it with --privileged or explicitly passing the specific --device s.","title":"Setup"},{"location":"examples/cameras/#show-captured-images","text":"Using rosys.ui you can show the latest captured images from each camera: #!/usr/bin/env python3 from nicegui import ui from rosys.vision import UsbCameraProviderSimulation , camera_provider camera_provider = UsbCameraProviderSimulation () camera_provider . add_camera ( camera_provider . create_calibrated ( 'test_cam' , width = 800 , height = 600 )) def refresh () -> None : for uid , camera in camera_provider . cameras . items (): if uid not in feeds : feeds [ uid ] = ui . image () feeds [ uid ] . set_source ( camera_provider . get_latest_image_url ( camera )) feeds = {} ui . timer ( 0.3 , refresh ) ui . run ( title = 'RoSys' ) The ui.timer regularly updates the source property of the ui.image . The cameras latest_image_uri property provides the URI to the latest captured image. This example uses a UsbCameraProviderSimulation with a single simulated test camera. But you can replace the provider with a UsbCameraProviderHardware .","title":"Show Captured Images"},{"location":"examples/cameras/#remote-operation","text":"A fairly often required use case on real mobile robots is the remote operation. In a simple use case you may only need to visualize one camera and have some steering controls. Here we use the NEW_CAMERA event to display the first camera to control real Hardware : #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Odometer , Steerer , joystick , keyboard_control from rosys.hardware import RobotBrain , SerialCommunication , WheelsHardware , WheelsSimulation from rosys.vision import Camera , UsbCameraProviderHardware , UsbCameraProviderSimulation if SerialCommunication . is_possible (): communication = SerialCommunication () robot_brain = RobotBrain ( communication ) wheels = WheelsHardware ( robot_brain ) camera_provider = UsbCameraProviderHardware () else : wheels = WheelsSimulation () camera_provider = UsbCameraProviderSimulation () camera_provider . restore = lambda _ : None # NOTE: disable persistence test_cam = camera_provider . create_calibrated ( 'test_cam' , width = 800 , height = 600 ) ui . on_startup ( lambda : camera_provider . add_camera ( test_cam )) steerer = Steerer ( wheels ) odometer = Odometer ( wheels ) async def add_main_camera ( camera : Camera ) -> None : camera_card . clear () # remove \"seeking camera\" label with camera_card : maincam = ui . image () ui . timer ( 1 , lambda : maincam . set_source ( camera_provider . get_latest_image_url ( camera ))) camera_provider . CAMERA_ADDED . register ( add_main_camera ) with ui . card () . tight () . style ( 'width:30em' ) as camera_card : ui . label ( 'seeking main camera' ) . classes ( 'm-8 text-center' ) with ui . card () . tight () . style ( 'width:30em' ): with ui . row (): with ui . card () . tight (): joystick ( steerer ) keyboard_control ( steerer ) ui . markdown ( 'steer with joystick on the left or<br />SHIFT + arrow keys' ) . classes ( 'm-8 text-center' ) ui . run ( title = 'RoSys' ) By adding a Joystick and KeyboardControl the robot is ready to go for remote operation.","title":"Remote Operation"},{"location":"examples/click-and-drive/","text":"Click-and-drive \u00b6 In this example we create a simulated robot which uses an automation to drive wherever the user clicks. #!/usr/bin/env python3 from nicegui import ui from rosys.automation import Automator , automation_controls from rosys.driving import Driver , Odometer , robot_object from rosys.geometry import Point , Prism from rosys.hardware import WheelsSimulation shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) driver = Driver ( wheels , odometer ) automator = Automator ( wheels , None ) async def handle_click ( msg ): for hit in msg . hits : if hit . object_id == 'ground' : target = Point ( x = hit . point . x , y = hit . point . y ) automator . start ( driver . drive_to ( target )) with ui . scene ( on_click = handle_click ): robot_object ( shape , odometer , debug = True ) ui . label ( 'click into the scene to drive the robot' ) with ui . row (): automation_controls ( automator ) ui . label ( 'you can also pause/resume or stop the running automation' ) ui . run ( title = 'RoSys' ) Modules Besides wheels, odometer and a robot shape we need a driver that enables the robot to drive along a given path as well as an automator to start and stop such an automated behavior. Click handler NiceGUI's 3D scene allows registering a click handler that can iterate through all hit points and find the target on the ground. Driver Among others, the driver has an async method drive_to which lets the robot follow a straight line to a given target. Automator and automation controls The automator starts the async method and allows pausing, resuming and stopping it, e.g. with the AutomationControls UI element.","title":"Click-and-drive"},{"location":"examples/click-and-drive/#click-and-drive","text":"In this example we create a simulated robot which uses an automation to drive wherever the user clicks. #!/usr/bin/env python3 from nicegui import ui from rosys.automation import Automator , automation_controls from rosys.driving import Driver , Odometer , robot_object from rosys.geometry import Point , Prism from rosys.hardware import WheelsSimulation shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) driver = Driver ( wheels , odometer ) automator = Automator ( wheels , None ) async def handle_click ( msg ): for hit in msg . hits : if hit . object_id == 'ground' : target = Point ( x = hit . point . x , y = hit . point . y ) automator . start ( driver . drive_to ( target )) with ui . scene ( on_click = handle_click ): robot_object ( shape , odometer , debug = True ) ui . label ( 'click into the scene to drive the robot' ) with ui . row (): automation_controls ( automator ) ui . label ( 'you can also pause/resume or stop the running automation' ) ui . run ( title = 'RoSys' ) Modules Besides wheels, odometer and a robot shape we need a driver that enables the robot to drive along a given path as well as an automator to start and stop such an automated behavior. Click handler NiceGUI's 3D scene allows registering a click handler that can iterate through all hit points and find the target on the ground. Driver Among others, the driver has an async method drive_to which lets the robot follow a straight line to a given target. Automator and automation controls The automator starts the async method and allows pausing, resuming and stopping it, e.g. with the AutomationControls UI element.","title":"Click-and-drive"},{"location":"examples/hardware/","text":"Hardware \u00b6 The other examples use simulated hardware for simplicity and easy execution on any development system. To be able to control real hardware we recommend to derive a Simulation and Hardware version from a shared interface. Depending on your environment you can then instantiate the correct implementation without bothering with it in the rest of your code. Custom Implementation \u00b6 For a differential-steering controlled robot, RoSys offers a Wheels base class plus a WheelsSimulation . The following example illustrates the content of wheels_for_custom_hardware.py : import rosys import rosys.geometry import rosys.hardware class WheelsForCustomHardware ( rosys . hardware . Wheels ): def __init__ ( self ) -> None : super () . __init__ () # TODO raise exception if hardware is not available # update measured velocities regularly to feed the odometer rosys . on_repeat ( self . read_current_velocity , 0.01 ) # stop the wheels if RoSys is shutdown (or reloaded) rosys . on_shutdown ( self . stop ) async def drive ( self , linear : float , angular : float ) -> None : # TODO send hardware command to drive with given linear and angular velocity ... async def stop ( self ) -> None : # TODO send hardware command to stop the wheels ... async def read_current_velocity ( self ) -> None : velocities : list [ rosys . geometry . Velocity ] = [] # TODO: read measured velocities from the hardware self . VELOCITY_MEASURED . emit ( velocities ) Depending on your hardware you may need to modify a PWM signal, send commands via CAN bus or serial, use Protobuf over Ethernet or something else. By raising an exception if the real hardware is not available a robot controlled by keyboard or joystick looks like this: #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Odometer , Steerer , joystick , keyboard_control from rosys.hardware import WheelsSimulation from wheels_for_custom_hardware import WheelsForCustomHardware try : wheels = WheelsForCustomHardware () except : wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) keyboard_control ( steerer ) joystick ( steerer ) ui . run ( title = 'RoSys' ) Robot Brain \u00b6 The Zauberzeug Robot Brain is an industrial-grade controller which combines artificial intelligence with machinery. It has a built-in ESP32 microcontroller with Lizard installed to do the actual hardware communication in realtime. Serial communication is used to send and receive messages between the built-in NVidia Jetson and the microcontroller. You can call SerialCommunication.is_possible() to automatically switch between simulation and real hardware. The module WheelsHardware expects a RobotBrain , which controls the SerialCommunication with the microcontroller. #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Odometer , Steerer , joystick , keyboard_control from rosys.hardware import RobotBrain , SerialCommunication , WheelsHardware , WheelsSimulation , communication is_real = SerialCommunication . is_possible () if is_real : communication = SerialCommunication () robot_brain = RobotBrain ( communication ) wheels = WheelsHardware ( robot_brain ) else : wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) keyboard_control ( steerer ) joystick ( steerer ) if is_real : communication . debug_ui () robot_brain . developer_ui () ui . run ( title = 'RoSys' ) With communication.debug_ui() you can add some helpful UI elements for debugging the serial communication. Furthermore, with robot_brain.developer_ui() you can add UI elements to configure and reboot Lizard . The Lizard configuration for a differential-steering controlled robot with an ODrive might look as follows: can = Can(32, 33, 1000000) l = ODriveMotor(can, 0x000) r = ODriveMotor(can, 0x100) l.m_per_tick = 0.0627 r.m_per_tick = 0.0627 wheels = ODriveWheels(l, r) wheels.width = 0.515 core.output(\"core.millis wheels.linear_speed:3 wheels.angular_speed:3\")","title":"Hardware"},{"location":"examples/hardware/#hardware","text":"The other examples use simulated hardware for simplicity and easy execution on any development system. To be able to control real hardware we recommend to derive a Simulation and Hardware version from a shared interface. Depending on your environment you can then instantiate the correct implementation without bothering with it in the rest of your code.","title":"Hardware"},{"location":"examples/hardware/#custom-implementation","text":"For a differential-steering controlled robot, RoSys offers a Wheels base class plus a WheelsSimulation . The following example illustrates the content of wheels_for_custom_hardware.py : import rosys import rosys.geometry import rosys.hardware class WheelsForCustomHardware ( rosys . hardware . Wheels ): def __init__ ( self ) -> None : super () . __init__ () # TODO raise exception if hardware is not available # update measured velocities regularly to feed the odometer rosys . on_repeat ( self . read_current_velocity , 0.01 ) # stop the wheels if RoSys is shutdown (or reloaded) rosys . on_shutdown ( self . stop ) async def drive ( self , linear : float , angular : float ) -> None : # TODO send hardware command to drive with given linear and angular velocity ... async def stop ( self ) -> None : # TODO send hardware command to stop the wheels ... async def read_current_velocity ( self ) -> None : velocities : list [ rosys . geometry . Velocity ] = [] # TODO: read measured velocities from the hardware self . VELOCITY_MEASURED . emit ( velocities ) Depending on your hardware you may need to modify a PWM signal, send commands via CAN bus or serial, use Protobuf over Ethernet or something else. By raising an exception if the real hardware is not available a robot controlled by keyboard or joystick looks like this: #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Odometer , Steerer , joystick , keyboard_control from rosys.hardware import WheelsSimulation from wheels_for_custom_hardware import WheelsForCustomHardware try : wheels = WheelsForCustomHardware () except : wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) keyboard_control ( steerer ) joystick ( steerer ) ui . run ( title = 'RoSys' )","title":"Custom Implementation"},{"location":"examples/hardware/#robot-brain","text":"The Zauberzeug Robot Brain is an industrial-grade controller which combines artificial intelligence with machinery. It has a built-in ESP32 microcontroller with Lizard installed to do the actual hardware communication in realtime. Serial communication is used to send and receive messages between the built-in NVidia Jetson and the microcontroller. You can call SerialCommunication.is_possible() to automatically switch between simulation and real hardware. The module WheelsHardware expects a RobotBrain , which controls the SerialCommunication with the microcontroller. #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Odometer , Steerer , joystick , keyboard_control from rosys.hardware import RobotBrain , SerialCommunication , WheelsHardware , WheelsSimulation , communication is_real = SerialCommunication . is_possible () if is_real : communication = SerialCommunication () robot_brain = RobotBrain ( communication ) wheels = WheelsHardware ( robot_brain ) else : wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) keyboard_control ( steerer ) joystick ( steerer ) if is_real : communication . debug_ui () robot_brain . developer_ui () ui . run ( title = 'RoSys' ) With communication.debug_ui() you can add some helpful UI elements for debugging the serial communication. Furthermore, with robot_brain.developer_ui() you can add UI elements to configure and reboot Lizard . The Lizard configuration for a differential-steering controlled robot with an ODrive might look as follows: can = Can(32, 33, 1000000) l = ODriveMotor(can, 0x000) r = ODriveMotor(can, 0x100) l.m_per_tick = 0.0627 r.m_per_tick = 0.0627 wheels = ODriveWheels(l, r) wheels.width = 0.515 core.output(\"core.millis wheels.linear_speed:3 wheels.angular_speed:3\")","title":"Robot Brain"},{"location":"examples/navigation/","text":"Navigation \u00b6 This example is similar to Click-and-drive but includes a PathPlanner to find a path around an obstacle. #!/usr/bin/env python3 from nicegui import ui from rosys.automation import Automator from rosys.driving import Driver , Odometer , robot_object from rosys.geometry import Point , Pose , Prism from rosys.hardware import WheelsSimulation from rosys.pathplanning import Obstacle , PathPlanner , obstacle_object , path_object shape = Prism . default_robot_shape () path_planner = PathPlanner ( shape ) path_planner . restore = lambda _ : None # NOTE: disable persistence path_planner . obstacles [ '0' ] = Obstacle ( id = '0' , outline = [ Point ( x = 3 , y = 0 ), Point ( x = 0 , y = 3 ), Point ( x = 3 , y = 3 )]) wheels = WheelsSimulation () odometer = Odometer ( wheels ) driver = Driver ( wheels , odometer ) automator = Automator ( wheels , None ) async def handle_click ( msg ): for hit in msg . hits : if hit . object_id == 'ground' : yaw = odometer . prediction . point . direction ( hit . point ) goal = Pose ( x = hit . point . x , y = hit . point . y , yaw = yaw ) path = await path_planner . search ( start = odometer . prediction , goal = goal ) path3d . update ( path ) automator . start ( driver . drive_path ( path )) with ui . scene ( on_click = handle_click , width = 600 ): robot_object ( shape , odometer ) obstacle_object ( path_planner . obstacles ) path3d = path_object () ui . label ( 'click into the scene to drive the robot' ) ui . run ( title = 'RoSys' ) Path Following \u00b6 When following a path, a \"carrot\" is dragged along a spline and the robot follows it like a donkey. Additionally, there is a virtual \"hook\" attached to the robot, which is pulled towards the carrot. There are three parameters: hook_offset : How far from the wheel axis (i.e. the coordinate center of the robot) is the hook, which is pulled towards the carrot. carrot_offset : How far ahead of the carrot is the robot pulled. This parameter is necessary in order to have the hook pulled a bit further, even though the carrot already reached the end of the spline. carrot_distance : How long is the \"thread\" between hook and carrot (or the offset point ahead of the carrot, respectively). In the following illustration these points are depicted as spheres: the coordinate center of the robot (blue, small), the hook (blue, large), carrot (orange, small), offset point ahead of the carrot (orange, large). You can display a wire frame version of the robot by passing debug=true to the robot_object . Note The automation drive_spline has an optional argument flip_hook . It turns the hook 180 degrees to the back of the robot, while preserving the distance hook_offset to the robot's coordinate center. This allows the robot to drive backwards to a point behind it instead of turning around and approaching it forwards. A more complex example can be found in the RoSys GitHub repository . There you can create new obstacles and choose between straight driving or navigation.","title":"Navigation"},{"location":"examples/navigation/#navigation","text":"This example is similar to Click-and-drive but includes a PathPlanner to find a path around an obstacle. #!/usr/bin/env python3 from nicegui import ui from rosys.automation import Automator from rosys.driving import Driver , Odometer , robot_object from rosys.geometry import Point , Pose , Prism from rosys.hardware import WheelsSimulation from rosys.pathplanning import Obstacle , PathPlanner , obstacle_object , path_object shape = Prism . default_robot_shape () path_planner = PathPlanner ( shape ) path_planner . restore = lambda _ : None # NOTE: disable persistence path_planner . obstacles [ '0' ] = Obstacle ( id = '0' , outline = [ Point ( x = 3 , y = 0 ), Point ( x = 0 , y = 3 ), Point ( x = 3 , y = 3 )]) wheels = WheelsSimulation () odometer = Odometer ( wheels ) driver = Driver ( wheels , odometer ) automator = Automator ( wheels , None ) async def handle_click ( msg ): for hit in msg . hits : if hit . object_id == 'ground' : yaw = odometer . prediction . point . direction ( hit . point ) goal = Pose ( x = hit . point . x , y = hit . point . y , yaw = yaw ) path = await path_planner . search ( start = odometer . prediction , goal = goal ) path3d . update ( path ) automator . start ( driver . drive_path ( path )) with ui . scene ( on_click = handle_click , width = 600 ): robot_object ( shape , odometer ) obstacle_object ( path_planner . obstacles ) path3d = path_object () ui . label ( 'click into the scene to drive the robot' ) ui . run ( title = 'RoSys' )","title":"Navigation"},{"location":"examples/navigation/#path-following","text":"When following a path, a \"carrot\" is dragged along a spline and the robot follows it like a donkey. Additionally, there is a virtual \"hook\" attached to the robot, which is pulled towards the carrot. There are three parameters: hook_offset : How far from the wheel axis (i.e. the coordinate center of the robot) is the hook, which is pulled towards the carrot. carrot_offset : How far ahead of the carrot is the robot pulled. This parameter is necessary in order to have the hook pulled a bit further, even though the carrot already reached the end of the spline. carrot_distance : How long is the \"thread\" between hook and carrot (or the offset point ahead of the carrot, respectively). In the following illustration these points are depicted as spheres: the coordinate center of the robot (blue, small), the hook (blue, large), carrot (orange, small), offset point ahead of the carrot (orange, large). You can display a wire frame version of the robot by passing debug=true to the robot_object . Note The automation drive_spline has an optional argument flip_hook . It turns the hook 180 degrees to the back of the robot, while preserving the distance hook_offset to the robot's coordinate center. This allows the robot to drive backwards to a point behind it instead of turning around and approaching it forwards. A more complex example can be found in the RoSys GitHub repository . There you can create new obstacles and choose between straight driving or navigation.","title":"Path Following"},{"location":"examples/persistence/","text":"Persistence \u00b6 RoSys' persistence module provides an easy interface to backup and restore parts of the object state. The following example demonstrates a Model class that has value , which is manipulated with a ui.slider . #!/usr/bin/env python3 from typing import Any from nicegui import ui from rosys import persistence class Model : def __init__ ( self ) -> None : self . value : float = 1.0 persistence . register ( self ) self . needs_backup = False def restore ( self , data : dict [ str , Any ]) -> None : self . value = data . get ( 'value' , 1.0 ) def backup ( self ) -> dict [ str , Any ]: return { 'value' : self . value , } model = Model () ui . slider ( min = 0 , max = 10.0 , step = 0.1 ) . bind_value ( model , 'value' ) . props ( 'label-always' ) ui . run ( title = 'RoSys' ) By registering itself with the persistence module and implementing backup , restore and needs_backup , RoSys will automatically write the value to a file in the directory ~/.rosys/. The filename contains the name of the module from where the register call was made. After restarting the script, the value will be restored to its last state. The needs_backup flag can be set to True to enforce a backup within RoSys' next backup cycle, which happens every 10 seconds. During shutdown, all backups are performed, independent of the needs_backup flag. The backup function can return any dictionary that represents the current state. It should match the restore function so that it can translate it back to object state. You should choose wisely which values to persist. Try to avoid consuming unnecessary CPU and IO bandwidth for volatile things like wheel odometry or other sensor readings. Note that the persistence module contains a number of helper functions: to_dict : converts (dictionaries or lists of) dataclasses into a dictionary (or list) from_dict : converts a dictionary into a dataclass of given type replace_dict : replaces the content of a dictionary using from_dict for each item replace_list : replaces the content of a list using from_dict for each item replace_dataclass : replaces the attributes of a dataclass with the values of a dictionary","title":"Persistence"},{"location":"examples/persistence/#persistence","text":"RoSys' persistence module provides an easy interface to backup and restore parts of the object state. The following example demonstrates a Model class that has value , which is manipulated with a ui.slider . #!/usr/bin/env python3 from typing import Any from nicegui import ui from rosys import persistence class Model : def __init__ ( self ) -> None : self . value : float = 1.0 persistence . register ( self ) self . needs_backup = False def restore ( self , data : dict [ str , Any ]) -> None : self . value = data . get ( 'value' , 1.0 ) def backup ( self ) -> dict [ str , Any ]: return { 'value' : self . value , } model = Model () ui . slider ( min = 0 , max = 10.0 , step = 0.1 ) . bind_value ( model , 'value' ) . props ( 'label-always' ) ui . run ( title = 'RoSys' ) By registering itself with the persistence module and implementing backup , restore and needs_backup , RoSys will automatically write the value to a file in the directory ~/.rosys/. The filename contains the name of the module from where the register call was made. After restarting the script, the value will be restored to its last state. The needs_backup flag can be set to True to enforce a backup within RoSys' next backup cycle, which happens every 10 seconds. During shutdown, all backups are performed, independent of the needs_backup flag. The backup function can return any dictionary that represents the current state. It should match the restore function so that it can translate it back to object state. You should choose wisely which values to persist. Try to avoid consuming unnecessary CPU and IO bandwidth for volatile things like wheel odometry or other sensor readings. Note that the persistence module contains a number of helper functions: to_dict : converts (dictionaries or lists of) dataclasses into a dictionary (or list) from_dict : converts a dictionary into a dataclass of given type replace_dict : replaces the content of a dictionary using from_dict for each item replace_list : replaces the content of a list using from_dict for each item replace_dataclass : replaces the attributes of a dataclass with the values of a dictionary","title":"Persistence"},{"location":"examples/schedule/","text":"Schedule automations \u00b6 You can schedule when the robot should be active. The Schedule module comes with its own UI to manipulate the half-hourly time plan. #!/usr/bin/env python3 from nicegui import ui from rosys.automation import Automator , Schedule , automation_controls from rosys.driving import Driver , Odometer , robot_object from rosys.geometry import Point , Prism from rosys.hardware import WheelsSimulation async def drive_around () -> None : while True : await driver . drive_to ( Point ( x = 1 , y = 0 )) await driver . drive_to ( Point ( x = 0 , y = 0 )) async def drive_home () -> None : await driver . drive_to ( Point ( x =- 3 , y = 0 )) shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) driver = Driver ( wheels , odometer ) automator = Automator ( wheels , None , default_automation = drive_around ) locations = { ( 52.520008 , 13.404954 ): 'Berlin' , ( 40.730610 , 73.935242 ): 'New York' , None : 'no location' , } schedule = Schedule ( automator , on_activate = drive_around , on_deactivate = drive_home , location = None , locations = locations , is_enabled = True ) schedule . fill ( False ) # disable at all times so the user can enable it manually schedule . is_enabled = True # the schedule must be enabled to take any effect with ui . row () . classes ( 'items-end' ): schedule . ui () with ui . column () . classes ( 'items-end' ): with ui . row (): automation_controls ( automator ) with ui . scene ( height = 360 ): robot_object ( shape , odometer ) ui . run ( title = 'RoSys' ) There is also the possibility to pass a geographic location to restrict the activity to daylight only.","title":"Schedule automations"},{"location":"examples/schedule/#schedule-automations","text":"You can schedule when the robot should be active. The Schedule module comes with its own UI to manipulate the half-hourly time plan. #!/usr/bin/env python3 from nicegui import ui from rosys.automation import Automator , Schedule , automation_controls from rosys.driving import Driver , Odometer , robot_object from rosys.geometry import Point , Prism from rosys.hardware import WheelsSimulation async def drive_around () -> None : while True : await driver . drive_to ( Point ( x = 1 , y = 0 )) await driver . drive_to ( Point ( x = 0 , y = 0 )) async def drive_home () -> None : await driver . drive_to ( Point ( x =- 3 , y = 0 )) shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) driver = Driver ( wheels , odometer ) automator = Automator ( wheels , None , default_automation = drive_around ) locations = { ( 52.520008 , 13.404954 ): 'Berlin' , ( 40.730610 , 73.935242 ): 'New York' , None : 'no location' , } schedule = Schedule ( automator , on_activate = drive_around , on_deactivate = drive_home , location = None , locations = locations , is_enabled = True ) schedule . fill ( False ) # disable at all times so the user can enable it manually schedule . is_enabled = True # the schedule must be enabled to take any effect with ui . row () . classes ( 'items-end' ): schedule . ui () with ui . column () . classes ( 'items-end' ): with ui . row (): automation_controls ( automator ) with ui . scene ( height = 360 ): robot_object ( shape , odometer ) ui . run ( title = 'RoSys' ) There is also the possibility to pass a geographic location to restrict the activity to daylight only.","title":"Schedule automations"},{"location":"examples/steering/","text":"Steering \u00b6 The following example simulates a robot that can be steered using keyboard controls or a joystick via web interface. #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Odometer , Steerer , joystick , keyboard_control , robot_object from rosys.geometry import Prism from rosys.hardware import WheelsSimulation shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) keyboard_control ( steerer ) joystick ( steerer , size = 50 , color = 'blue' ) with ui . scene (): robot_object ( shape , odometer ) ui . run ( title = 'RoSys' ) Keyboard Control By adding a KeyboardControl to the user interface you enable steering the robot with the keyboard. Press the arrow keys while holding the SHIFT key to steer the robot. You can also modify the speed of the robot by pressing the a number key. Use the optional parameter default_speed to change the initial value. Joystick When operating from a mobile phone, you can use a Joystick to create a UI element with touch control. You can drive the robot by dragging the mouse inside the top left square.","title":"Steering"},{"location":"examples/steering/#steering","text":"The following example simulates a robot that can be steered using keyboard controls or a joystick via web interface. #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Odometer , Steerer , joystick , keyboard_control , robot_object from rosys.geometry import Prism from rosys.hardware import WheelsSimulation shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) keyboard_control ( steerer ) joystick ( steerer , size = 50 , color = 'blue' ) with ui . scene (): robot_object ( shape , odometer ) ui . run ( title = 'RoSys' ) Keyboard Control By adding a KeyboardControl to the user interface you enable steering the robot with the keyboard. Press the arrow keys while holding the SHIFT key to steer the robot. You can also modify the speed of the robot by pressing the a number key. Use the optional parameter default_speed to change the initial value. Joystick When operating from a mobile phone, you can use a Joystick to create a UI element with touch control. You can drive the robot by dragging the mouse inside the top left square.","title":"Steering"},{"location":"reference/SUMMARY/","text":"automation driving hardware pathplanning system vision","title":"SUMMARY"},{"location":"reference/rosys/automation/","text":"Automator \u00b6 Automator ( wheels : Optional [ Drivable ], steerer : Optional [ Steerer ], * , default_automation : Optional [ Callable ] = None ) -> None An automator allows running automations, i.e. coroutines that can be paused and resumed. See Click-and-drive for a simple example of an automation. wheels : Optional wheels (or any stoppable hardware representation) will be stopped when an automation pauses or stops. steerer : If provided, manually steering the robot will pause a currently running automation. default_automation : If provided, it allows the automator to start a new automation without passing an automation (e.g. via an \"Play\"-button like offered by the automation controls ). The passed function should return a new coroutine on every call. disable \u00b6 disable ( because : str ) -> None Disables the automator. No automations can be started while the automator is disabled. If an automation is running or paused it will be stopped. You need to provide a cause which will be used as notification message. enable \u00b6 enable () -> None Enables the automator. It is enabled by default. It can be disabled by calling disable() . pause \u00b6 pause ( because : str ) -> None Pauses the current automation. You need to provide a cause which will be used as notification message. resume \u00b6 resume () -> None Resumes the current automation. start \u00b6 start ( coro : Optional [ Coroutine ] = None ) -> None Starts a new automation. You can pass any coroutine. The automator will make sure it can be paused, resumed and stopped. stop \u00b6 stop ( because : str ) -> None Stops the current automation. You need to provide a cause which will be used as notification message. Events \u00b6 Name Description AUTOMATION_STARTED an automation has been started AUTOMATION_PAUSED an automation has been paused (string argument: description of the cause) AUTOMATION_RESUMED an automation has been resumed AUTOMATION_STOPPED an automation has been stopped (string argument: description of the cause) AUTOMATION_FAILED an automation has failed to complete (string argument: description of the cause) AUTOMATION_COMPLETED an automation has been completed app_controls \u00b6 app_controls ( robot_brain : RobotBrain , automator : Automator ) -> None The AppControls module enables the connection with a mobile-app-based user interface. It uses a given RobotBrain object to communicate with Lizard running on a microcontroller and in turn being connected to a mobile app via Bluetooth Low Energy. It displays buttons to control a given automator. notify async \u00b6 notify ( msg : str ) -> None show notification as Snackbar message on mobile device set_info async \u00b6 set_info ( msg : str ) -> None replace constantly shown info text on mobile device Events \u00b6 Name Description APP_CONNECTED an app connected via bluetooth (used to refresh information or similar) automation_controls \u00b6 automation_controls ( automator : Automator ) -> None This UI element contains start/stop/pause/resume buttons for controlling a given automator.","title":"automation"},{"location":"reference/rosys/automation/#rosys.automation.Automator","text":"Automator ( wheels : Optional [ Drivable ], steerer : Optional [ Steerer ], * , default_automation : Optional [ Callable ] = None ) -> None An automator allows running automations, i.e. coroutines that can be paused and resumed. See Click-and-drive for a simple example of an automation. wheels : Optional wheels (or any stoppable hardware representation) will be stopped when an automation pauses or stops. steerer : If provided, manually steering the robot will pause a currently running automation. default_automation : If provided, it allows the automator to start a new automation without passing an automation (e.g. via an \"Play\"-button like offered by the automation controls ). The passed function should return a new coroutine on every call.","title":"Automator"},{"location":"reference/rosys/automation/#rosys.automation.automator.Automator.disable","text":"disable ( because : str ) -> None Disables the automator. No automations can be started while the automator is disabled. If an automation is running or paused it will be stopped. You need to provide a cause which will be used as notification message.","title":"disable()"},{"location":"reference/rosys/automation/#rosys.automation.automator.Automator.enable","text":"enable () -> None Enables the automator. It is enabled by default. It can be disabled by calling disable() .","title":"enable()"},{"location":"reference/rosys/automation/#rosys.automation.automator.Automator.pause","text":"pause ( because : str ) -> None Pauses the current automation. You need to provide a cause which will be used as notification message.","title":"pause()"},{"location":"reference/rosys/automation/#rosys.automation.automator.Automator.resume","text":"resume () -> None Resumes the current automation.","title":"resume()"},{"location":"reference/rosys/automation/#rosys.automation.automator.Automator.start","text":"start ( coro : Optional [ Coroutine ] = None ) -> None Starts a new automation. You can pass any coroutine. The automator will make sure it can be paused, resumed and stopped.","title":"start()"},{"location":"reference/rosys/automation/#rosys.automation.automator.Automator.stop","text":"stop ( because : str ) -> None Stops the current automation. You need to provide a cause which will be used as notification message.","title":"stop()"},{"location":"reference/rosys/automation/#events","text":"Name Description AUTOMATION_STARTED an automation has been started AUTOMATION_PAUSED an automation has been paused (string argument: description of the cause) AUTOMATION_RESUMED an automation has been resumed AUTOMATION_STOPPED an automation has been stopped (string argument: description of the cause) AUTOMATION_FAILED an automation has failed to complete (string argument: description of the cause) AUTOMATION_COMPLETED an automation has been completed","title":"Events"},{"location":"reference/rosys/automation/#rosys.automation.app_controls","text":"app_controls ( robot_brain : RobotBrain , automator : Automator ) -> None The AppControls module enables the connection with a mobile-app-based user interface. It uses a given RobotBrain object to communicate with Lizard running on a microcontroller and in turn being connected to a mobile app via Bluetooth Low Energy. It displays buttons to control a given automator.","title":"app_controls"},{"location":"reference/rosys/automation/#rosys.automation.app_controls_.AppControls.notify","text":"notify ( msg : str ) -> None show notification as Snackbar message on mobile device","title":"notify()"},{"location":"reference/rosys/automation/#rosys.automation.app_controls_.AppControls.set_info","text":"set_info ( msg : str ) -> None replace constantly shown info text on mobile device","title":"set_info()"},{"location":"reference/rosys/automation/#events_1","text":"Name Description APP_CONNECTED an app connected via bluetooth (used to refresh information or similar)","title":"Events"},{"location":"reference/rosys/automation/#rosys.automation.automation_controls","text":"automation_controls ( automator : Automator ) -> None This UI element contains start/stop/pause/resume buttons for controlling a given automator.","title":"automation_controls"},{"location":"reference/rosys/driving/","text":"Driver \u00b6 Driver ( wheels : Drivable , odometer : Odometer ) -> None The driver module allows following a given path. It requires a wheels module (or any drivable hardware representation) to execute individual drive commands. It also requires an odometer to get a current prediction of the robot's pose. Its parameters allow controlling the specific drive behavior. Odometer \u00b6 Odometer ( wheels : VelocityProvider ) -> None An odometer collects velocity information from a given wheels module (or any velocity-providing hardware representation). It can also handle \"detections\", i.e. absolute pose information with timestamps. Given the history of previously received velocities, it can update its prediction of the current pose. The get_pose method provides robot poses from the within the last 10 seconds. Events \u00b6 Name Description ROBOT_MOVED a robot movement is detected Steerer \u00b6 Steerer ( wheels : Drivable , speed_scaling : float = 1.0 ) -> None The steerer module translates x-y information (e.g. from a joystick) to linear/angular velocities sent to the robot. The wheels module can be any drivable hardware representation. Changing the steering state emits events that can be used to react to manual user interaction. The conversion from x-y joystick coordinates to linear and angular velocities is implemented as follows: The coordinates are translated into an angle (forward: 0 degrees, backward: 180 degrees). If the angle is below 110 degrees, y is used as linear velocity and x is used as angular velocity: Pulling the joystick to the front-right corner leads to a clockwise rotation while driving forwards. Above 110 degrees the angular velocity is flipped: Pulling the joystick to the rear-right corner leads to a counter-clockwise rotation while driving backwards. From 100 degrees to 110 degrees both velocities are throttled with a linear ramp from factor 1.0 down to 0.0. From 110 degrees to 120 degrees the throttle factor is linearly ramped from 0.0 back to 1.0. Events \u00b6 Name Description STEERING_STARTED steering has started STEERING_STOPPED steering has stopped driver_object \u00b6 DriverObject \u00b6 DriverObject ( driver : Driver ) -> None Bases: Group The DriverObject UI element displays the path following process in a 3D scene. The current pose is taken from a given odometer. An optional driver module shows debugging information about a current path-following process. The debug argument can be set to show a wireframe instead of a closed polygon. joystick \u00b6 joystick ( steerer : Steerer , ** options ) -> None Bases: NiceGuiJoystick The Joystick UI element allows controlling a given steerer via touch events. keyboard_control \u00b6 keyboard_control ( steerer : Steerer , * , default_speed : float = 2.0 ) -> None The KeyboardControl UI element allows controlling a given steerer via keyboard events. Hold shift while pressing an arrow key to steer the robot. You can change the speed with the number keys 1 to 9 and the initial speed via the default_speed argument. robot_object \u00b6 robot_object ( shape : Prism , odometer : Odometer , * , debug : bool = False ) -> None Bases: Group The RobotObject UI element displays the robot with its given shape in a 3D scene. The current pose is taken from a given odometer. The debug argument can be set to show a wireframe instead of a closed polygon. with_stl \u00b6 with_stl ( url : str , * , x : float = 0 , y : float = 0 , z : float = 0 , omega : float = 0 , phi : float = 0 , kappa : float = 0 , scale : float = 1.0 , color : str = \"#ffffff\" , opacity : float = 1.0 ) -> RobotObject Sets an STL to be displayed as the robot. The file can be served from a local directory with ui.add_static_files(url, path) .","title":"driving"},{"location":"reference/rosys/driving/#rosys.driving.Driver","text":"Driver ( wheels : Drivable , odometer : Odometer ) -> None The driver module allows following a given path. It requires a wheels module (or any drivable hardware representation) to execute individual drive commands. It also requires an odometer to get a current prediction of the robot's pose. Its parameters allow controlling the specific drive behavior.","title":"Driver"},{"location":"reference/rosys/driving/#rosys.driving.Odometer","text":"Odometer ( wheels : VelocityProvider ) -> None An odometer collects velocity information from a given wheels module (or any velocity-providing hardware representation). It can also handle \"detections\", i.e. absolute pose information with timestamps. Given the history of previously received velocities, it can update its prediction of the current pose. The get_pose method provides robot poses from the within the last 10 seconds.","title":"Odometer"},{"location":"reference/rosys/driving/#events","text":"Name Description ROBOT_MOVED a robot movement is detected","title":"Events"},{"location":"reference/rosys/driving/#rosys.driving.Steerer","text":"Steerer ( wheels : Drivable , speed_scaling : float = 1.0 ) -> None The steerer module translates x-y information (e.g. from a joystick) to linear/angular velocities sent to the robot. The wheels module can be any drivable hardware representation. Changing the steering state emits events that can be used to react to manual user interaction. The conversion from x-y joystick coordinates to linear and angular velocities is implemented as follows: The coordinates are translated into an angle (forward: 0 degrees, backward: 180 degrees). If the angle is below 110 degrees, y is used as linear velocity and x is used as angular velocity: Pulling the joystick to the front-right corner leads to a clockwise rotation while driving forwards. Above 110 degrees the angular velocity is flipped: Pulling the joystick to the rear-right corner leads to a counter-clockwise rotation while driving backwards. From 100 degrees to 110 degrees both velocities are throttled with a linear ramp from factor 1.0 down to 0.0. From 110 degrees to 120 degrees the throttle factor is linearly ramped from 0.0 back to 1.0.","title":"Steerer"},{"location":"reference/rosys/driving/#events_1","text":"Name Description STEERING_STARTED steering has started STEERING_STOPPED steering has stopped","title":"Events"},{"location":"reference/rosys/driving/#rosys.driving.driver_object","text":"","title":"driver_object"},{"location":"reference/rosys/driving/#rosys.driving.driver_object.DriverObject","text":"DriverObject ( driver : Driver ) -> None Bases: Group The DriverObject UI element displays the path following process in a 3D scene. The current pose is taken from a given odometer. An optional driver module shows debugging information about a current path-following process. The debug argument can be set to show a wireframe instead of a closed polygon.","title":"DriverObject"},{"location":"reference/rosys/driving/#rosys.driving.joystick","text":"joystick ( steerer : Steerer , ** options ) -> None Bases: NiceGuiJoystick The Joystick UI element allows controlling a given steerer via touch events.","title":"joystick"},{"location":"reference/rosys/driving/#rosys.driving.keyboard_control","text":"keyboard_control ( steerer : Steerer , * , default_speed : float = 2.0 ) -> None The KeyboardControl UI element allows controlling a given steerer via keyboard events. Hold shift while pressing an arrow key to steer the robot. You can change the speed with the number keys 1 to 9 and the initial speed via the default_speed argument.","title":"keyboard_control"},{"location":"reference/rosys/driving/#rosys.driving.robot_object","text":"robot_object ( shape : Prism , odometer : Odometer , * , debug : bool = False ) -> None Bases: Group The RobotObject UI element displays the robot with its given shape in a 3D scene. The current pose is taken from a given odometer. The debug argument can be set to show a wireframe instead of a closed polygon.","title":"robot_object"},{"location":"reference/rosys/driving/#rosys.driving.robot_object_.RobotObject.with_stl","text":"with_stl ( url : str , * , x : float = 0 , y : float = 0 , z : float = 0 , omega : float = 0 , phi : float = 0 , kappa : float = 0 , scale : float = 1.0 , color : str = \"#ffffff\" , opacity : float = 1.0 ) -> RobotObject Sets an STL to be displayed as the robot. The file can be served from a local directory with ui.add_static_files(url, path) .","title":"with_stl()"},{"location":"reference/rosys/hardware/","text":"Communication \u00b6 Communication () -> None Bases: abc . ABC This abstract module defines an interface for communicating with a microcontroller. Besides sending and receiving messages a communication module provides a property whether communication is possible. It can also provide a piece of debug UI. RobotBrain \u00b6 RobotBrain ( communication : Communication , lizard_startup : str = \"lizard.txt\" , ) -> None This module manages the communication with a Zauberzeug Robot Brain . It expects a communication object, which is used for the actual read and write operations. Besides providing some basic methods like configuring or restarting the microcontroller, it augments and verifies checksums for each message. Events \u00b6 Name Description LINE_RECEIVED a line has been received from the microcontroller (argument: line as string) SerialCommunication \u00b6 SerialCommunication ( baud_rate : int = 115200 ) -> None Bases: Communication This module implements a communication via a serial device with a given baud rate. It contains a list of search paths for finding the serial device. WebCommunication \u00b6 WebCommunication () -> None Bases: Communication Remote connection to the Robot Brain's ESP. This makes it possible to keep developing on your fast computer while communicating with the hardware components connected to a physical Robot Brain. Wheels \u00b6 Wheels () -> None Bases: abc . ABC The wheels module is a simple example for a representation of real or simulated robot hardware. Wheels can be moved using the drive methods and provide measured velocities as an event. Events \u00b6 Name Description VELOCITY_MEASURED new velocity measurements are available for processing (argument: list of velocities) WheelsHardware \u00b6 WheelsHardware ( robot_brain : RobotBrain ) -> None Bases: Wheels This module implements wheels hardware. Drive and stop commands are forwarded to a given Robot Brain. Velocities are read and emitted regularly. WheelsSimulation \u00b6 WheelsSimulation () -> None Bases: Wheels This module simulates two wheels. Drive and stop commands impact internal velocities (linear and angular). A simulated pose is regularly updated with these velocities, while the velocities are emitted as an event.","title":"hardware"},{"location":"reference/rosys/hardware/#rosys.hardware.Communication","text":"Communication () -> None Bases: abc . ABC This abstract module defines an interface for communicating with a microcontroller. Besides sending and receiving messages a communication module provides a property whether communication is possible. It can also provide a piece of debug UI.","title":"Communication"},{"location":"reference/rosys/hardware/#rosys.hardware.RobotBrain","text":"RobotBrain ( communication : Communication , lizard_startup : str = \"lizard.txt\" , ) -> None This module manages the communication with a Zauberzeug Robot Brain . It expects a communication object, which is used for the actual read and write operations. Besides providing some basic methods like configuring or restarting the microcontroller, it augments and verifies checksums for each message.","title":"RobotBrain"},{"location":"reference/rosys/hardware/#events","text":"Name Description LINE_RECEIVED a line has been received from the microcontroller (argument: line as string)","title":"Events"},{"location":"reference/rosys/hardware/#rosys.hardware.SerialCommunication","text":"SerialCommunication ( baud_rate : int = 115200 ) -> None Bases: Communication This module implements a communication via a serial device with a given baud rate. It contains a list of search paths for finding the serial device.","title":"SerialCommunication"},{"location":"reference/rosys/hardware/#rosys.hardware.WebCommunication","text":"WebCommunication () -> None Bases: Communication Remote connection to the Robot Brain's ESP. This makes it possible to keep developing on your fast computer while communicating with the hardware components connected to a physical Robot Brain.","title":"WebCommunication"},{"location":"reference/rosys/hardware/#rosys.hardware.Wheels","text":"Wheels () -> None Bases: abc . ABC The wheels module is a simple example for a representation of real or simulated robot hardware. Wheels can be moved using the drive methods and provide measured velocities as an event.","title":"Wheels"},{"location":"reference/rosys/hardware/#events_1","text":"Name Description VELOCITY_MEASURED new velocity measurements are available for processing (argument: list of velocities)","title":"Events"},{"location":"reference/rosys/hardware/#rosys.hardware.WheelsHardware","text":"WheelsHardware ( robot_brain : RobotBrain ) -> None Bases: Wheels This module implements wheels hardware. Drive and stop commands are forwarded to a given Robot Brain. Velocities are read and emitted regularly.","title":"WheelsHardware"},{"location":"reference/rosys/hardware/#rosys.hardware.WheelsSimulation","text":"WheelsSimulation () -> None Bases: Wheels This module simulates two wheels. Drive and stop commands impact internal velocities (linear and angular). A simulated pose is regularly updated with these velocities, while the velocities are emitted as an event.","title":"WheelsSimulation"},{"location":"reference/rosys/pathplanning/","text":"PathPlanner \u00b6 PathPlanner ( robot_shape : Prism ) -> None This module runs a path planning algorithm in a separate process. If given, the algorithm respects the given robot shape as well as a dictionary of accessible areas and a dictionary of obstacles, both of which a backed up and restored automatically. The path planner can search paths, check if a spline interferes with obstacles and get the distance of a pose to any obstacle.","title":"pathplanning"},{"location":"reference/rosys/pathplanning/#rosys.pathplanning.PathPlanner","text":"PathPlanner ( robot_shape : Prism ) -> None This module runs a path planning algorithm in a separate process. If given, the algorithm respects the given robot shape as well as a dictionary of accessible areas and a dictionary of obstacles, both of which a backed up and restored automatically. The path planner can search paths, check if a spline interferes with obstacles and get the distance of a pose to any obstacle.","title":"PathPlanner"},{"location":"reference/rosys/system/","text":"wifi_button \u00b6 wifi_button () -> None Bases: ui . button The WiFi button indicates the current connectivity state and allows setting a new WiFi connection.","title":"system"},{"location":"reference/rosys/system/#rosys.system.wifi_button","text":"wifi_button () -> None Bases: ui . button The WiFi button indicates the current connectivity state and allows setting a new WiFi connection.","title":"wifi_button"},{"location":"reference/rosys/vision/","text":"CameraProjector \u00b6 CameraProjector ( camera_provider : CameraProvider ) -> None The camera projector computes a grid of projected image points on the ground plane. It is mainly used for visualization purposes. CameraProvider \u00b6 CameraProvider () -> None Bases: abc . ABC A camera provider holds a dictionary of cameras and manages additions and removals. The camera dictionary should not be modified directly but by using the camera provider's methods. This way respective events are emitted and consistency can be taken care of. The camera provider also creates an HTTP route to access camera images. Events \u00b6 Name Description CAMERA_ADDED a new camera has been added (argument: camera) CAMERA_REMOVED a camera has been removed (argument: camera id) NEW_IMAGE an new image is available (argument: image) Detector \u00b6 Detector () -> None Bases: abc . ABC A detector allows detecting objects in images. It also holds an upload queue for sending images with uncertain results to an active learning infrastructure like the Zauberzeug Learning Loop . Events \u00b6 Name Description NEW_DETECTIONS detection on an image is completed (argument: image) DetectorHardware \u00b6 DetectorHardware ( * , port : int = 8004 ) -> None Bases: Detector This detector communicates with a YOLO detector via Socket.IO. It automatically connects and reconnects, submits and receives detections and sends images that should be uploaded to the Zauberzeug Learning Loop . detect async \u00b6 detect ( image : Image , autoupload : Autoupload = Autoupload . FILTERED , tags : list [ str ] = [], ) -> None Runs detections on the image. Afterwards the image.detections property is filled. DetectorSimulation \u00b6 DetectorSimulation ( camera_provider : CameraProvider , * , noise : float = 1.0 ) -> None Bases: Detector This detector simulates object detection. It requires a camera provider in order to check visibility using the cameras' calibrations. Individual camera IDs can be added to a set of blocked_cameras to simulate occlusions during pytests. A list of simulated_objects can be filled to define what can be detected. An optional noise parameter controls the spatial accuracy in pixels. MultiCameraProvider \u00b6 MultiCameraProvider ( * camera_providers : CameraProvider , ) -> None Bases: CameraProvider A multi-camera provider combines multiple camera providers into one. This is useful if another module requires a single camera provider but the robot has multiple camera sources like USB and WiFi cameras. UsbCameraProviderHardware \u00b6 UsbCameraProviderHardware () -> None Bases: CameraProvider This module collects and provides real USB cameras. Camera devices are discovered through video4linux (v4l) and accessed with openCV. Therefore the program v4l2ctl and openCV (including python bindings) must be available. UsbCameraProviderSimulation \u00b6 UsbCameraProviderSimulation () -> None Bases: CameraProvider This module collects and simulates USB cameras and generates synthetic images. In the current implementation the images only contain the camera ID and the current time. camera_objects \u00b6 camera_objects ( camera_provider : CameraProvider , camera_projector : CameraProjector , * , px_per_m : float = 10000 , debug : bool = False ) -> None Bases: Group This module provides a UI element for displaying cameras in a 3D scene. It requires a camera provider as a source of cameras as well as a camera projector to show the current images projected on the ground plane. The px_per_m argument can be used to scale the camera frustums. With debug=True camera IDs are shown (default: False ).","title":"vision"},{"location":"reference/rosys/vision/#rosys.vision.CameraProjector","text":"CameraProjector ( camera_provider : CameraProvider ) -> None The camera projector computes a grid of projected image points on the ground plane. It is mainly used for visualization purposes.","title":"CameraProjector"},{"location":"reference/rosys/vision/#rosys.vision.CameraProvider","text":"CameraProvider () -> None Bases: abc . ABC A camera provider holds a dictionary of cameras and manages additions and removals. The camera dictionary should not be modified directly but by using the camera provider's methods. This way respective events are emitted and consistency can be taken care of. The camera provider also creates an HTTP route to access camera images.","title":"CameraProvider"},{"location":"reference/rosys/vision/#events","text":"Name Description CAMERA_ADDED a new camera has been added (argument: camera) CAMERA_REMOVED a camera has been removed (argument: camera id) NEW_IMAGE an new image is available (argument: image)","title":"Events"},{"location":"reference/rosys/vision/#rosys.vision.Detector","text":"Detector () -> None Bases: abc . ABC A detector allows detecting objects in images. It also holds an upload queue for sending images with uncertain results to an active learning infrastructure like the Zauberzeug Learning Loop .","title":"Detector"},{"location":"reference/rosys/vision/#events_1","text":"Name Description NEW_DETECTIONS detection on an image is completed (argument: image)","title":"Events"},{"location":"reference/rosys/vision/#rosys.vision.DetectorHardware","text":"DetectorHardware ( * , port : int = 8004 ) -> None Bases: Detector This detector communicates with a YOLO detector via Socket.IO. It automatically connects and reconnects, submits and receives detections and sends images that should be uploaded to the Zauberzeug Learning Loop .","title":"DetectorHardware"},{"location":"reference/rosys/vision/#rosys.vision.detector_hardware.DetectorHardware.detect","text":"detect ( image : Image , autoupload : Autoupload = Autoupload . FILTERED , tags : list [ str ] = [], ) -> None Runs detections on the image. Afterwards the image.detections property is filled.","title":"detect()"},{"location":"reference/rosys/vision/#rosys.vision.DetectorSimulation","text":"DetectorSimulation ( camera_provider : CameraProvider , * , noise : float = 1.0 ) -> None Bases: Detector This detector simulates object detection. It requires a camera provider in order to check visibility using the cameras' calibrations. Individual camera IDs can be added to a set of blocked_cameras to simulate occlusions during pytests. A list of simulated_objects can be filled to define what can be detected. An optional noise parameter controls the spatial accuracy in pixels.","title":"DetectorSimulation"},{"location":"reference/rosys/vision/#rosys.vision.MultiCameraProvider","text":"MultiCameraProvider ( * camera_providers : CameraProvider , ) -> None Bases: CameraProvider A multi-camera provider combines multiple camera providers into one. This is useful if another module requires a single camera provider but the robot has multiple camera sources like USB and WiFi cameras.","title":"MultiCameraProvider"},{"location":"reference/rosys/vision/#rosys.vision.UsbCameraProviderHardware","text":"UsbCameraProviderHardware () -> None Bases: CameraProvider This module collects and provides real USB cameras. Camera devices are discovered through video4linux (v4l) and accessed with openCV. Therefore the program v4l2ctl and openCV (including python bindings) must be available.","title":"UsbCameraProviderHardware"},{"location":"reference/rosys/vision/#rosys.vision.UsbCameraProviderSimulation","text":"UsbCameraProviderSimulation () -> None Bases: CameraProvider This module collects and simulates USB cameras and generates synthetic images. In the current implementation the images only contain the camera ID and the current time.","title":"UsbCameraProviderSimulation"},{"location":"reference/rosys/vision/#rosys.vision.camera_objects","text":"camera_objects ( camera_provider : CameraProvider , camera_projector : CameraProjector , * , px_per_m : float = 10000 , debug : bool = False ) -> None Bases: Group This module provides a UI element for displaying cameras in a 3D scene. It requires a camera provider as a source of cameras as well as a camera projector to show the current images projected on the ground plane. The px_per_m argument can be used to scale the camera frustums. With debug=True camera IDs are shown (default: False ).","title":"camera_objects"}]}