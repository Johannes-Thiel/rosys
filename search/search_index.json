{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RoSys - The Robot System \u00b6 RoSys provides an easy-to-use robot system. Its purpose is similar to ROS . But RoSys is fully based on modern web technologies and focusses on mobile robotics. See full documentation at rosys.io . Principles \u00b6 All Python Business logic is wired in Python while computation-heavy tasks are encapsulated through websockets or bindings. Shared State All code can access and manipulate a shared and typesafe state -- this does not mean it should. Good software design is still necessary. But it is much easier to do if you do not have to perform serialization all the time. No Threading Thanks to asyncio you can write the business logic without locks and mutex mechanisms. The running system feels like everything is happening in parallel. But each code block is executed one after another through an event queue and yields execution as soon as it waits for I/O or heavy computation. The latter is still executed in threads to not block the rest of the business logic. Web UI Most machines need some kind of human interaction. We made sure your robot can be operated fully off the grid with any web browser by incorporating NiceGUI . It is also possible to proxy the user interface through a gateway for remote operation. Simulation Robot hardware is often slower than your own computer. Therefore RoSys supports a simulation mode for rapid development. To get maximum performance the current implementation does not run a full physics engine. Testing You can use pytest to write high-level integration tests. It is based on the above-described simulation mode and accelerates the robot's time for super fast execution. Note Currently RoSys is mostly tested and developed on the Zauberzeug Robot Brain which uses Lizard for communication with motors, sensors and other peripherals. But the software architecture of RoSys also allows you to write your own actors if you prefer another industrial PC or setup.","title":"About"},{"location":"#rosys-the-robot-system","text":"RoSys provides an easy-to-use robot system. Its purpose is similar to ROS . But RoSys is fully based on modern web technologies and focusses on mobile robotics. See full documentation at rosys.io .","title":"RoSys - The Robot System"},{"location":"#principles","text":"All Python Business logic is wired in Python while computation-heavy tasks are encapsulated through websockets or bindings. Shared State All code can access and manipulate a shared and typesafe state -- this does not mean it should. Good software design is still necessary. But it is much easier to do if you do not have to perform serialization all the time. No Threading Thanks to asyncio you can write the business logic without locks and mutex mechanisms. The running system feels like everything is happening in parallel. But each code block is executed one after another through an event queue and yields execution as soon as it waits for I/O or heavy computation. The latter is still executed in threads to not block the rest of the business logic. Web UI Most machines need some kind of human interaction. We made sure your robot can be operated fully off the grid with any web browser by incorporating NiceGUI . It is also possible to proxy the user interface through a gateway for remote operation. Simulation Robot hardware is often slower than your own computer. Therefore RoSys supports a simulation mode for rapid development. To get maximum performance the current implementation does not run a full physics engine. Testing You can use pytest to write high-level integration tests. It is based on the above-described simulation mode and accelerates the robot's time for super fast execution. Note Currently RoSys is mostly tested and developed on the Zauberzeug Robot Brain which uses Lizard for communication with motors, sensors and other peripherals. But the software architecture of RoSys also allows you to write your own actors if you prefer another industrial PC or setup.","title":"Principles"},{"location":"development/","text":"Development \u00b6 Logging \u00b6 RoSys uses the Python logging package with namespaced loggers. For example, the steerer actor writes its logs as rosys.steerer . This can be used for fine-granular control of what should show on the console. As a general starting point we suggest reading the Python Logging HOWTO . In the following examples we use Python's logging dictConfig for configuration, because it provides the most flexibility while having all configuration in one place. Show Info Messages \u00b6 To only print RoSys messages at the info level to the console we can use a configuration like this: #!/usr/bin/env python3 from nicegui import ui import logging import logging.config import rosys import rosys.ui logging . config . dictConfig ({ 'version' : 1 , 'disable_existing_loggers' : True , # to make sure this config is used 'formatters' : { 'default' : { 'format' : ' %(asctime)s - %(levelname)s - %(message)s ' , 'datefmt' : '%Y-%m- %d %H:%M:%S' , }, }, 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, }, 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' ], 'level' : 'INFO' , 'propagate' : False , }, }, }) # setup runtime = rosys . Runtime () rosys . ui . configure ( ui , runtime ) rosys . ui . joystick () ui . run ( title = 'RoSys' , port = 8080 ) As you move the joystick rosys.steerer , messages will appear on the console: 2022-01-11 06:53:21 - INFO - start steering 2022-01-11 06:53:22 - INFO - stop steering 2022-01-11 06:53:23 - INFO - start steering 2022-01-11 06:53:23 - INFO - stop steering Adding Loggers \u00b6 You can easily add more loggers. For example, to see debug messages of the event system you can add 'rosys.event' : { 'handlers' : [ 'console' ], 'level' : 'DEBUG' , 'propagate' : False , }, Most of the time we turn off log propagation to ensure the configuration we defined ourselves is really used. Logging to File \u00b6 Sometimes it is helpful to write intensive logging into a file and only show some messages on the console. For this you can add a file handler : 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, 'file' : { 'level' : 'DEBUG' , 'class' : 'logging.handlers.RotatingFileHandler' , 'formatter' : 'default' , 'filename' : os . path . expanduser ( '~/.rosys/example.log' ), 'maxBytes' : 1024 * 1000 , 'backupCount' : 3 } }, Then you can decide for each logger which handlers should be used: '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' , 'file' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' , 'file' ], 'level' : 'INFO' , 'propagate' : False , }, 'rosys.event' : { 'handlers' : [ 'file' ], 'level' : 'DEBUG' , 'propagate' : False , }, 'rosys.runtime' : { 'handlers' : [ 'file' ], 'level' : 'DEBUG' , 'propagate' : False , }, }, }) Note The above file logger writes to ~/.rosys . For development it is very helpful to have auto-reloading on file change activated . Therefore logging should always be stored outside of your project's source directory. Formatting \u00b6 It is quite useful to see from which file and line number a log entry was triggered. To keep the log lines from getting too long, you can create a log filter which computes the relative path: class PackagePathFilter ( logging . Filter ): '''Provides relative path for log formatter. Original code borrowed from https://stackoverflow.com/a/52582536/3419103 ''' def filter ( self , record ): pathname = record . pathname record . relativepath = None abs_sys_paths = map ( os . path . abspath , sys . path ) for path in sorted ( abs_sys_paths , key = len , reverse = True ): # longer paths first if not path . endswith ( os . sep ): path += os . sep if pathname . startswith ( path ): record . relativepath = os . path . relpath ( pathname , path ) break return True You need to register the filter and apply it in the handler. Then you can change the format for the formatter: 'formatters' : { 'default' : { 'format' : ' %(asctime)s . %(msecs)03d [ %(levelname)s ] %(relativepath)s : %(lineno)d : %(message)s ' , 'datefmt' : '%Y-%m- %d %H:%M:%S' , }, }, 'filters' : { 'package_path_filter' : { '()' : PackagePathFilter , }, }, 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'filters' : [ 'package_path_filter' ], 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, }, Log output then looks like this: 2022-01-11 06:51:00.319 [DEBUG] rosys/runtime.py:78: startup completed Profiling \u00b6 You can add a profiler button to your UI: from rosys.ui import create_profiler ... create_profiler ( ui ) When the button is pressed, the profiler yappi will start recording data. When stopped, you will see its output on the console. Memory Leaks \u00b6 To analyze memory leaks RoSys allows the integration of pyloot as a separate page: from nicegui import ui import rosys.ui runtime = rosys . Runtime () rosys . ui . configure ( ui , runtime ) rosys . ui . pyloot_page () This will add a route /pyloot to your app. The graphs will continuously update and show you which types of object counts are growing. You can then inspect them. To analyze cyclic references the objgraph library can be very helpful. You can call rosys.ui.objgraph_page() , which will add the route /objgraph to your app. Continuous Build \u00b6 We run our continuous integration with GitHub Actions. For each commit the pytests are executed. Releases \u00b6 We publish releases by creating a new version on GitHub and describe the changes. A GitHub Action then performs the following steps: If the pytests are successful, a poetry build and deployment to pypi is issued. A multi-arch docker image is built and pushed to Docker Hub .","title":"Development"},{"location":"development/#development","text":"","title":"Development"},{"location":"development/#logging","text":"RoSys uses the Python logging package with namespaced loggers. For example, the steerer actor writes its logs as rosys.steerer . This can be used for fine-granular control of what should show on the console. As a general starting point we suggest reading the Python Logging HOWTO . In the following examples we use Python's logging dictConfig for configuration, because it provides the most flexibility while having all configuration in one place.","title":"Logging"},{"location":"development/#show-info-messages","text":"To only print RoSys messages at the info level to the console we can use a configuration like this: #!/usr/bin/env python3 from nicegui import ui import logging import logging.config import rosys import rosys.ui logging . config . dictConfig ({ 'version' : 1 , 'disable_existing_loggers' : True , # to make sure this config is used 'formatters' : { 'default' : { 'format' : ' %(asctime)s - %(levelname)s - %(message)s ' , 'datefmt' : '%Y-%m- %d %H:%M:%S' , }, }, 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, }, 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' ], 'level' : 'INFO' , 'propagate' : False , }, }, }) # setup runtime = rosys . Runtime () rosys . ui . configure ( ui , runtime ) rosys . ui . joystick () ui . run ( title = 'RoSys' , port = 8080 ) As you move the joystick rosys.steerer , messages will appear on the console: 2022-01-11 06:53:21 - INFO - start steering 2022-01-11 06:53:22 - INFO - stop steering 2022-01-11 06:53:23 - INFO - start steering 2022-01-11 06:53:23 - INFO - stop steering","title":"Show Info Messages"},{"location":"development/#adding-loggers","text":"You can easily add more loggers. For example, to see debug messages of the event system you can add 'rosys.event' : { 'handlers' : [ 'console' ], 'level' : 'DEBUG' , 'propagate' : False , }, Most of the time we turn off log propagation to ensure the configuration we defined ourselves is really used.","title":"Adding Loggers"},{"location":"development/#logging-to-file","text":"Sometimes it is helpful to write intensive logging into a file and only show some messages on the console. For this you can add a file handler : 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, 'file' : { 'level' : 'DEBUG' , 'class' : 'logging.handlers.RotatingFileHandler' , 'formatter' : 'default' , 'filename' : os . path . expanduser ( '~/.rosys/example.log' ), 'maxBytes' : 1024 * 1000 , 'backupCount' : 3 } }, Then you can decide for each logger which handlers should be used: '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' , 'file' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' , 'file' ], 'level' : 'INFO' , 'propagate' : False , }, 'rosys.event' : { 'handlers' : [ 'file' ], 'level' : 'DEBUG' , 'propagate' : False , }, 'rosys.runtime' : { 'handlers' : [ 'file' ], 'level' : 'DEBUG' , 'propagate' : False , }, }, }) Note The above file logger writes to ~/.rosys . For development it is very helpful to have auto-reloading on file change activated . Therefore logging should always be stored outside of your project's source directory.","title":"Logging to File"},{"location":"development/#formatting","text":"It is quite useful to see from which file and line number a log entry was triggered. To keep the log lines from getting too long, you can create a log filter which computes the relative path: class PackagePathFilter ( logging . Filter ): '''Provides relative path for log formatter. Original code borrowed from https://stackoverflow.com/a/52582536/3419103 ''' def filter ( self , record ): pathname = record . pathname record . relativepath = None abs_sys_paths = map ( os . path . abspath , sys . path ) for path in sorted ( abs_sys_paths , key = len , reverse = True ): # longer paths first if not path . endswith ( os . sep ): path += os . sep if pathname . startswith ( path ): record . relativepath = os . path . relpath ( pathname , path ) break return True You need to register the filter and apply it in the handler. Then you can change the format for the formatter: 'formatters' : { 'default' : { 'format' : ' %(asctime)s . %(msecs)03d [ %(levelname)s ] %(relativepath)s : %(lineno)d : %(message)s ' , 'datefmt' : '%Y-%m- %d %H:%M:%S' , }, }, 'filters' : { 'package_path_filter' : { '()' : PackagePathFilter , }, }, 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'filters' : [ 'package_path_filter' ], 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, }, Log output then looks like this: 2022-01-11 06:51:00.319 [DEBUG] rosys/runtime.py:78: startup completed","title":"Formatting"},{"location":"development/#profiling","text":"You can add a profiler button to your UI: from rosys.ui import create_profiler ... create_profiler ( ui ) When the button is pressed, the profiler yappi will start recording data. When stopped, you will see its output on the console.","title":"Profiling"},{"location":"development/#memory-leaks","text":"To analyze memory leaks RoSys allows the integration of pyloot as a separate page: from nicegui import ui import rosys.ui runtime = rosys . Runtime () rosys . ui . configure ( ui , runtime ) rosys . ui . pyloot_page () This will add a route /pyloot to your app. The graphs will continuously update and show you which types of object counts are growing. You can then inspect them. To analyze cyclic references the objgraph library can be very helpful. You can call rosys.ui.objgraph_page() , which will add the route /objgraph to your app.","title":"Memory Leaks"},{"location":"development/#continuous-build","text":"We run our continuous integration with GitHub Actions. For each commit the pytests are executed.","title":"Continuous Build"},{"location":"development/#releases","text":"We publish releases by creating a new version on GitHub and describe the changes. A GitHub Action then performs the following steps: If the pytests are successful, a poetry build and deployment to pypi is issued. A multi-arch docker image is built and pushed to Docker Hub .","title":"Releases"},{"location":"getting_started/","text":"Getting Started \u00b6 First install RoSys with pip or Docker. Then create a directory to host your code and put it under version control. Name your entry file main.py and add the following content: #!/usr/bin/env python3 from nicegui import ui import rosys import rosys.ui # setup runtime = rosys . Runtime () rosys . ui . configure ( ui , runtime ) # keyboard control rosys . ui . keyboard_control () # 3d scene with ui . scene () as scene : robot = rosys . ui . robot_object () ui . label ( 'hold SHIFT to steer with the keyboard arrow keys' ) # start ui . run ( title = 'RoSys' ) If you launch the program, your browser will open the url http://0.0.0.0:8080/ and present a 3d view: Explanations \u00b6 Imports \u00b6 The User Interface is built with NiceGUI . The imports must be stated separately to make it possible to run RoSys without it. Setup \u00b6 As you can read up in the \"Architecture\" chapter RoSys provides a runtime to manage the actors which operate on the world. The command rosys.ui.configure(ui, runtime) connects the user interface with the runtime. Keyboard Control \u00b6 By calling rosys.ui.keyboard_control() you create a keyboard event listener which will steer the robot. There are also other possibilities of steering the robot like a Joystick or clicking in the 3d scene . 3D Scene \u00b6 As it is common for groups of UI elements in NiceGUI, a 3d scene is created by using context through Python's with statement. Every command \"inside\" is applied to the created scene. Here a 3d representation of the robot is created ( rosys.ui.robot_object() ). Then the ui.timer ensures its position is updated every 50 ms. A ui.label is used afterwards to explain the keyboard interaction. Note: The label is on the same intendation level as the ui.scene object, not within. See NiceGUI for a complete API reference. Start \u00b6 NiceGUI provides a ui.run command which launches the webserver and presents the interface as configured above. If you modify the code, an automatic reload is triggered. This is very convenient, but can be deactivated by passing reload=False .","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"First install RoSys with pip or Docker. Then create a directory to host your code and put it under version control. Name your entry file main.py and add the following content: #!/usr/bin/env python3 from nicegui import ui import rosys import rosys.ui # setup runtime = rosys . Runtime () rosys . ui . configure ( ui , runtime ) # keyboard control rosys . ui . keyboard_control () # 3d scene with ui . scene () as scene : robot = rosys . ui . robot_object () ui . label ( 'hold SHIFT to steer with the keyboard arrow keys' ) # start ui . run ( title = 'RoSys' ) If you launch the program, your browser will open the url http://0.0.0.0:8080/ and present a 3d view:","title":"Getting Started"},{"location":"getting_started/#explanations","text":"","title":"Explanations"},{"location":"getting_started/#imports","text":"The User Interface is built with NiceGUI . The imports must be stated separately to make it possible to run RoSys without it.","title":"Imports"},{"location":"getting_started/#setup","text":"As you can read up in the \"Architecture\" chapter RoSys provides a runtime to manage the actors which operate on the world. The command rosys.ui.configure(ui, runtime) connects the user interface with the runtime.","title":"Setup"},{"location":"getting_started/#keyboard-control","text":"By calling rosys.ui.keyboard_control() you create a keyboard event listener which will steer the robot. There are also other possibilities of steering the robot like a Joystick or clicking in the 3d scene .","title":"Keyboard Control"},{"location":"getting_started/#3d-scene","text":"As it is common for groups of UI elements in NiceGUI, a 3d scene is created by using context through Python's with statement. Every command \"inside\" is applied to the created scene. Here a 3d representation of the robot is created ( rosys.ui.robot_object() ). Then the ui.timer ensures its position is updated every 50 ms. A ui.label is used afterwards to explain the keyboard interaction. Note: The label is on the same intendation level as the ui.scene object, not within. See NiceGUI for a complete API reference.","title":"3D Scene"},{"location":"getting_started/#start","text":"NiceGUI provides a ui.run command which launches the webserver and presents the interface as configured above. If you modify the code, an automatic reload is triggered. This is very convenient, but can be deactivated by passing reload=False .","title":"Start"},{"location":"installation/","text":"Installation \u00b6 On Your Desktop \u00b6 python3 -m pip install rosys See Getting Started for what to do next. On The Robot \u00b6 While the above installation commands work in a well setup environment, it is often easier to run RoSys inside a docker container, especially on Nvidia Jetson devices with their old 18.04 LTS Ubuntu. Launching \u00b6 There are some specialities needed to start RoSys in different environments (Mac, Linux, Jetson, ...). To simplify the usage we wrapped this in a script called ./docker.sh which you can also use and adapt in your own project. Have a look at the examples to see how a setup of your own repository could look like. Hardware access \u00b6 You can configure the dockerized system by putting these variables into the /.env file: ESP_SERIAL=/dev/ttyTHS1 : path to the ESP device; default is /dev/null to be able to start RoSys anywhere ... Remote Development \u00b6 You can develop quite a lot of functionality with a simulated robot on your own computer. But there comes a time when you want to run your code on a real robot. Normally you will therefore start the container on the Robot Brain and connect via WiFi to the web interface. By using VS Code Remote Containers you can continue development as if you are using your own computer. Unfortunately some robot hardware (for example Nvidia Jetson) is much much slower than your own machine. With a large code base this can result in long restart times after you change some code (30 seconds or more). By launching sudo ./controller/esp_proxy.py on the Robot Brain you can keep developing on your computer while beeing connected to the hardware via WiFi. When the runtime is initalized, it will first try to find the ESP32 of the Robot Brain locally. If this does not work, it tries to reach the Robot Brain via the local WiFi connection. Only if this also fails, it will fallback on a simulated hardware system.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#on-your-desktop","text":"python3 -m pip install rosys See Getting Started for what to do next.","title":"On Your Desktop"},{"location":"installation/#on-the-robot","text":"While the above installation commands work in a well setup environment, it is often easier to run RoSys inside a docker container, especially on Nvidia Jetson devices with their old 18.04 LTS Ubuntu.","title":"On The Robot"},{"location":"installation/#launching","text":"There are some specialities needed to start RoSys in different environments (Mac, Linux, Jetson, ...). To simplify the usage we wrapped this in a script called ./docker.sh which you can also use and adapt in your own project. Have a look at the examples to see how a setup of your own repository could look like.","title":"Launching"},{"location":"installation/#hardware-access","text":"You can configure the dockerized system by putting these variables into the /.env file: ESP_SERIAL=/dev/ttyTHS1 : path to the ESP device; default is /dev/null to be able to start RoSys anywhere ...","title":"Hardware access"},{"location":"installation/#remote-development","text":"You can develop quite a lot of functionality with a simulated robot on your own computer. But there comes a time when you want to run your code on a real robot. Normally you will therefore start the container on the Robot Brain and connect via WiFi to the web interface. By using VS Code Remote Containers you can continue development as if you are using your own computer. Unfortunately some robot hardware (for example Nvidia Jetson) is much much slower than your own machine. With a large code base this can result in long restart times after you change some code (30 seconds or more). By launching sudo ./controller/esp_proxy.py on the Robot Brain you can keep developing on your computer while beeing connected to the hardware via WiFi. When the runtime is initalized, it will first try to find the ESP32 of the Robot Brain locally. If this does not work, it tries to reach the Robot Brain via the local WiFi connection. Only if this also fails, it will fallback on a simulated hardware system.","title":"Remote Development"},{"location":"safety/","text":"Safety \u00b6 Python is fast enough for most high level logic. But of course it has no realtime guarantees. Safety-relevant behavior should therefore be written in Lizard and executed on a suitable microprocessor. The microprocessor governs the hardware of the robot and must be able to perform safety actions like triggering emergency hold etc. We suggest you use an industrial PC like the Zauberzeug Robot Brain . It provides a Linux system with AI acceleration to run RoSys, an integrated ESP32 to run Lizard and six I/O sockets for CAN, RS484, SPI, I2C, ... with a software controllable ENABLE switch. Operation States \u00b6 on \u00b6 This is the default state after launching RoSys. To enter the next state \"manual.drive\" you should write an automation which ensures the robot is ready (eg. no active emergency stops). manual.drive \u00b6 Normally this is the default target after \"on\". All axes should not be moved -- except the drive units. The purpose of this state is to steer the robot by an operator. homing \u00b6 To transition to higher-level operation states like \"manual.operate\" or \"auto\" the robot requires to know the zero position of each motor axis. Normally homing is done sequentially on a per-axis basis. manual.operate \u00b6 Mostly used during development but also sometimes for maintenance. All axes can be controlled by the operator as long as the movement does not violate safety requirements. It can only be activated if homing was successful. auto \u00b6 This state indicates the fully autonomous operation of the robot. Activation must happen through a user interaction and requires a successful homing. See UI Automation Controls for a built-in solution to start/stop and pause/resume such automations. hold \u00b6 In this state the robot must completely stop as quick as possible, for example by applying the breaks. This state is entered if any safety requirement is violated. stop \u00b6 Cuts all power connections leading to a total immediate shutdown which must be reactivated manually. Most robots do not require this state.","title":"Safety"},{"location":"safety/#safety","text":"Python is fast enough for most high level logic. But of course it has no realtime guarantees. Safety-relevant behavior should therefore be written in Lizard and executed on a suitable microprocessor. The microprocessor governs the hardware of the robot and must be able to perform safety actions like triggering emergency hold etc. We suggest you use an industrial PC like the Zauberzeug Robot Brain . It provides a Linux system with AI acceleration to run RoSys, an integrated ESP32 to run Lizard and six I/O sockets for CAN, RS484, SPI, I2C, ... with a software controllable ENABLE switch.","title":"Safety"},{"location":"safety/#operation-states","text":"","title":"Operation States"},{"location":"safety/#on","text":"This is the default state after launching RoSys. To enter the next state \"manual.drive\" you should write an automation which ensures the robot is ready (eg. no active emergency stops).","title":"on"},{"location":"safety/#manualdrive","text":"Normally this is the default target after \"on\". All axes should not be moved -- except the drive units. The purpose of this state is to steer the robot by an operator.","title":"manual.drive"},{"location":"safety/#homing","text":"To transition to higher-level operation states like \"manual.operate\" or \"auto\" the robot requires to know the zero position of each motor axis. Normally homing is done sequentially on a per-axis basis.","title":"homing"},{"location":"safety/#manualoperate","text":"Mostly used during development but also sometimes for maintenance. All axes can be controlled by the operator as long as the movement does not violate safety requirements. It can only be activated if homing was successful.","title":"manual.operate"},{"location":"safety/#auto","text":"This state indicates the fully autonomous operation of the robot. Activation must happen through a user interaction and requires a successful homing. See UI Automation Controls for a built-in solution to start/stop and pause/resume such automations.","title":"auto"},{"location":"safety/#hold","text":"In this state the robot must completely stop as quick as possible, for example by applying the breaks. This state is entered if any safety requirement is violated.","title":"hold"},{"location":"safety/#stop","text":"Cuts all power connections leading to a total immediate shutdown which must be reactivated manually. Most robots do not require this state.","title":"stop"},{"location":"troubleshooting/","text":"Troubleshooting \u00b6 Asyncio Warning \u00b6 While running RoSys you may see warnings similar to this one: 2021-10-31 15:08:04.040 [WARNING] asyncio: Executing <Task pending name='Task-255' coro=<handle_event() running at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:344> wait_for=<_GatheringFuture pending cb=[<TaskWakeupMethWrapper object at 0x7f7001f8e0>()] created at /usr/local/lib/python3.9/asyncio/tasks.py:705> created at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:261> took 0.238 seconds This means some coroutine is clogging the event loop for too long. In the above example it is a whopping 238 ms in which no other actor can do anything. This is an eternity when machine communication is expected to happen about every 10 ms. The warning also provides a (not so readable) hint where the time is consumed. The example above is one of the more frequent scenarios. It means some code inside a user interaction event handler (e.g. handle_event() in justpy.py ) is blocking. Try to figure out which UI event code is responsible by commenting out parts of your logic and try to reproduce the warning systematically.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"troubleshooting/#asyncio-warning","text":"While running RoSys you may see warnings similar to this one: 2021-10-31 15:08:04.040 [WARNING] asyncio: Executing <Task pending name='Task-255' coro=<handle_event() running at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:344> wait_for=<_GatheringFuture pending cb=[<TaskWakeupMethWrapper object at 0x7f7001f8e0>()] created at /usr/local/lib/python3.9/asyncio/tasks.py:705> created at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:261> took 0.238 seconds This means some coroutine is clogging the event loop for too long. In the above example it is a whopping 238 ms in which no other actor can do anything. This is an eternity when machine communication is expected to happen about every 10 ms. The warning also provides a (not so readable) hint where the time is consumed. The example above is one of the more frequent scenarios. It means some code inside a user interaction event handler (e.g. handle_event() in justpy.py ) is blocking. Try to figure out which UI event code is responsible by commenting out parts of your logic and try to reproduce the warning systematically.","title":"Asyncio Warning"},{"location":"architecture/actors/","text":"Actors \u00b6 Invocation \u00b6 Continuos Invocation \u00b6 The runtime will invoke the actor's step method in the interval defined in the class variable interval . Delayed Execution \u00b6 If you want to delay the execution, you should invoke await rosys.sleep(seconds: float) . Using time.sleep would result in blocking the whole runtime and await asyncio.sleep would delay execution of tests. Threading and Multiprocessing \u00b6 Not every piece of code is already using asyncio. The actor class provides convenience functions for IO and CPU bound work. IO Bound \u00b6 If you need to read from an external device or use a non-async HTTP library like requests , you should wrap the code in a coroutine and await it with await rosys.run.io_bound(...) . CPU Bound \u00b6 If you need to do some heavy computation, you should wrap the code in a coroutine and await it with await rosys.run.cpu_bound(...) . Notifications \u00b6 Actors can notify the user through self.notify('message to the user') . When using the default UI , the notifications will show as snackbar messages. The history of notifications is stored in the world .","title":"Actors"},{"location":"architecture/actors/#actors","text":"","title":"Actors"},{"location":"architecture/actors/#invocation","text":"","title":"Invocation"},{"location":"architecture/actors/#continuos-invocation","text":"The runtime will invoke the actor's step method in the interval defined in the class variable interval .","title":"Continuos Invocation"},{"location":"architecture/actors/#delayed-execution","text":"If you want to delay the execution, you should invoke await rosys.sleep(seconds: float) . Using time.sleep would result in blocking the whole runtime and await asyncio.sleep would delay execution of tests.","title":"Delayed Execution"},{"location":"architecture/actors/#threading-and-multiprocessing","text":"Not every piece of code is already using asyncio. The actor class provides convenience functions for IO and CPU bound work.","title":"Threading and Multiprocessing"},{"location":"architecture/actors/#io-bound","text":"If you need to read from an external device or use a non-async HTTP library like requests , you should wrap the code in a coroutine and await it with await rosys.run.io_bound(...) .","title":"IO Bound"},{"location":"architecture/actors/#cpu-bound","text":"If you need to do some heavy computation, you should wrap the code in a coroutine and await it with await rosys.run.cpu_bound(...) .","title":"CPU Bound"},{"location":"architecture/actors/#notifications","text":"Actors can notify the user through self.notify('message to the user') . When using the default UI , the notifications will show as snackbar messages. The history of notifications is stored in the world .","title":"Notifications"},{"location":"architecture/architecture_overview/","text":"Architecture Overview \u00b6 World \u00b6 All state is stored in a data class called World . It not only contains the robot's position, time, obstacles, etc. but also images and other sensor data. This makes it easy to persist and restore the full state and simplifies the modularization of different actors. Actors \u00b6 Actors encapsulate behavior. Each actor performs a specific task by reading and manipulating the world and emitting events, for example: communication with a sensor, AI inference, monitoring battery level or similar. Actors can specify a frequency at which they want to get called. Note The \"robot\" is not an actor but rather represented by an object in the world. Actors like the odometer or steerer use and transform these data to update this representation regulary. Events \u00b6 Events allow actors to signal a notable new state (most of the time written to the world). Other actors or UI components can register for these events to act upon them. This is helpful for example if one actor fetches an image and another one should process it as soon as it is available. Runtime \u00b6 The runtime manages the world and all actors. It also provides safety functionality like starting/stopping automations (see below), logging and error handling. Automations \u00b6 RoSys provides an actor which is accessed through runtime.automator . The automator can receive instruction sequences which we call \"automations\". Normally they contain machine commands followed by conditions which can be awaited before proceeding with the next commands. For example, there are already built-in automations for steering the robot along a spline.","title":"Overview"},{"location":"architecture/architecture_overview/#architecture-overview","text":"","title":"Architecture Overview"},{"location":"architecture/architecture_overview/#world","text":"All state is stored in a data class called World . It not only contains the robot's position, time, obstacles, etc. but also images and other sensor data. This makes it easy to persist and restore the full state and simplifies the modularization of different actors.","title":"World"},{"location":"architecture/architecture_overview/#actors","text":"Actors encapsulate behavior. Each actor performs a specific task by reading and manipulating the world and emitting events, for example: communication with a sensor, AI inference, monitoring battery level or similar. Actors can specify a frequency at which they want to get called. Note The \"robot\" is not an actor but rather represented by an object in the world. Actors like the odometer or steerer use and transform these data to update this representation regulary.","title":"Actors"},{"location":"architecture/architecture_overview/#events","text":"Events allow actors to signal a notable new state (most of the time written to the world). Other actors or UI components can register for these events to act upon them. This is helpful for example if one actor fetches an image and another one should process it as soon as it is available.","title":"Events"},{"location":"architecture/architecture_overview/#runtime","text":"The runtime manages the world and all actors. It also provides safety functionality like starting/stopping automations (see below), logging and error handling.","title":"Runtime"},{"location":"architecture/architecture_overview/#automations","text":"RoSys provides an actor which is accessed through runtime.automator . The automator can receive instruction sequences which we call \"automations\". Normally they contain machine commands followed by conditions which can be awaited before proceeding with the next commands. For example, there are already built-in automations for steering the robot along a spline.","title":"Automations"},{"location":"architecture/automations/","text":"Automations \u00b6 RoSys is inherently designed to be used for autonomous robots. The automator actor provides an easy way to write logical step-wise rules for this. Automation Controls \u00b6 RoSys provides ready-made UI elements to start, pause, resume and stop automations. See Automation Controls in the User Interface section for an example. Default Automation \u00b6 In most cases your robot will have one automation which describes its automatic behavior. This automation can be passed to rosys.ui.automation_controls in your main file. When the user presses the start button, RoSys will begin executing this default automation. Pausing \u00b6 To pause the ongoing automation you should fire the event event.Id.PAUSE_AUTOMATION and provide an explanation as string parameter. The runtime provides a handy wrapper runtime.automation.pause(because='...') . From within an actor you can use self.pause_automation(because='...') .","title":"Automations"},{"location":"architecture/automations/#automations","text":"RoSys is inherently designed to be used for autonomous robots. The automator actor provides an easy way to write logical step-wise rules for this.","title":"Automations"},{"location":"architecture/automations/#automation-controls","text":"RoSys provides ready-made UI elements to start, pause, resume and stop automations. See Automation Controls in the User Interface section for an example.","title":"Automation Controls"},{"location":"architecture/automations/#default-automation","text":"In most cases your robot will have one automation which describes its automatic behavior. This automation can be passed to rosys.ui.automation_controls in your main file. When the user presses the start button, RoSys will begin executing this default automation.","title":"Default Automation"},{"location":"architecture/automations/#pausing","text":"To pause the ongoing automation you should fire the event event.Id.PAUSE_AUTOMATION and provide an explanation as string parameter. The runtime provides a handy wrapper runtime.automation.pause(because='...') . From within an actor you can use self.pause_automation(because='...') .","title":"Pausing"},{"location":"architecture/events/","text":"Events \u00b6 RoSys provides an integrated event bus to chain otherwise separated parts of the system together. This allows loosely coupled actors where each one has its own dedicated role in a more complex workflow. For example the Lizard actor repeatedly reads the current machine data like velocity, battery, ..., writes them into the world and finally fires the event NEW_MACHINE_DATA . The Odometer actor has registered itself for this event to compute the new position of the robot. Other actors may also register for the same event to monitor battery level or bump events. Example \u00b6 Watching the battery level: #!/usr/bin/env python3 from nicegui import ui from rosys import event from rosys.actors import Actor from rosys.automations import drive_path from rosys.world import PathSegment , Pose , Spline import rosys.ui runtime = rosys . Runtime () rosys . ui . configure ( ui , runtime ) class BatteryGuard ( Actor ): def __init__ ( self ) -> None : super () . __init__ () event . register ( event . Id . NEW_MACHINE_DATA , self . check_battery ) def check_battery ( self ): if self . world . robot . battery < 24 : event . emit ( event . Id . PAUSE_AUTOMATION , 'battery level is below 24 V' ) runtime . with_actors ( BatteryGuard ()) voltage = ui . label () ui . timer ( 1 , lambda : voltage . set_text ( f ' { runtime . world . robot . battery : .1f } V, pose: { runtime . world . robot . prediction } ' )) path = [ PathSegment ( spline = Spline . from_poses ( Pose (), Pose ( x = 10 , y = 2 )))] async def automation (): await drive_path ( runtime . world , runtime . hardware , path ) rosys . ui . automation_controls ( automation ) ui . run ( title = 'RoSys' , port = 8080 )","title":"Events"},{"location":"architecture/events/#events","text":"RoSys provides an integrated event bus to chain otherwise separated parts of the system together. This allows loosely coupled actors where each one has its own dedicated role in a more complex workflow. For example the Lizard actor repeatedly reads the current machine data like velocity, battery, ..., writes them into the world and finally fires the event NEW_MACHINE_DATA . The Odometer actor has registered itself for this event to compute the new position of the robot. Other actors may also register for the same event to monitor battery level or bump events.","title":"Events"},{"location":"architecture/events/#example","text":"Watching the battery level: #!/usr/bin/env python3 from nicegui import ui from rosys import event from rosys.actors import Actor from rosys.automations import drive_path from rosys.world import PathSegment , Pose , Spline import rosys.ui runtime = rosys . Runtime () rosys . ui . configure ( ui , runtime ) class BatteryGuard ( Actor ): def __init__ ( self ) -> None : super () . __init__ () event . register ( event . Id . NEW_MACHINE_DATA , self . check_battery ) def check_battery ( self ): if self . world . robot . battery < 24 : event . emit ( event . Id . PAUSE_AUTOMATION , 'battery level is below 24 V' ) runtime . with_actors ( BatteryGuard ()) voltage = ui . label () ui . timer ( 1 , lambda : voltage . set_text ( f ' { runtime . world . robot . battery : .1f } V, pose: { runtime . world . robot . prediction } ' )) path = [ PathSegment ( spline = Spline . from_poses ( Pose (), Pose ( x = 10 , y = 2 )))] async def automation (): await drive_path ( runtime . world , runtime . hardware , path ) rosys . ui . automation_controls ( automation ) ui . run ( title = 'RoSys' , port = 8080 )","title":"Example"},{"location":"architecture/user_interface/","text":"User Interface \u00b6 RoSys plays very well with NiceGUI and provides additional robot-related components through the rosys.ui package. NiceGUI is a high-level web UI framework on top of JustPy . This means you can write all UI code in Python. The state is automatically reflected in the browser through websockets. RoSys can also be used with other user interfaces or interaction models if required, for example a completely app-based control through Bluetooth LE with Flutter. Configure \u00b6 To make RoSys UI elements aware of the RoSys runtime you must call rosys.ui.configure : from nicegui import ui import rosys import rosys.ui runtime = rosys . Runtime () rosys . ui . configure ( ui , runtime ) Afterwards you can use the UI elements provided by RoSys like automation controls , 3d scene , cameras or joystick .","title":"User Interface"},{"location":"architecture/user_interface/#user-interface","text":"RoSys plays very well with NiceGUI and provides additional robot-related components through the rosys.ui package. NiceGUI is a high-level web UI framework on top of JustPy . This means you can write all UI code in Python. The state is automatically reflected in the browser through websockets. RoSys can also be used with other user interfaces or interaction models if required, for example a completely app-based control through Bluetooth LE with Flutter.","title":"User Interface"},{"location":"architecture/user_interface/#configure","text":"To make RoSys UI elements aware of the RoSys runtime you must call rosys.ui.configure : from nicegui import ui import rosys import rosys.ui runtime = rosys . Runtime () rosys . ui . configure ( ui , runtime ) Afterwards you can use the UI elements provided by RoSys like automation controls , 3d scene , cameras or joystick .","title":"Configure"},{"location":"architecture/world/","text":"World \u00b6 The world object is the global data class which is constantly updated and changed by actors . To store your own data, we suggest to derive a project-specific world from the one RoSys provides. Robot \u00b6 The robot's state is stored in world.robot . Besides a shape the object stores configuration parameters like maximum linear/angular speed, minimum_turning_radius and similar. It also has a hardware configuration which will be translated to Lizard commands on startup. There are three Pose objects describing the current position and orientation of the robot: detection : latest externally referenced detection (obtained by GPS/RTK or cameras) prediction : building on top of detection this pose is improved by all odometry information accumulated afterwards; this is the pose you normally use to determine where the robot is right now simulation : the true pose of the robot without any noise (only used in simulation and testing) Notifications \u00b6 Any notification scheduled by actors is stored in world.notifications as a list of two-value tuples. The first value is the timestamp of the notification as float and the second contains the actual message. Most recent messages are at the end of the list.","title":"World"},{"location":"architecture/world/#world","text":"The world object is the global data class which is constantly updated and changed by actors . To store your own data, we suggest to derive a project-specific world from the one RoSys provides.","title":"World"},{"location":"architecture/world/#robot","text":"The robot's state is stored in world.robot . Besides a shape the object stores configuration parameters like maximum linear/angular speed, minimum_turning_radius and similar. It also has a hardware configuration which will be translated to Lizard commands on startup. There are three Pose objects describing the current position and orientation of the robot: detection : latest externally referenced detection (obtained by GPS/RTK or cameras) prediction : building on top of detection this pose is improved by all odometry information accumulated afterwards; this is the pose you normally use to determine where the robot is right now simulation : the true pose of the robot without any noise (only used in simulation and testing)","title":"Robot"},{"location":"architecture/world/#notifications","text":"Any notification scheduled by actors is stored in world.notifications as a list of two-value tuples. The first value is the timestamp of the notification as float and the second contains the actual message. Most recent messages are at the end of the list.","title":"Notifications"},{"location":"features/3d_scene/","text":"3D Scene \u00b6 Robot and Shape \u00b6 It is often desired to visualize all the robot's information about the world. To do so you can create a 3d scene with NiceGUI . RoSys provides a robot_object to render and update the robot: from rosys.world import Robot , RobotShape , World shape = RobotShape ( outline = [ ( 0 , 0 ), ( - 0.5 , - 0.5 ), ( 1.5 , - 0.5 ), ( 1.75 , 0 ), ( 1.5 , 0.5 ), ( - 0.5 , 0.5 ), ]) # the shape for the robot will be used in 3d rendering world = World ( robot = Robot ( shape = shape )) runtime = rosys . Runtime ( world ) rosys . ui . configure ( ui , runtime ) with ui . scene () as scene : # by passing `debug=True` to the robot 3d object you will see the wireframe, axis-center and follow-the-line target robot = rosys . ui . robot_object ( debug = True ) Click Handler \u00b6 You can also pass a click handler to the 3d scene. Here is a full example example for driving to a point on the ground by starting the built-in automation called drive_to : #!/usr/bin/env python3 from rosys.automations import drive_to from nicegui import ui import rosys import rosys.ui from rosys.world import Point runtime = rosys . Runtime () rosys . ui . configure ( ui , runtime ) async def handle_click ( msg ): for hit in msg . hits : target = Point ( x = hit . point . x , y = hit . point . y ) runtime . automator . start ( drive_to ( runtime . world , runtime . hardware , target )) with ui . scene ( on_click = handle_click ) as scene : robot = rosys . ui . robot_object ( debug = True ) ui . label ( 'click into the scene to drive the robot' ) ui . run ( title = 'RoSys' , port = 8080 )","title":"3D Scene"},{"location":"features/3d_scene/#3d-scene","text":"","title":"3D Scene"},{"location":"features/3d_scene/#robot-and-shape","text":"It is often desired to visualize all the robot's information about the world. To do so you can create a 3d scene with NiceGUI . RoSys provides a robot_object to render and update the robot: from rosys.world import Robot , RobotShape , World shape = RobotShape ( outline = [ ( 0 , 0 ), ( - 0.5 , - 0.5 ), ( 1.5 , - 0.5 ), ( 1.75 , 0 ), ( 1.5 , 0.5 ), ( - 0.5 , 0.5 ), ]) # the shape for the robot will be used in 3d rendering world = World ( robot = Robot ( shape = shape )) runtime = rosys . Runtime ( world ) rosys . ui . configure ( ui , runtime ) with ui . scene () as scene : # by passing `debug=True` to the robot 3d object you will see the wireframe, axis-center and follow-the-line target robot = rosys . ui . robot_object ( debug = True )","title":"Robot and Shape"},{"location":"features/3d_scene/#click-handler","text":"You can also pass a click handler to the 3d scene. Here is a full example example for driving to a point on the ground by starting the built-in automation called drive_to : #!/usr/bin/env python3 from rosys.automations import drive_to from nicegui import ui import rosys import rosys.ui from rosys.world import Point runtime = rosys . Runtime () rosys . ui . configure ( ui , runtime ) async def handle_click ( msg ): for hit in msg . hits : target = Point ( x = hit . point . x , y = hit . point . y ) runtime . automator . start ( drive_to ( runtime . world , runtime . hardware , target )) with ui . scene ( on_click = handle_click ) as scene : robot = rosys . ui . robot_object ( debug = True ) ui . label ( 'click into the scene to drive the robot' ) ui . run ( title = 'RoSys' , port = 8080 )","title":"Click Handler"},{"location":"features/automation_controls/","text":"Automation Controls \u00b6 Required for safety and good usability: automation processes only begin after the user actively requests it. Also the user should always be able to pause/resume and stop an ongoing automation. While you could write your own UI, RoSys already provides a ready-made set of elements with rosys.ui.automation_controls() . Building on the click handler from the 3d scene example we can add these easily: with ui . row (): rosys . ui . automation_controls () ui . label ( 'you can also pause/resume or stop the running automation' ) The ui.row() context arranged the control buttons in a row.","title":"Automation Controls"},{"location":"features/automation_controls/#automation-controls","text":"Required for safety and good usability: automation processes only begin after the user actively requests it. Also the user should always be able to pause/resume and stop an ongoing automation. While you could write your own UI, RoSys already provides a ready-made set of elements with rosys.ui.automation_controls() . Building on the click handler from the 3d scene example we can add these easily: with ui . row (): rosys . ui . automation_controls () ui . label ( 'you can also pause/resume or stop the running automation' ) The ui.row() context arranged the control buttons in a row.","title":"Automation Controls"},{"location":"features/cameras/","text":"Cameras \u00b6 RoSys provides instant camera access for object detection , remote operation and similar use cases. Any plugged in camera becomes an entry in world.usb_cameras containing recorded images and configuration parameters like exposure. Setup \u00b6 Camera devices are discovered through video4linux (v4l) and accessed with openCV. Therefore the program v4l2ctl and openCV (including python bindings) must be available. The simplest way is to use our RoSys docker image which provides the full required software stack. Make sure the container can access the usb devices by starting it with privileged or explicit passing the devices . Show Captured Images \u00b6 Through the use of rosys.ui (see User Interface ) you can launch a website which shows the latest captured images from each camera: #!/usr/bin/env python3 from nicegui import ui import rosys import rosys.ui # setup runtime = rosys . Runtime () rosys . ui . configure ( ui , runtime ) def refresh (): for uid , camera in runtime . world . usb_cameras . items (): if uid not in feeds : with ui . card () . tight () . style ( 'width:30em;' ): feeds [ uid ] = ui . image () feeds [ uid ] . set_source ( camera . latest_image_uri ) # refresh timer feeds = {} ui . timer ( 0.3 , refresh ) ui . run ( title = 'RoSys' , port = 8080 ) A timer updates the source property of the ui.image in a given interval. The cameras latest_image_uri property provides the uri to the latest captured image. Thereby the browser is always fetching new images. Remote Operation \u00b6 A fairly often required use case is the remote operation of a robot. In a simple use case you may only need to visualize one camera and have some steering controls. Here we use the event.Id.NEW_CAMERA to only display the first camera: async def add_main_camera ( camera : Camera ): camera_card . clear () # remove \"seeking cam\" label with camera_card : maincam = ui . image () ui . timer ( 1 , lambda : maincam . set_source ( camera . latest_image_uri )) rosys . event . unregister ( rosys . event . Id . NEW_CAMERA , add_main_camera ) # we only show the first cam with ui . card () . tight () . style ( 'width:30em;' ) as camera_card : ui . label ( 'seeking main camera' ) . style ( 'margin:1em' ) rosys . event . register ( rosys . event . Id . NEW_CAMERA , add_main_camera ) By adding joystick and keyboard_control the robot is ready to go for remote operation: with ui . card () . tight (): rosys . ui . joystick () ui . markdown ( 'use joystick on the left<br> or SHIFT + arrow keys to steer' ) . style ( 'margin:2em;text-align:center' ) rosys . ui . keyboard_control () ui . run ( title = 'RoSys' , port = 8080 )","title":"Cameras"},{"location":"features/cameras/#cameras","text":"RoSys provides instant camera access for object detection , remote operation and similar use cases. Any plugged in camera becomes an entry in world.usb_cameras containing recorded images and configuration parameters like exposure.","title":"Cameras"},{"location":"features/cameras/#setup","text":"Camera devices are discovered through video4linux (v4l) and accessed with openCV. Therefore the program v4l2ctl and openCV (including python bindings) must be available. The simplest way is to use our RoSys docker image which provides the full required software stack. Make sure the container can access the usb devices by starting it with privileged or explicit passing the devices .","title":"Setup"},{"location":"features/cameras/#show-captured-images","text":"Through the use of rosys.ui (see User Interface ) you can launch a website which shows the latest captured images from each camera: #!/usr/bin/env python3 from nicegui import ui import rosys import rosys.ui # setup runtime = rosys . Runtime () rosys . ui . configure ( ui , runtime ) def refresh (): for uid , camera in runtime . world . usb_cameras . items (): if uid not in feeds : with ui . card () . tight () . style ( 'width:30em;' ): feeds [ uid ] = ui . image () feeds [ uid ] . set_source ( camera . latest_image_uri ) # refresh timer feeds = {} ui . timer ( 0.3 , refresh ) ui . run ( title = 'RoSys' , port = 8080 ) A timer updates the source property of the ui.image in a given interval. The cameras latest_image_uri property provides the uri to the latest captured image. Thereby the browser is always fetching new images.","title":"Show Captured Images"},{"location":"features/cameras/#remote-operation","text":"A fairly often required use case is the remote operation of a robot. In a simple use case you may only need to visualize one camera and have some steering controls. Here we use the event.Id.NEW_CAMERA to only display the first camera: async def add_main_camera ( camera : Camera ): camera_card . clear () # remove \"seeking cam\" label with camera_card : maincam = ui . image () ui . timer ( 1 , lambda : maincam . set_source ( camera . latest_image_uri )) rosys . event . unregister ( rosys . event . Id . NEW_CAMERA , add_main_camera ) # we only show the first cam with ui . card () . tight () . style ( 'width:30em;' ) as camera_card : ui . label ( 'seeking main camera' ) . style ( 'margin:1em' ) rosys . event . register ( rosys . event . Id . NEW_CAMERA , add_main_camera ) By adding joystick and keyboard_control the robot is ready to go for remote operation: with ui . card () . tight (): rosys . ui . joystick () ui . markdown ( 'use joystick on the left<br> or SHIFT + arrow keys to steer' ) . style ( 'margin:2em;text-align:center' ) rosys . ui . keyboard_control () ui . run ( title = 'RoSys' , port = 8080 )","title":"Remote Operation"},{"location":"features/manual_steering/","text":"Manual Steering \u00b6 Keyboard Control \u00b6 By calling rosys.ui.keyboard_control() you enable steering the robot with the keyboard (see Getting Started for a full example). Press the arrow keys to steer the robot while holding the SHIFT key down. You can also modify the speed of the robot by pressing the a number key. Use the optional parameter default_speed to change the initial value. Joystick \u00b6 When operating from a mobile phone, you can use rosys.ui.joystick() to create a UI element with touch control. You can drive the robot by dragging the mouse inside the top left square:","title":"Manual Steering"},{"location":"features/manual_steering/#manual-steering","text":"","title":"Manual Steering"},{"location":"features/manual_steering/#keyboard-control","text":"By calling rosys.ui.keyboard_control() you enable steering the robot with the keyboard (see Getting Started for a full example). Press the arrow keys to steer the robot while holding the SHIFT key down. You can also modify the speed of the robot by pressing the a number key. Use the optional parameter default_speed to change the initial value.","title":"Keyboard Control"},{"location":"features/manual_steering/#joystick","text":"When operating from a mobile phone, you can use rosys.ui.joystick() to create a UI element with touch control. You can drive the robot by dragging the mouse inside the top left square:","title":"Joystick"},{"location":"features/navigation/","text":"Navigation \u00b6 Path Following \u00b6 When following a path, a \"carrot\" is dragged along a spline and the robot follows it like a donkey. Additionally, there is a virtual \"hook\" attached to the robot, which is pulled towards the carrot. There are three parameters: hook_offset : How far from the wheel axis (i.e. the coordinate center of the robot) is the hook, which is pulled towards the carrot. carrot_offset : How far ahead of the carrot is the robot pulled. This parameter is necessary in order to have the hook pulled a bit further, even though the carrot already reached the end of the spline. carrot_distance : How long is the \"thread\" between hook and carrot (or the offset point ahead of the carrot, respectively). In the following illustration these points are depicted as spheres: the coordinate center of the robot (blue, small), the hook (blue, large), carrot (orange, small), offset point ahead of the carrot (orange, large). Note The automation drive_spline has an optional argument flip_hook . It turns the hook 180 degrees to the back of the robot, while preserving the distance hook_offset to the robot's coordinate center. This allows the robot to drive backwards to a point behind it instead of turning around and approaching it forwards.","title":"Navigation"},{"location":"features/navigation/#navigation","text":"","title":"Navigation"},{"location":"features/navigation/#path-following","text":"When following a path, a \"carrot\" is dragged along a spline and the robot follows it like a donkey. Additionally, there is a virtual \"hook\" attached to the robot, which is pulled towards the carrot. There are three parameters: hook_offset : How far from the wheel axis (i.e. the coordinate center of the robot) is the hook, which is pulled towards the carrot. carrot_offset : How far ahead of the carrot is the robot pulled. This parameter is necessary in order to have the hook pulled a bit further, even though the carrot already reached the end of the spline. carrot_distance : How long is the \"thread\" between hook and carrot (or the offset point ahead of the carrot, respectively). In the following illustration these points are depicted as spheres: the coordinate center of the robot (blue, small), the hook (blue, large), carrot (orange, small), offset point ahead of the carrot (orange, large). Note The automation drive_spline has an optional argument flip_hook . It turns the hook 180 degrees to the back of the robot, while preserving the distance hook_offset to the robot's coordinate center. This allows the robot to drive backwards to a point behind it instead of turning around and approaching it forwards.","title":"Path Following"},{"location":"features/object_detection/","text":"Object Detection \u00b6 tbd.","title":"Object Detection"},{"location":"features/object_detection/#object-detection","text":"tbd.","title":"Object Detection"}]}