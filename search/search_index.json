{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RoSys - The Robot System \u00b6 RoSys provides an easy-to-use robot system. Its purpose is similar to ROS . But RoSys is fully based on modern web technologies and focusses on mobile robotics. See full documentation at rosys.io . Currently RoSys is mostly tested and developed on the Zauberzeug Robot Brain which uses Lizard for communication with motors, sensors and other peripherals. But the software architecture of RoSys also allows you to write your own modules if you prefer another industrial PC or setup. Principles \u00b6 All Python \u00b6 Business logic is wired in Python while computation-heavy tasks are encapsulated through websockets or bindings. Shared State \u00b6 All code can access and manipulate a shared and typesafe state -- this does not mean it should. Good software design is still necessary. But it is much easier to do if you do not have to perform serialization all the time. No Threading \u00b6 Thanks to asyncio you can write the business logic without locks and mutex mechanisms. The running system feels like everything is happening in parallel. But each code block is executed one after another through an event queue and yields execution as soon as it waits for I/O or heavy computation. The latter is still executed in threads to not block the rest of the business logic. Web UI \u00b6 Most machines need some kind of human interaction. We made sure your robot can be operated fully off the grid with any web browser by incorporating NiceGUI . It is also possible to proxy the user interface through a gateway for remote operation. Simulation \u00b6 Robot hardware is often slower than your own computer. Therefore RoSys supports a simulation mode for rapid development. To get maximum performance the current implementation does not run a full physics engine. Testing \u00b6 You can use pytest to write high-level integration tests. It is based on the above-described simulation mode and accelerates the robot's time for super fast execution. Architecture and Features \u00b6 Modules \u00b6 RoSys modules basically are Python modules encapsulate certain functionality. They can hold their own state, register lifecycle hooks, run methods repeatedly and subscribe to or raise events . Most modules depend on other modules. Lifecycle Hooks And Loops \u00b6 Modules can register functions for being called on_startup or on_shutdown as well as repeatedly with a given interval. Events \u00b6 Modules can provide events to allow coupling otherwise separated modules of the system. For example on module might read sensor data and raise an event NEW_SENSOR_DATA , without knowing of any consumers. Another module can register on NEW_SENSOR_DATA and act accordingly when being called. Automations \u00b6 RoSys provides an Automator module for running \"automations\". Automations are coroutines that can not only be started and stopped, but also paused and resumed, e.g. using AutomationControls . Persistence \u00b6 Modules can register backup and restore methods to read and write their state to disk. RoSys Time \u00b6 If you want to delay the execution, you should invoke await rosys.sleep(seconds: float) . This causes to wait until the RoSys time has elapsed the desired amount of time. In pytests the RoSys time is simulated and can advance much faster if no CPU-intensive operation is performed. Threading And Multiprocessing \u00b6 Not every piece of code is already using asyncio. The actor class provides convenience functions for IO and CPU bound work. IO Bound: If you need to read from an external device or use a non-async HTTP library like requests , you should wrap the code in a function and await it with await rosys.run.io_bound(...) . CPU Bound: If you need to do some heavy computation and want to spawn another process, you should wrap the code in a function and await it with await rosys.run.cpu_bound(...) . Safety \u00b6 Python is fast enough for most high level logic, but has no realtime guarantees. Safety-relevant behavior should therefore be written in Lizard and executed on a suitable microprocessor. The microprocessor governs the hardware of the robot and must be able to perform safety actions like triggering emergency hold etc. We suggest you use an industrial PC with an integrated controller like the Zauberzeug Robot Brain . It provides a Linux system with AI acceleration to run RoSys, two integrated ESP32 to run Lizard and six I/O sockets with up to 24 GPIOs for digital I/Os, CAN, RS485, SPI, I2C, ... with a software controllable ENABLE switch. User Interface \u00b6 RoSys plays very well with NiceGUI and provides additional robot-related UI elements. NiceGUI is a high-level web UI framework on top of JustPy . This means you can write all UI code in Python. The state is automatically reflected in the browser through WebSockets. RoSys can also be used with other user interfaces or interaction models if required, for example a completely app-based control through Bluetooth Low Energy with Flutter. Notifications \u00b6 Modules can notify the user through rosys.notify('message to the user') . When using NiceGUI, the notifications will show as snackbar messages. The history of notifications is stored in the list rosys.notifications .","title":"About"},{"location":"#rosys-the-robot-system","text":"RoSys provides an easy-to-use robot system. Its purpose is similar to ROS . But RoSys is fully based on modern web technologies and focusses on mobile robotics. See full documentation at rosys.io . Currently RoSys is mostly tested and developed on the Zauberzeug Robot Brain which uses Lizard for communication with motors, sensors and other peripherals. But the software architecture of RoSys also allows you to write your own modules if you prefer another industrial PC or setup.","title":"RoSys - The Robot System"},{"location":"#principles","text":"","title":"Principles"},{"location":"#all-python","text":"Business logic is wired in Python while computation-heavy tasks are encapsulated through websockets or bindings.","title":"All Python"},{"location":"#shared-state","text":"All code can access and manipulate a shared and typesafe state -- this does not mean it should. Good software design is still necessary. But it is much easier to do if you do not have to perform serialization all the time.","title":"Shared State"},{"location":"#no-threading","text":"Thanks to asyncio you can write the business logic without locks and mutex mechanisms. The running system feels like everything is happening in parallel. But each code block is executed one after another through an event queue and yields execution as soon as it waits for I/O or heavy computation. The latter is still executed in threads to not block the rest of the business logic.","title":"No Threading"},{"location":"#web-ui","text":"Most machines need some kind of human interaction. We made sure your robot can be operated fully off the grid with any web browser by incorporating NiceGUI . It is also possible to proxy the user interface through a gateway for remote operation.","title":"Web UI"},{"location":"#simulation","text":"Robot hardware is often slower than your own computer. Therefore RoSys supports a simulation mode for rapid development. To get maximum performance the current implementation does not run a full physics engine.","title":"Simulation"},{"location":"#testing","text":"You can use pytest to write high-level integration tests. It is based on the above-described simulation mode and accelerates the robot's time for super fast execution.","title":"Testing"},{"location":"#architecture-and-features","text":"","title":"Architecture and Features"},{"location":"#modules","text":"RoSys modules basically are Python modules encapsulate certain functionality. They can hold their own state, register lifecycle hooks, run methods repeatedly and subscribe to or raise events . Most modules depend on other modules.","title":"Modules"},{"location":"#lifecycle-hooks-and-loops","text":"Modules can register functions for being called on_startup or on_shutdown as well as repeatedly with a given interval.","title":"Lifecycle Hooks And Loops"},{"location":"#events","text":"Modules can provide events to allow coupling otherwise separated modules of the system. For example on module might read sensor data and raise an event NEW_SENSOR_DATA , without knowing of any consumers. Another module can register on NEW_SENSOR_DATA and act accordingly when being called.","title":"Events"},{"location":"#automations","text":"RoSys provides an Automator module for running \"automations\". Automations are coroutines that can not only be started and stopped, but also paused and resumed, e.g. using AutomationControls .","title":"Automations"},{"location":"#persistence","text":"Modules can register backup and restore methods to read and write their state to disk.","title":"Persistence"},{"location":"#rosys-time","text":"If you want to delay the execution, you should invoke await rosys.sleep(seconds: float) . This causes to wait until the RoSys time has elapsed the desired amount of time. In pytests the RoSys time is simulated and can advance much faster if no CPU-intensive operation is performed.","title":"RoSys Time"},{"location":"#threading-and-multiprocessing","text":"Not every piece of code is already using asyncio. The actor class provides convenience functions for IO and CPU bound work. IO Bound: If you need to read from an external device or use a non-async HTTP library like requests , you should wrap the code in a function and await it with await rosys.run.io_bound(...) . CPU Bound: If you need to do some heavy computation and want to spawn another process, you should wrap the code in a function and await it with await rosys.run.cpu_bound(...) .","title":"Threading And Multiprocessing"},{"location":"#safety","text":"Python is fast enough for most high level logic, but has no realtime guarantees. Safety-relevant behavior should therefore be written in Lizard and executed on a suitable microprocessor. The microprocessor governs the hardware of the robot and must be able to perform safety actions like triggering emergency hold etc. We suggest you use an industrial PC with an integrated controller like the Zauberzeug Robot Brain . It provides a Linux system with AI acceleration to run RoSys, two integrated ESP32 to run Lizard and six I/O sockets with up to 24 GPIOs for digital I/Os, CAN, RS485, SPI, I2C, ... with a software controllable ENABLE switch.","title":"Safety"},{"location":"#user-interface","text":"RoSys plays very well with NiceGUI and provides additional robot-related UI elements. NiceGUI is a high-level web UI framework on top of JustPy . This means you can write all UI code in Python. The state is automatically reflected in the browser through WebSockets. RoSys can also be used with other user interfaces or interaction models if required, for example a completely app-based control through Bluetooth Low Energy with Flutter.","title":"User Interface"},{"location":"#notifications","text":"Modules can notify the user through rosys.notify('message to the user') . When using NiceGUI, the notifications will show as snackbar messages. The history of notifications is stored in the list rosys.notifications .","title":"Notifications"},{"location":"development/","text":"Development \u00b6 Logging \u00b6 RoSys uses the Python logging package with namespaced loggers. For example, the steerer module writes its logs as rosys.steerer . This can be used for fine-granular control of what should show on the console. As a general starting point we suggest reading the Python Logging HOWTO . In the following examples we use Python's logging dictConfig for configuration, because it provides the most flexibility while having all configuration in one place. Show Info Messages \u00b6 To only print RoSys messages at the info level to the console we can use a configuration like this: #!/usr/bin/env python3 import logging import logging.config from nicegui import ui from rosys.driving import Joystick , Odometer , Steerer from rosys.hardware import WheelsSimulation logging . config . dictConfig ({ 'version' : 1 , 'disable_existing_loggers' : True , # to make sure this config is used 'formatters' : { 'default' : { 'format' : ' %(asctime)s - %(levelname)s - %(message)s ' , 'datefmt' : '%Y-%m- %d %H:%M:%S' , }, }, 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, }, 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' ], 'level' : 'INFO' , 'propagate' : False , }, }, }) wheels = WheelsSimulation () steerer = Steerer ( wheels ) odometer = Odometer ( wheels ) Joystick ( steerer ) ui . run ( title = 'RoSys' ) As you move the joystick, rosys.steerer messages will appear on the console: 2022-01-11 06:53:21 - INFO - start steering 2022-01-11 06:53:22 - INFO - stop steering 2022-01-11 06:53:23 - INFO - start steering 2022-01-11 06:53:23 - INFO - stop steering Adding Loggers \u00b6 You can easily add more loggers. For example, to see debug messages of the odometer you can add 'rosys.odometer' : { 'handlers' : [ 'console' ], 'level' : 'DEBUG' , 'propagate' : False , }, Most of the time we turn off log propagation to ensure the configuration we defined ourselves is really used. Logging to File \u00b6 Sometimes it is helpful to write intensive logging into a file and only show some messages on the console. For this you can add a file handler : 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, 'file' : { 'level' : 'DEBUG' , 'class' : 'logging.handlers.RotatingFileHandler' , 'formatter' : 'default' , 'filename' : os . path . expanduser ( '~/.rosys/example.log' ), 'maxBytes' : 1024 * 1000 , 'backupCount' : 3 } }, Then you can decide for each logger which handlers should be used: 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' , 'file' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' , 'file' ], 'level' : 'INFO' , 'propagate' : False , }, 'rosys.event' : { 'handlers' : [ 'file' ], 'level' : 'DEBUG' , 'propagate' : False , }, 'rosys.core' : { 'handlers' : [ 'file' ], 'level' : 'DEBUG' , 'propagate' : False , }, }, Note The above file logger writes to ~/.rosys . For development it is very helpful to have auto-reloading on file change activated . Therefore logging should always be stored outside of your project's source directory. Formatting \u00b6 It is quite useful to see from which file and line number a log entry was triggered. To keep the log lines from getting too long, you can create a log filter which computes the relative path: class PackagePathFilter ( logging . Filter ): '''Provides relative path for log formatter. Original code borrowed from https://stackoverflow.com/a/52582536/3419103 ''' def filter ( self , record : logging . LogRecord ) -> bool : pathname = record . pathname record . relative_path = None abs_sys_paths = map ( os . path . abspath , sys . path ) for path in sorted ( abs_sys_paths , key = len , reverse = True ): # longer paths first if not path . endswith ( os . sep ): path += os . sep if pathname . startswith ( path ): record . relative_path = os . path . relpath ( pathname , path ) break return True You need to register the filter and apply it in the handler. Then you can change the format for the formatter: 'filters' : { 'package_path_filter' : { '()' : PackagePathFilter , }, }, 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'filters' : [ 'package_path_filter' ], 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, }, 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' ], 'level' : 'INFO' , 'propagate' : False , }, }, Log output then looks like this: 2022-01-11 06:51:00.319 [DEBUG] rosys/runtime.py:78: startup completed Profiling \u00b6 You can add a profile decorator to expensive functions and add a profiler button to your UI: #!/usr/bin/env python3 import rosys from nicegui import ui from rosys.analysis import ProfileButton , profiling @profiling . profile def compute () -> None : s = 0 for i in range ( 1_000_000 ): s += i ** 2 ui . notify ( s ) rosys . on_repeat ( compute , 1.0 ) ProfileButton () ui . run () When the button is pressed, the profiler yappi will start recording data. When stopped, you will see its output on the console: Line # Hits Time Per Hit % Time Line Contents ============================================================== 7 @profiling.profile 8 def compute() -> None: 9 3 21.0 7.0 0.0 s = 0 10 3000003 433138.0 0.1 28.2 for i in range(1_000_000): 11 3000000 1098975.0 0.4 71.6 s += i**2 12 3 2151.0 717.0 0.1 ui.notify(s) Continuous Build \u00b6 We run our continuous integration with GitHub Actions. For each commit the pytests are executed. Releases \u00b6 We publish releases by creating a new version on GitHub and describe the changes. A GitHub Action then performs the following steps: If the pytests are successful, a poetry build and deployment to pypi is issued. A multi-arch docker image is built and pushed to Docker Hub .","title":"Development"},{"location":"development/#development","text":"","title":"Development"},{"location":"development/#logging","text":"RoSys uses the Python logging package with namespaced loggers. For example, the steerer module writes its logs as rosys.steerer . This can be used for fine-granular control of what should show on the console. As a general starting point we suggest reading the Python Logging HOWTO . In the following examples we use Python's logging dictConfig for configuration, because it provides the most flexibility while having all configuration in one place.","title":"Logging"},{"location":"development/#show-info-messages","text":"To only print RoSys messages at the info level to the console we can use a configuration like this: #!/usr/bin/env python3 import logging import logging.config from nicegui import ui from rosys.driving import Joystick , Odometer , Steerer from rosys.hardware import WheelsSimulation logging . config . dictConfig ({ 'version' : 1 , 'disable_existing_loggers' : True , # to make sure this config is used 'formatters' : { 'default' : { 'format' : ' %(asctime)s - %(levelname)s - %(message)s ' , 'datefmt' : '%Y-%m- %d %H:%M:%S' , }, }, 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, }, 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' ], 'level' : 'INFO' , 'propagate' : False , }, }, }) wheels = WheelsSimulation () steerer = Steerer ( wheels ) odometer = Odometer ( wheels ) Joystick ( steerer ) ui . run ( title = 'RoSys' ) As you move the joystick, rosys.steerer messages will appear on the console: 2022-01-11 06:53:21 - INFO - start steering 2022-01-11 06:53:22 - INFO - stop steering 2022-01-11 06:53:23 - INFO - start steering 2022-01-11 06:53:23 - INFO - stop steering","title":"Show Info Messages"},{"location":"development/#adding-loggers","text":"You can easily add more loggers. For example, to see debug messages of the odometer you can add 'rosys.odometer' : { 'handlers' : [ 'console' ], 'level' : 'DEBUG' , 'propagate' : False , }, Most of the time we turn off log propagation to ensure the configuration we defined ourselves is really used.","title":"Adding Loggers"},{"location":"development/#logging-to-file","text":"Sometimes it is helpful to write intensive logging into a file and only show some messages on the console. For this you can add a file handler : 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, 'file' : { 'level' : 'DEBUG' , 'class' : 'logging.handlers.RotatingFileHandler' , 'formatter' : 'default' , 'filename' : os . path . expanduser ( '~/.rosys/example.log' ), 'maxBytes' : 1024 * 1000 , 'backupCount' : 3 } }, Then you can decide for each logger which handlers should be used: 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' , 'file' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' , 'file' ], 'level' : 'INFO' , 'propagate' : False , }, 'rosys.event' : { 'handlers' : [ 'file' ], 'level' : 'DEBUG' , 'propagate' : False , }, 'rosys.core' : { 'handlers' : [ 'file' ], 'level' : 'DEBUG' , 'propagate' : False , }, }, Note The above file logger writes to ~/.rosys . For development it is very helpful to have auto-reloading on file change activated . Therefore logging should always be stored outside of your project's source directory.","title":"Logging to File"},{"location":"development/#formatting","text":"It is quite useful to see from which file and line number a log entry was triggered. To keep the log lines from getting too long, you can create a log filter which computes the relative path: class PackagePathFilter ( logging . Filter ): '''Provides relative path for log formatter. Original code borrowed from https://stackoverflow.com/a/52582536/3419103 ''' def filter ( self , record : logging . LogRecord ) -> bool : pathname = record . pathname record . relative_path = None abs_sys_paths = map ( os . path . abspath , sys . path ) for path in sorted ( abs_sys_paths , key = len , reverse = True ): # longer paths first if not path . endswith ( os . sep ): path += os . sep if pathname . startswith ( path ): record . relative_path = os . path . relpath ( pathname , path ) break return True You need to register the filter and apply it in the handler. Then you can change the format for the formatter: 'filters' : { 'package_path_filter' : { '()' : PackagePathFilter , }, }, 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'filters' : [ 'package_path_filter' ], 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, }, 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' ], 'level' : 'INFO' , 'propagate' : False , }, }, Log output then looks like this: 2022-01-11 06:51:00.319 [DEBUG] rosys/runtime.py:78: startup completed","title":"Formatting"},{"location":"development/#profiling","text":"You can add a profile decorator to expensive functions and add a profiler button to your UI: #!/usr/bin/env python3 import rosys from nicegui import ui from rosys.analysis import ProfileButton , profiling @profiling . profile def compute () -> None : s = 0 for i in range ( 1_000_000 ): s += i ** 2 ui . notify ( s ) rosys . on_repeat ( compute , 1.0 ) ProfileButton () ui . run () When the button is pressed, the profiler yappi will start recording data. When stopped, you will see its output on the console: Line # Hits Time Per Hit % Time Line Contents ============================================================== 7 @profiling.profile 8 def compute() -> None: 9 3 21.0 7.0 0.0 s = 0 10 3000003 433138.0 0.1 28.2 for i in range(1_000_000): 11 3000000 1098975.0 0.4 71.6 s += i**2 12 3 2151.0 717.0 0.1 ui.notify(s)","title":"Profiling"},{"location":"development/#continuous-build","text":"We run our continuous integration with GitHub Actions. For each commit the pytests are executed.","title":"Continuous Build"},{"location":"development/#releases","text":"We publish releases by creating a new version on GitHub and describe the changes. A GitHub Action then performs the following steps: If the pytests are successful, a poetry build and deployment to pypi is issued. A multi-arch docker image is built and pushed to Docker Hub .","title":"Releases"},{"location":"getting_started/","text":"Getting Started \u00b6 First install RoSys with pip or Docker. Then create a directory to host your code and put it under version control. Name your entry file main.py and add the following content: #!/usr/bin/env python3 from nicegui import ui from rosys.driving import KeyboardControl , Odometer , RobotObject , Steerer from rosys.geometry import Prism from rosys.hardware import WheelsSimulation # setup shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) # ui KeyboardControl ( steerer ) with ui . scene (): RobotObject ( shape , odometer ) ui . label ( 'hold SHIFT to steer with the keyboard arrow keys' ) # start ui . run ( title = 'RoSys' ) If you launch the program, your browser will open the url http://0.0.0.0:8080/ and present a 3d view: Explanation \u00b6 Imports \u00b6 The user interface is built with NiceGUI . The individual RoSys modules come in packages driving , geometry , hardware and others. Setup \u00b6 In this example we create a Steerer which needs an Odometer . Here we work without real hardware, so two wheels are simulated. Please see Hardware for an example which can actually be used on a mobile robot. For visualization purposes we also need the approximate robot shape. User Interface \u00b6 The user interface consists of keyboard control with access to the steerer as well as a 3D view of the scene. The latter only contains the RobotObject with the given shape. The robot pose is constantly updated from the odometer. See NiceGUI for more details about its API. Start \u00b6 NiceGUI provides a ui.run command which launches the web server and opens the corresponding web application. If you modify the code, a reload is triggered automatically. This is very convenient, but can be deactivated by passing reload=False .","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"First install RoSys with pip or Docker. Then create a directory to host your code and put it under version control. Name your entry file main.py and add the following content: #!/usr/bin/env python3 from nicegui import ui from rosys.driving import KeyboardControl , Odometer , RobotObject , Steerer from rosys.geometry import Prism from rosys.hardware import WheelsSimulation # setup shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) # ui KeyboardControl ( steerer ) with ui . scene (): RobotObject ( shape , odometer ) ui . label ( 'hold SHIFT to steer with the keyboard arrow keys' ) # start ui . run ( title = 'RoSys' ) If you launch the program, your browser will open the url http://0.0.0.0:8080/ and present a 3d view:","title":"Getting Started"},{"location":"getting_started/#explanation","text":"","title":"Explanation"},{"location":"getting_started/#imports","text":"The user interface is built with NiceGUI . The individual RoSys modules come in packages driving , geometry , hardware and others.","title":"Imports"},{"location":"getting_started/#setup","text":"In this example we create a Steerer which needs an Odometer . Here we work without real hardware, so two wheels are simulated. Please see Hardware for an example which can actually be used on a mobile robot. For visualization purposes we also need the approximate robot shape.","title":"Setup"},{"location":"getting_started/#user-interface","text":"The user interface consists of keyboard control with access to the steerer as well as a 3D view of the scene. The latter only contains the RobotObject with the given shape. The robot pose is constantly updated from the odometer. See NiceGUI for more details about its API.","title":"User Interface"},{"location":"getting_started/#start","text":"NiceGUI provides a ui.run command which launches the web server and opens the corresponding web application. If you modify the code, a reload is triggered automatically. This is very convenient, but can be deactivated by passing reload=False .","title":"Start"},{"location":"installation/","text":"Installation \u00b6 On Your Desktop \u00b6 python3 -m pip install rosys See Getting Started for what to do next. On The Robot \u00b6 While the above installation commands work in a well setup environment, it is often easier to run RoSys inside a docker container, especially on Nvidia Jetson devices with their old 18.04 LTS Ubuntu. Launching \u00b6 There are some specialities needed to start RoSys in different environments (Mac, Linux, Jetson, ...). To simplify the usage we wrapped this in a script called ./docker.sh which you can also use and adapt in your own project. Have a look at the examples to see how a setup of your own repository could look like. Remote Development \u00b6 You can develop quite a lot of functionality with a simulated robot on your own computer. But there comes a time when you want to run your code on a real robot. Normally you will therefore start the container on the Robot Brain and connect via WiFi to the web interface. By using VS Code Remote Containers you can continue development as if you are using your own computer. Unfortunately some robot hardware (for example Nvidia Jetson) is much much slower than your own machine. With a large code base this can result in long restart times after you change some code (30 seconds or more). By launching rosys/hardware/hardware_proxy.py on the Robot Brain you can keep developing on your computer while being connected to the hardware via WiFi. When the runtime is initialized, it will first try to find the ESP32 of the Robot Brain locally. If this does not work, it tries to reach the Robot Brain via the local WiFi connection. Only if this also fails, it will fallback on a simulated hardware system.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#on-your-desktop","text":"python3 -m pip install rosys See Getting Started for what to do next.","title":"On Your Desktop"},{"location":"installation/#on-the-robot","text":"While the above installation commands work in a well setup environment, it is often easier to run RoSys inside a docker container, especially on Nvidia Jetson devices with their old 18.04 LTS Ubuntu.","title":"On The Robot"},{"location":"installation/#launching","text":"There are some specialities needed to start RoSys in different environments (Mac, Linux, Jetson, ...). To simplify the usage we wrapped this in a script called ./docker.sh which you can also use and adapt in your own project. Have a look at the examples to see how a setup of your own repository could look like.","title":"Launching"},{"location":"installation/#remote-development","text":"You can develop quite a lot of functionality with a simulated robot on your own computer. But there comes a time when you want to run your code on a real robot. Normally you will therefore start the container on the Robot Brain and connect via WiFi to the web interface. By using VS Code Remote Containers you can continue development as if you are using your own computer. Unfortunately some robot hardware (for example Nvidia Jetson) is much much slower than your own machine. With a large code base this can result in long restart times after you change some code (30 seconds or more). By launching rosys/hardware/hardware_proxy.py on the Robot Brain you can keep developing on your computer while being connected to the hardware via WiFi. When the runtime is initialized, it will first try to find the ESP32 of the Robot Brain locally. If this does not work, it tries to reach the Robot Brain via the local WiFi connection. Only if this also fails, it will fallback on a simulated hardware system.","title":"Remote Development"},{"location":"troubleshooting/","text":"Troubleshooting \u00b6 Asyncio Warning \u00b6 While running RoSys you may see warnings similar to this one: 2021-10-31 15:08:04.040 [WARNING] asyncio: Executing <Task pending name='Task-255' coro=<handle_event() running at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:344> wait_for=<_GatheringFuture pending cb=[<TaskWakeupMethWrapper object at 0x7f7001f8e0>()] created at /usr/local/lib/python3.9/asyncio/tasks.py:705> created at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:261> took 0.238 seconds This means some coroutine is clogging the event loop for too long. In the above example it is a whopping 238 ms in which no other actor can do anything. This is an eternity when machine communication is expected to happen about every 10 ms. The warning also provides a (not so readable) hint where the time is consumed. The example above is one of the more frequent scenarios. It means some code inside a user interaction event handler (e.g. handle_event() in justpy.py ) is blocking. Try to figure out which UI event code is responsible by commenting out parts of your logic and try to reproduce the warning systematically.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"troubleshooting/#asyncio-warning","text":"While running RoSys you may see warnings similar to this one: 2021-10-31 15:08:04.040 [WARNING] asyncio: Executing <Task pending name='Task-255' coro=<handle_event() running at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:344> wait_for=<_GatheringFuture pending cb=[<TaskWakeupMethWrapper object at 0x7f7001f8e0>()] created at /usr/local/lib/python3.9/asyncio/tasks.py:705> created at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:261> took 0.238 seconds This means some coroutine is clogging the event loop for too long. In the above example it is a whopping 238 ms in which no other actor can do anything. This is an eternity when machine communication is expected to happen about every 10 ms. The warning also provides a (not so readable) hint where the time is consumed. The example above is one of the more frequent scenarios. It means some code inside a user interaction event handler (e.g. handle_event() in justpy.py ) is blocking. Try to figure out which UI event code is responsible by commenting out parts of your logic and try to reproduce the warning systematically.","title":"Asyncio Warning"},{"location":"examples/cameras/","text":"Cameras \u00b6 RoSys provides instant camera access for object detection, remote operation and similar use cases. Setup \u00b6 USB camera devices are discovered through video4linux (v4l) and accessed with openCV. Therefore the program v4l2ctl and openCV (including python bindings) must be available. We recommend to use the RoSys docker image which provides the full required software stack. Make sure the container can access the USB devices by starting it with --privileged or explicitly passing the specific --device s. Show Captured Images \u00b6 Using rosys.ui you can show the latest captured images from each camera: #!/usr/bin/env python3 from nicegui import ui from rosys.vision import UsbCameraProviderSimulation , camera_provider camera_provider = UsbCameraProviderSimulation () camera_provider . add_camera ( camera_provider . create_calibrated ( 'test_cam' , width = 800 , height = 600 )) def refresh () -> None : for uid , camera in camera_provider . cameras . items (): if uid not in feeds : feeds [ uid ] = ui . image () feeds [ uid ] . set_source ( camera_provider . get_latest_image_url ( camera )) feeds = {} ui . timer ( 0.3 , refresh ) ui . run ( title = 'RoSys' ) The ui.timer regularly updates the source property of the ui.image . The cameras latest_image_uri property provides the URI to the latest captured image. This example uses a UsbCameraProviderSimulation with a single simulated test camera. But you can replace the provider with a UsbCameraProviderHardware . Remote Operation \u00b6 A fairly often required use case on real mobile robots is the remote operation. In a simple use case you may only need to visualize one camera and have some steering controls. Here we use the NEW_CAMERA event to display the first camera to control real Hardware : #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Joystick , KeyboardControl , Odometer , Steerer from rosys.hardware import RobotBrain , SerialCommunication , WheelsHardware , WheelsSimulation from rosys.vision import Camera , UsbCameraProviderHardware , UsbCameraProviderSimulation if SerialCommunication . is_possible (): communication = SerialCommunication () robot_brain = RobotBrain ( communication ) wheels = WheelsHardware ( robot_brain ) camera_provider = UsbCameraProviderHardware () else : wheels = WheelsSimulation () camera_provider = UsbCameraProviderSimulation () camera_provider . restore = lambda _ : None # NOTE: disable persistence test_cam = camera_provider . create_calibrated ( 'test_cam' , width = 800 , height = 600 ) ui . on_startup ( lambda : camera_provider . add_camera ( test_cam )) steerer = Steerer ( wheels ) odometer = Odometer ( wheels ) async def add_main_camera ( camera : Camera ) -> None : camera_card . clear () # remove \"seeking camera\" label with camera_card : maincam = ui . image () ui . timer ( 1 , lambda : maincam . set_source ( camera_provider . get_latest_image_url ( camera ))) camera_provider . CAMERA_ADDED . register ( add_main_camera ) with ui . card () . tight () . style ( 'width:30em' ) as camera_card : ui . label ( 'seeking main camera' ) . classes ( 'm-8 text-center' ) with ui . card () . tight () . style ( 'width:30em' ): with ui . row (): with ui . card () . tight (): Joystick ( steerer ) KeyboardControl ( steerer ) ui . markdown ( 'steer with joystick on the left or<br />SHIFT + arrow keys' ) . classes ( 'm-8 text-center' ) ui . run ( title = 'RoSys' ) By adding a Joystick and KeyboardControl the robot is ready to go for remote operation.","title":"Cameras"},{"location":"examples/cameras/#cameras","text":"RoSys provides instant camera access for object detection, remote operation and similar use cases.","title":"Cameras"},{"location":"examples/cameras/#setup","text":"USB camera devices are discovered through video4linux (v4l) and accessed with openCV. Therefore the program v4l2ctl and openCV (including python bindings) must be available. We recommend to use the RoSys docker image which provides the full required software stack. Make sure the container can access the USB devices by starting it with --privileged or explicitly passing the specific --device s.","title":"Setup"},{"location":"examples/cameras/#show-captured-images","text":"Using rosys.ui you can show the latest captured images from each camera: #!/usr/bin/env python3 from nicegui import ui from rosys.vision import UsbCameraProviderSimulation , camera_provider camera_provider = UsbCameraProviderSimulation () camera_provider . add_camera ( camera_provider . create_calibrated ( 'test_cam' , width = 800 , height = 600 )) def refresh () -> None : for uid , camera in camera_provider . cameras . items (): if uid not in feeds : feeds [ uid ] = ui . image () feeds [ uid ] . set_source ( camera_provider . get_latest_image_url ( camera )) feeds = {} ui . timer ( 0.3 , refresh ) ui . run ( title = 'RoSys' ) The ui.timer regularly updates the source property of the ui.image . The cameras latest_image_uri property provides the URI to the latest captured image. This example uses a UsbCameraProviderSimulation with a single simulated test camera. But you can replace the provider with a UsbCameraProviderHardware .","title":"Show Captured Images"},{"location":"examples/cameras/#remote-operation","text":"A fairly often required use case on real mobile robots is the remote operation. In a simple use case you may only need to visualize one camera and have some steering controls. Here we use the NEW_CAMERA event to display the first camera to control real Hardware : #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Joystick , KeyboardControl , Odometer , Steerer from rosys.hardware import RobotBrain , SerialCommunication , WheelsHardware , WheelsSimulation from rosys.vision import Camera , UsbCameraProviderHardware , UsbCameraProviderSimulation if SerialCommunication . is_possible (): communication = SerialCommunication () robot_brain = RobotBrain ( communication ) wheels = WheelsHardware ( robot_brain ) camera_provider = UsbCameraProviderHardware () else : wheels = WheelsSimulation () camera_provider = UsbCameraProviderSimulation () camera_provider . restore = lambda _ : None # NOTE: disable persistence test_cam = camera_provider . create_calibrated ( 'test_cam' , width = 800 , height = 600 ) ui . on_startup ( lambda : camera_provider . add_camera ( test_cam )) steerer = Steerer ( wheels ) odometer = Odometer ( wheels ) async def add_main_camera ( camera : Camera ) -> None : camera_card . clear () # remove \"seeking camera\" label with camera_card : maincam = ui . image () ui . timer ( 1 , lambda : maincam . set_source ( camera_provider . get_latest_image_url ( camera ))) camera_provider . CAMERA_ADDED . register ( add_main_camera ) with ui . card () . tight () . style ( 'width:30em' ) as camera_card : ui . label ( 'seeking main camera' ) . classes ( 'm-8 text-center' ) with ui . card () . tight () . style ( 'width:30em' ): with ui . row (): with ui . card () . tight (): Joystick ( steerer ) KeyboardControl ( steerer ) ui . markdown ( 'steer with joystick on the left or<br />SHIFT + arrow keys' ) . classes ( 'm-8 text-center' ) ui . run ( title = 'RoSys' ) By adding a Joystick and KeyboardControl the robot is ready to go for remote operation.","title":"Remote Operation"},{"location":"examples/click-and-drive/","text":"Click-and-drive \u00b6 In this example we create a simulated robot that drives wherever the user clicks. #!/usr/bin/env python3 from nicegui import ui from rosys.automation import AutomationControls , Automator from rosys.driving import Driver , Odometer , RobotObject from rosys.geometry import Point , Prism from rosys.hardware import WheelsSimulation shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) driver = Driver ( wheels , odometer ) automator = Automator ( wheels , None ) async def handle_click ( msg ): for hit in msg . hits : if hit . object_id == 'ground' : target = Point ( x = hit . point . x , y = hit . point . y ) automator . start ( driver . drive_to ( target )) with ui . scene ( on_click = handle_click ): RobotObject ( shape , odometer , debug = True ) ui . label ( 'click into the scene to drive the robot' ) with ui . row (): AutomationControls ( automator ) ui . label ( 'you can also pause/resume or stop the running automation' ) ui . run ( title = 'RoSys' ) Modules Besides wheels, odometer and a robot shape we need a driver that enables the robot to drive along a given path as well as an automator to start and stop such an automated behavior. Click handler NiceGUI's 3D scene allows registering a click handler that can iterate through all hit points and find the target on the ground. Driver Among others, the driver has an async method drive_to which lets the robot follow a straight line to a given target. Automator and automation controls The automator starts the async method and allows pausing, resuming and stopping it, e.g. with the AutomationControls UI element.","title":"Click-and-drive"},{"location":"examples/click-and-drive/#click-and-drive","text":"In this example we create a simulated robot that drives wherever the user clicks. #!/usr/bin/env python3 from nicegui import ui from rosys.automation import AutomationControls , Automator from rosys.driving import Driver , Odometer , RobotObject from rosys.geometry import Point , Prism from rosys.hardware import WheelsSimulation shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) driver = Driver ( wheels , odometer ) automator = Automator ( wheels , None ) async def handle_click ( msg ): for hit in msg . hits : if hit . object_id == 'ground' : target = Point ( x = hit . point . x , y = hit . point . y ) automator . start ( driver . drive_to ( target )) with ui . scene ( on_click = handle_click ): RobotObject ( shape , odometer , debug = True ) ui . label ( 'click into the scene to drive the robot' ) with ui . row (): AutomationControls ( automator ) ui . label ( 'you can also pause/resume or stop the running automation' ) ui . run ( title = 'RoSys' ) Modules Besides wheels, odometer and a robot shape we need a driver that enables the robot to drive along a given path as well as an automator to start and stop such an automated behavior. Click handler NiceGUI's 3D scene allows registering a click handler that can iterate through all hit points and find the target on the ground. Driver Among others, the driver has an async method drive_to which lets the robot follow a straight line to a given target. Automator and automation controls The automator starts the async method and allows pausing, resuming and stopping it, e.g. with the AutomationControls UI element.","title":"Click-and-drive"},{"location":"examples/hardware/","text":"Hardware \u00b6 You can use SerialCommunication.is_possible() to automatically switch between a simulation and real hardware. The module WheelsHardware expects a RobotBrain , which controls the SerialCommunication with a microcontroller. The Zauberzeug Robot Brain is an industrial-grade controller to combine artificial intelligence with machinery. #!/usr/bin/env python3 from nicegui import ui from rosys.driving import KeyboardControl , Odometer , Steerer from rosys.hardware import RobotBrain , SerialCommunication , WheelsHardware , WheelsSimulation , communication is_real = SerialCommunication . is_possible () if is_real : communication = SerialCommunication () robot_brain = RobotBrain ( communication ) wheels = WheelsHardware ( robot_brain ) else : wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) KeyboardControl ( steerer ) if is_real : communication . debug_ui () async def configure (): await robot_brain . configure ( 'example_hardware.txt' ) ui . button ( 'Configure Lizard' , on_click = configure ) . props ( 'outline' ) ui . run ( title = 'RoSys' ) With communication.debug_ui() you can add some helpful UI elements for debugging the serial communication. Furthermore, you can add a button to send the Lizard configuration to the microcontroller running the Lizard firmware. This way it recognizes wheel commands from the RobotBrain module and responds accordingly. The lizard configuration for two wheels controlled with an ODrive might look as follows: can = Can(32, 33, 1000000) l = ODriveMotor(can, 0x000) r = ODriveMotor(can, 0x100) l.m_per_tick = 0.0627 r.m_per_tick = 0.0627 wheels = ODriveWheels(l, r) wheels.width = 0.515 core.output(\"core.millis wheels.linear_speed:3 wheels.angular_speed:3\")","title":"Hardware"},{"location":"examples/hardware/#hardware","text":"You can use SerialCommunication.is_possible() to automatically switch between a simulation and real hardware. The module WheelsHardware expects a RobotBrain , which controls the SerialCommunication with a microcontroller. The Zauberzeug Robot Brain is an industrial-grade controller to combine artificial intelligence with machinery. #!/usr/bin/env python3 from nicegui import ui from rosys.driving import KeyboardControl , Odometer , Steerer from rosys.hardware import RobotBrain , SerialCommunication , WheelsHardware , WheelsSimulation , communication is_real = SerialCommunication . is_possible () if is_real : communication = SerialCommunication () robot_brain = RobotBrain ( communication ) wheels = WheelsHardware ( robot_brain ) else : wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) KeyboardControl ( steerer ) if is_real : communication . debug_ui () async def configure (): await robot_brain . configure ( 'example_hardware.txt' ) ui . button ( 'Configure Lizard' , on_click = configure ) . props ( 'outline' ) ui . run ( title = 'RoSys' ) With communication.debug_ui() you can add some helpful UI elements for debugging the serial communication. Furthermore, you can add a button to send the Lizard configuration to the microcontroller running the Lizard firmware. This way it recognizes wheel commands from the RobotBrain module and responds accordingly. The lizard configuration for two wheels controlled with an ODrive might look as follows: can = Can(32, 33, 1000000) l = ODriveMotor(can, 0x000) r = ODriveMotor(can, 0x100) l.m_per_tick = 0.0627 r.m_per_tick = 0.0627 wheels = ODriveWheels(l, r) wheels.width = 0.515 core.output(\"core.millis wheels.linear_speed:3 wheels.angular_speed:3\")","title":"Hardware"},{"location":"examples/navigation/","text":"Navigation \u00b6 This example is similar to Click-and-drive but includes a PathPlanner to find a path around an obstacle. #!/usr/bin/env python3 from nicegui import ui from rosys.automation import Automator from rosys.driving import Driver , Odometer , RobotObject from rosys.geometry import Point , Pose , Prism from rosys.hardware import WheelsSimulation from rosys.pathplanning import Obstacle , ObstacleObject , PathObject , PathPlanner shape = Prism . default_robot_shape () path_planner = PathPlanner ( shape ) path_planner . restore = lambda _ : None # NOTE: disable persistence path_planner . obstacles [ '0' ] = Obstacle ( id = '0' , outline = [ Point ( x = 3 , y = 0 ), Point ( x = 0 , y = 3 ), Point ( x = 3 , y = 3 )]) wheels = WheelsSimulation () odometer = Odometer ( wheels ) driver = Driver ( wheels , odometer ) automator = Automator ( wheels , None ) async def handle_click ( msg ): for hit in msg . hits : if hit . object_id == 'ground' : yaw = odometer . prediction . point . direction ( hit . point ) goal = Pose ( x = hit . point . x , y = hit . point . y , yaw = yaw ) path = await path_planner . search ( start = odometer . prediction , goal = goal ) path3d . update ( path ) automator . start ( driver . drive_path ( path )) with ui . scene ( on_click = handle_click , width = 600 ): RobotObject ( shape , odometer ) ObstacleObject ( path_planner . obstacles ) path3d = PathObject () ui . label ( 'click into the scene to drive the robot' ) ui . run ( title = 'RoSys' ) Geometry \u00b6 When following a path, a \"carrot\" is dragged along a spline and the robot follows it like a donkey. Additionally, there is a virtual \"hook\" attached to the robot, which is pulled towards the carrot. There are three parameters: hook_offset : How far from the wheel axis (i.e. the coordinate center of the robot) is the hook, which is pulled towards the carrot. carrot_offset : How far ahead of the carrot is the robot pulled. This parameter is necessary in order to have the hook pulled a bit further, even though the carrot already reached the end of the spline. carrot_distance : How long is the \"thread\" between hook and carrot (or the offset point ahead of the carrot, respectively). In the following illustration these points are depicted as spheres: the coordinate center of the robot (blue, small), the hook (blue, large), carrot (orange, small), offset point ahead of the carrot (orange, large). Note The automation drive_spline has an optional argument flip_hook . It turns the hook 180 degrees to the back of the robot, while preserving the distance hook_offset to the robot's coordinate center. This allows the robot to drive backwards to a point behind it instead of turning around and approaching it forwards.","title":"Navigation"},{"location":"examples/navigation/#navigation","text":"This example is similar to Click-and-drive but includes a PathPlanner to find a path around an obstacle. #!/usr/bin/env python3 from nicegui import ui from rosys.automation import Automator from rosys.driving import Driver , Odometer , RobotObject from rosys.geometry import Point , Pose , Prism from rosys.hardware import WheelsSimulation from rosys.pathplanning import Obstacle , ObstacleObject , PathObject , PathPlanner shape = Prism . default_robot_shape () path_planner = PathPlanner ( shape ) path_planner . restore = lambda _ : None # NOTE: disable persistence path_planner . obstacles [ '0' ] = Obstacle ( id = '0' , outline = [ Point ( x = 3 , y = 0 ), Point ( x = 0 , y = 3 ), Point ( x = 3 , y = 3 )]) wheels = WheelsSimulation () odometer = Odometer ( wheels ) driver = Driver ( wheels , odometer ) automator = Automator ( wheels , None ) async def handle_click ( msg ): for hit in msg . hits : if hit . object_id == 'ground' : yaw = odometer . prediction . point . direction ( hit . point ) goal = Pose ( x = hit . point . x , y = hit . point . y , yaw = yaw ) path = await path_planner . search ( start = odometer . prediction , goal = goal ) path3d . update ( path ) automator . start ( driver . drive_path ( path )) with ui . scene ( on_click = handle_click , width = 600 ): RobotObject ( shape , odometer ) ObstacleObject ( path_planner . obstacles ) path3d = PathObject () ui . label ( 'click into the scene to drive the robot' ) ui . run ( title = 'RoSys' )","title":"Navigation"},{"location":"examples/navigation/#geometry","text":"When following a path, a \"carrot\" is dragged along a spline and the robot follows it like a donkey. Additionally, there is a virtual \"hook\" attached to the robot, which is pulled towards the carrot. There are three parameters: hook_offset : How far from the wheel axis (i.e. the coordinate center of the robot) is the hook, which is pulled towards the carrot. carrot_offset : How far ahead of the carrot is the robot pulled. This parameter is necessary in order to have the hook pulled a bit further, even though the carrot already reached the end of the spline. carrot_distance : How long is the \"thread\" between hook and carrot (or the offset point ahead of the carrot, respectively). In the following illustration these points are depicted as spheres: the coordinate center of the robot (blue, small), the hook (blue, large), carrot (orange, small), offset point ahead of the carrot (orange, large). Note The automation drive_spline has an optional argument flip_hook . It turns the hook 180 degrees to the back of the robot, while preserving the distance hook_offset to the robot's coordinate center. This allows the robot to drive backwards to a point behind it instead of turning around and approaching it forwards.","title":"Geometry"},{"location":"examples/steering/","text":"Steering \u00b6 The following example simulates a robot that can be steered using keyboard controls or a joystick. #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Joystick , KeyboardControl , Odometer , RobotObject , Steerer from rosys.geometry import Prism from rosys.hardware import WheelsSimulation shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) KeyboardControl ( steerer ) Joystick ( steerer , size = 50 , color = 'blue' ) with ui . scene (): RobotObject ( shape , odometer ) ui . run ( title = 'RoSys' ) Keyboard Control By adding a KeyboardControl to the user interface you enable steering the robot with the keyboard. Press the arrow keys while holding the SHIFT key to steer the robot. You can also modify the speed of the robot by pressing the a number key. Use the optional parameter default_speed to change the initial value. Joystick When operating from a mobile phone, you can use a Joystick to create a UI element with touch control. You can drive the robot by dragging the mouse inside the top left square.","title":"Steering"},{"location":"examples/steering/#steering","text":"The following example simulates a robot that can be steered using keyboard controls or a joystick. #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Joystick , KeyboardControl , Odometer , RobotObject , Steerer from rosys.geometry import Prism from rosys.hardware import WheelsSimulation shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) KeyboardControl ( steerer ) Joystick ( steerer , size = 50 , color = 'blue' ) with ui . scene (): RobotObject ( shape , odometer ) ui . run ( title = 'RoSys' ) Keyboard Control By adding a KeyboardControl to the user interface you enable steering the robot with the keyboard. Press the arrow keys while holding the SHIFT key to steer the robot. You can also modify the speed of the robot by pressing the a number key. Use the optional parameter default_speed to change the initial value. Joystick When operating from a mobile phone, you can use a Joystick to create a UI element with touch control. You can drive the robot by dragging the mouse inside the top left square.","title":"Steering"},{"location":"reference/SUMMARY/","text":"","title":"Module Reference"}]}