{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RoSys - The Robot System \u00b6 RoSys provides an easy-to-use robot system. Its purpose is similar to ROS . But RoSys is fully based on modern web technologies and focusses on mobile robotics. See full documentation at rosys.io . Currently RoSys is mostly tested and developed on the Zauberzeug Robot Brain which uses Lizard for communication with motors, sensors and other peripherals. But the software architecture of RoSys also allows you to write your own modules if you prefer another industrial PC or setup. Principles \u00b6 All Python \u00b6 Business logic is wired in Python while computation-heavy tasks are encapsulated through websockets or bindings. Shared State \u00b6 All code can access and manipulate a shared and typesafe state -- this does not mean it should. Good software design is still necessary. But it is much easier to do if you do not have to perform serialization all the time. No Threading \u00b6 Thanks to asyncio you can write the business logic without locks and mutex mechanisms. The running system feels like everything is happening in parallel. But each code block is executed one after another through an event queue and yields execution as soon as it waits for I/O or heavy computation. The latter is still executed in threads to not block the rest of the business logic. Web UI \u00b6 Most machines need some kind of human interaction. We made sure your robot can be operated fully off the grid with any web browser by incorporating NiceGUI . It is also possible to proxy the user interface through a gateway for remote operation. Simulation \u00b6 Robot hardware is often slower than your own computer. Therefore RoSys supports a simulation mode for rapid development. To get maximum performance the current implementation does not run a full physics engine. Testing \u00b6 You can use pytest to write high-level integration tests. It is based on the above-described simulation mode and accelerates the robot's time for super fast execution. Architecture and Features \u00b6 Modules \u00b6 RoSys modules basically are Python modules encapsulate certain functionality. They can hold their own state, register lifecycle hooks, run methods repeatedly and subscribe to or raise events . Most modules depend on other modules. Lifecycle Hooks And Loops \u00b6 Modules can register functions for being called on_startup or on_shutdown as well as repeatedly with a given interval. Events \u00b6 Modules can provide events to allow coupling otherwise separated modules of the system. For example on module might read sensor data and raise an event NEW_SENSOR_DATA , without knowing of any consumers. Another module can register on NEW_SENSOR_DATA and act accordingly when being called. Automations \u00b6 RoSys provides an Automator module for running \"automations\". Automations are coroutines that can not only be started and stopped, but also paused and resumed, e.g. using AutomationControls . Persistence \u00b6 Modules can register backup and restore methods to read and write their state to disk. RoSys Time \u00b6 If you want to delay the execution, you should invoke await rosys.sleep(seconds: float) . This causes to wait until the RoSys time has elapsed the desired amount of time. In pytests the RoSys time is simulated and can advance much faster if no CPU-intensive operation is performed. Threading And Multiprocessing \u00b6 Not every piece of code is already using asyncio. The actor class provides convenience functions for IO and CPU bound work. IO Bound: If you need to read from an external device or use a non-async HTTP library like requests , you should wrap the code in a function and await it with await rosys.run.io_bound(...) . CPU Bound: If you need to do some heavy computation and want to spawn another process, you should wrap the code in a function and await it with await rosys.run.cpu_bound(...) . Safety \u00b6 Python is fast enough for most high level logic, but has no realtime guarantees. Safety-relevant behavior should therefore be written in Lizard and executed on a suitable microprocessor. The microprocessor governs the hardware of the robot and must be able to perform safety actions like triggering emergency hold etc. We suggest you use an industrial PC like the Zauberzeug Robot Brain . It provides a Linux system with AI acceleration to run RoSys, an integrated ESP32 to run Lizard and six I/O sockets for CAN, RS485, SPI, I2C, ... with a software controllable ENABLE switch. User Interface \u00b6 RoSys plays very well with NiceGUI and provides additional robot-related UI elements. NiceGUI is a high-level web UI framework on top of JustPy . This means you can write all UI code in Python. The state is automatically reflected in the browser through WebSockets. RoSys can also be used with other user interfaces or interaction models if required, for example a completely app-based control through Bluetooth Low Energy with Flutter. Notifications \u00b6 Modules can notify the user through rosys.notify('message to the user') . When using NiceGUI, the notifications will show as snackbar messages. The history of notifications is stored in the list rosys.notifications .","title":"About"},{"location":"#rosys-the-robot-system","text":"RoSys provides an easy-to-use robot system. Its purpose is similar to ROS . But RoSys is fully based on modern web technologies and focusses on mobile robotics. See full documentation at rosys.io . Currently RoSys is mostly tested and developed on the Zauberzeug Robot Brain which uses Lizard for communication with motors, sensors and other peripherals. But the software architecture of RoSys also allows you to write your own modules if you prefer another industrial PC or setup.","title":"RoSys - The Robot System"},{"location":"#principles","text":"","title":"Principles"},{"location":"#all-python","text":"Business logic is wired in Python while computation-heavy tasks are encapsulated through websockets or bindings.","title":"All Python"},{"location":"#shared-state","text":"All code can access and manipulate a shared and typesafe state -- this does not mean it should. Good software design is still necessary. But it is much easier to do if you do not have to perform serialization all the time.","title":"Shared State"},{"location":"#no-threading","text":"Thanks to asyncio you can write the business logic without locks and mutex mechanisms. The running system feels like everything is happening in parallel. But each code block is executed one after another through an event queue and yields execution as soon as it waits for I/O or heavy computation. The latter is still executed in threads to not block the rest of the business logic.","title":"No Threading"},{"location":"#web-ui","text":"Most machines need some kind of human interaction. We made sure your robot can be operated fully off the grid with any web browser by incorporating NiceGUI . It is also possible to proxy the user interface through a gateway for remote operation.","title":"Web UI"},{"location":"#simulation","text":"Robot hardware is often slower than your own computer. Therefore RoSys supports a simulation mode for rapid development. To get maximum performance the current implementation does not run a full physics engine.","title":"Simulation"},{"location":"#testing","text":"You can use pytest to write high-level integration tests. It is based on the above-described simulation mode and accelerates the robot's time for super fast execution.","title":"Testing"},{"location":"#architecture-and-features","text":"","title":"Architecture and Features"},{"location":"#modules","text":"RoSys modules basically are Python modules encapsulate certain functionality. They can hold their own state, register lifecycle hooks, run methods repeatedly and subscribe to or raise events . Most modules depend on other modules.","title":"Modules"},{"location":"#lifecycle-hooks-and-loops","text":"Modules can register functions for being called on_startup or on_shutdown as well as repeatedly with a given interval.","title":"Lifecycle Hooks And Loops"},{"location":"#events","text":"Modules can provide events to allow coupling otherwise separated modules of the system. For example on module might read sensor data and raise an event NEW_SENSOR_DATA , without knowing of any consumers. Another module can register on NEW_SENSOR_DATA and act accordingly when being called.","title":"Events"},{"location":"#automations","text":"RoSys provides an Automator module for running \"automations\". Automations are coroutines that can not only be started and stopped, but also paused and resumed, e.g. using AutomationControls .","title":"Automations"},{"location":"#persistence","text":"Modules can register backup and restore methods to read and write their state to disk.","title":"Persistence"},{"location":"#rosys-time","text":"If you want to delay the execution, you should invoke await rosys.sleep(seconds: float) . This causes to wait until the RoSys time has elapsed the desired amount of time. In pytests the RoSys time is simulated and can advance much faster if no CPU-intensive operation is performed.","title":"RoSys Time"},{"location":"#threading-and-multiprocessing","text":"Not every piece of code is already using asyncio. The actor class provides convenience functions for IO and CPU bound work. IO Bound: If you need to read from an external device or use a non-async HTTP library like requests , you should wrap the code in a function and await it with await rosys.run.io_bound(...) . CPU Bound: If you need to do some heavy computation and want to spawn another process, you should wrap the code in a function and await it with await rosys.run.cpu_bound(...) .","title":"Threading And Multiprocessing"},{"location":"#safety","text":"Python is fast enough for most high level logic, but has no realtime guarantees. Safety-relevant behavior should therefore be written in Lizard and executed on a suitable microprocessor. The microprocessor governs the hardware of the robot and must be able to perform safety actions like triggering emergency hold etc. We suggest you use an industrial PC like the Zauberzeug Robot Brain . It provides a Linux system with AI acceleration to run RoSys, an integrated ESP32 to run Lizard and six I/O sockets for CAN, RS485, SPI, I2C, ... with a software controllable ENABLE switch.","title":"Safety"},{"location":"#user-interface","text":"RoSys plays very well with NiceGUI and provides additional robot-related UI elements. NiceGUI is a high-level web UI framework on top of JustPy . This means you can write all UI code in Python. The state is automatically reflected in the browser through WebSockets. RoSys can also be used with other user interfaces or interaction models if required, for example a completely app-based control through Bluetooth Low Energy with Flutter.","title":"User Interface"},{"location":"#notifications","text":"Modules can notify the user through rosys.notify('message to the user') . When using NiceGUI, the notifications will show as snackbar messages. The history of notifications is stored in the list rosys.notifications .","title":"Notifications"},{"location":"development/","text":"Development \u00b6 Logging \u00b6 RoSys uses the Python logging package with namespaced loggers. For example, the steerer module writes its logs as rosys.steerer . This can be used for fine-granular control of what should show on the console. As a general starting point we suggest reading the Python Logging HOWTO . In the following examples we use Python's logging dictConfig for configuration, because it provides the most flexibility while having all configuration in one place. Show Info Messages \u00b6 To only print RoSys messages at the info level to the console we can use a configuration like this: #!/usr/bin/env python3 import logging import logging.config from nicegui import ui from rosys.driving import Joystick , Odometer , Steerer from rosys.hardware import WheelsSimulation logging . config . dictConfig ({ 'version' : 1 , 'disable_existing_loggers' : True , # to make sure this config is used 'formatters' : { 'default' : { 'format' : ' %(asctime)s - %(levelname)s - %(message)s ' , 'datefmt' : '%Y-%m- %d %H:%M:%S' , }, }, 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, }, 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' ], 'level' : 'INFO' , 'propagate' : False , }, }, }) wheels = WheelsSimulation () steerer = Steerer ( wheels ) odometer = Odometer ( wheels ) Joystick ( steerer ) ui . run ( title = 'RoSys' ) As you move the joystick, rosys.steerer messages will appear on the console: 2022-01-11 06:53:21 - INFO - start steering 2022-01-11 06:53:22 - INFO - stop steering 2022-01-11 06:53:23 - INFO - start steering 2022-01-11 06:53:23 - INFO - stop steering Adding Loggers \u00b6 You can easily add more loggers. For example, to see debug messages of the odometer you can add 'rosys.odometer' : { 'handlers' : [ 'console' ], 'level' : 'DEBUG' , 'propagate' : False , }, Most of the time we turn off log propagation to ensure the configuration we defined ourselves is really used. Logging to File \u00b6 Sometimes it is helpful to write intensive logging into a file and only show some messages on the console. For this you can add a file handler : 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, 'file' : { 'level' : 'DEBUG' , 'class' : 'logging.handlers.RotatingFileHandler' , 'formatter' : 'default' , 'filename' : os . path . expanduser ( '~/.rosys/example.log' ), 'maxBytes' : 1024 * 1000 , 'backupCount' : 3 } }, Then you can decide for each logger which handlers should be used: 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' , 'file' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' , 'file' ], 'level' : 'INFO' , 'propagate' : False , }, 'rosys.event' : { 'handlers' : [ 'file' ], 'level' : 'DEBUG' , 'propagate' : False , }, 'rosys.core' : { 'handlers' : [ 'file' ], 'level' : 'DEBUG' , 'propagate' : False , }, }, Note The above file logger writes to ~/.rosys . For development it is very helpful to have auto-reloading on file change activated . Therefore logging should always be stored outside of your project's source directory. Formatting \u00b6 It is quite useful to see from which file and line number a log entry was triggered. To keep the log lines from getting too long, you can create a log filter which computes the relative path: class PackagePathFilter ( logging . Filter ): '''Provides relative path for log formatter. Original code borrowed from https://stackoverflow.com/a/52582536/3419103 ''' def filter ( self , record : logging . LogRecord ) -> bool : pathname = record . pathname record . relative_path = None abs_sys_paths = map ( os . path . abspath , sys . path ) for path in sorted ( abs_sys_paths , key = len , reverse = True ): # longer paths first if not path . endswith ( os . sep ): path += os . sep if pathname . startswith ( path ): record . relative_path = os . path . relpath ( pathname , path ) break return True You need to register the filter and apply it in the handler. Then you can change the format for the formatter: 'filters' : { 'package_path_filter' : { '()' : PackagePathFilter , }, }, 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'filters' : [ 'package_path_filter' ], 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, }, 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' ], 'level' : 'INFO' , 'propagate' : False , }, }, Log output then looks like this: 2022-01-11 06:51:00.319 [DEBUG] rosys/runtime.py:78: startup completed Profiling \u00b6 You can add a profile decorator to expensive functions and add a profiler button to your UI: #!/usr/bin/env python3 import rosys from nicegui import ui from rosys.debugging import ProfileButton , profiling @profiling . profile def compute () -> None : s = 0 for i in range ( 1_000_000 ): s += i ** 2 ui . notify ( s ) rosys . on_repeat ( compute , 1.0 ) ProfileButton () ui . run () When the button is pressed, the profiler yappi will start recording data. When stopped, you will see its output on the console: Line # Hits Time Per Hit % Time Line Contents ============================================================== 7 @profiling.profile 8 def compute() -> None: 9 3 21.0 7.0 0.0 s = 0 10 3000003 433138.0 0.1 28.2 for i in range(1_000_000): 11 3000000 1098975.0 0.4 71.6 s += i**2 12 3 2151.0 717.0 0.1 ui.notify(s) Continuous Build \u00b6 We run our continuous integration with GitHub Actions. For each commit the pytests are executed. Releases \u00b6 We publish releases by creating a new version on GitHub and describe the changes. A GitHub Action then performs the following steps: If the pytests are successful, a poetry build and deployment to pypi is issued. A multi-arch docker image is built and pushed to Docker Hub .","title":"Development"},{"location":"development/#development","text":"","title":"Development"},{"location":"development/#logging","text":"RoSys uses the Python logging package with namespaced loggers. For example, the steerer module writes its logs as rosys.steerer . This can be used for fine-granular control of what should show on the console. As a general starting point we suggest reading the Python Logging HOWTO . In the following examples we use Python's logging dictConfig for configuration, because it provides the most flexibility while having all configuration in one place.","title":"Logging"},{"location":"development/#show-info-messages","text":"To only print RoSys messages at the info level to the console we can use a configuration like this: #!/usr/bin/env python3 import logging import logging.config from nicegui import ui from rosys.driving import Joystick , Odometer , Steerer from rosys.hardware import WheelsSimulation logging . config . dictConfig ({ 'version' : 1 , 'disable_existing_loggers' : True , # to make sure this config is used 'formatters' : { 'default' : { 'format' : ' %(asctime)s - %(levelname)s - %(message)s ' , 'datefmt' : '%Y-%m- %d %H:%M:%S' , }, }, 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, }, 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' ], 'level' : 'INFO' , 'propagate' : False , }, }, }) wheels = WheelsSimulation () steerer = Steerer ( wheels ) odometer = Odometer ( wheels ) Joystick ( steerer ) ui . run ( title = 'RoSys' ) As you move the joystick, rosys.steerer messages will appear on the console: 2022-01-11 06:53:21 - INFO - start steering 2022-01-11 06:53:22 - INFO - stop steering 2022-01-11 06:53:23 - INFO - start steering 2022-01-11 06:53:23 - INFO - stop steering","title":"Show Info Messages"},{"location":"development/#adding-loggers","text":"You can easily add more loggers. For example, to see debug messages of the odometer you can add 'rosys.odometer' : { 'handlers' : [ 'console' ], 'level' : 'DEBUG' , 'propagate' : False , }, Most of the time we turn off log propagation to ensure the configuration we defined ourselves is really used.","title":"Adding Loggers"},{"location":"development/#logging-to-file","text":"Sometimes it is helpful to write intensive logging into a file and only show some messages on the console. For this you can add a file handler : 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, 'file' : { 'level' : 'DEBUG' , 'class' : 'logging.handlers.RotatingFileHandler' , 'formatter' : 'default' , 'filename' : os . path . expanduser ( '~/.rosys/example.log' ), 'maxBytes' : 1024 * 1000 , 'backupCount' : 3 } }, Then you can decide for each logger which handlers should be used: 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' , 'file' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' , 'file' ], 'level' : 'INFO' , 'propagate' : False , }, 'rosys.event' : { 'handlers' : [ 'file' ], 'level' : 'DEBUG' , 'propagate' : False , }, 'rosys.core' : { 'handlers' : [ 'file' ], 'level' : 'DEBUG' , 'propagate' : False , }, }, Note The above file logger writes to ~/.rosys . For development it is very helpful to have auto-reloading on file change activated . Therefore logging should always be stored outside of your project's source directory.","title":"Logging to File"},{"location":"development/#formatting","text":"It is quite useful to see from which file and line number a log entry was triggered. To keep the log lines from getting too long, you can create a log filter which computes the relative path: class PackagePathFilter ( logging . Filter ): '''Provides relative path for log formatter. Original code borrowed from https://stackoverflow.com/a/52582536/3419103 ''' def filter ( self , record : logging . LogRecord ) -> bool : pathname = record . pathname record . relative_path = None abs_sys_paths = map ( os . path . abspath , sys . path ) for path in sorted ( abs_sys_paths , key = len , reverse = True ): # longer paths first if not path . endswith ( os . sep ): path += os . sep if pathname . startswith ( path ): record . relative_path = os . path . relpath ( pathname , path ) break return True You need to register the filter and apply it in the handler. Then you can change the format for the formatter: 'filters' : { 'package_path_filter' : { '()' : PackagePathFilter , }, }, 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'filters' : [ 'package_path_filter' ], 'formatter' : 'default' , 'level' : 'DEBUG' , 'stream' : 'ext://sys.stdout' }, }, 'loggers' : { '' : { # this root logger is used for everything without a specific logger 'handlers' : [ 'console' ], 'level' : 'WARN' , 'propagate' : False , }, 'rosys' : { 'handlers' : [ 'console' ], 'level' : 'INFO' , 'propagate' : False , }, }, Log output then looks like this: 2022-01-11 06:51:00.319 [DEBUG] rosys/runtime.py:78: startup completed","title":"Formatting"},{"location":"development/#profiling","text":"You can add a profile decorator to expensive functions and add a profiler button to your UI: #!/usr/bin/env python3 import rosys from nicegui import ui from rosys.debugging import ProfileButton , profiling @profiling . profile def compute () -> None : s = 0 for i in range ( 1_000_000 ): s += i ** 2 ui . notify ( s ) rosys . on_repeat ( compute , 1.0 ) ProfileButton () ui . run () When the button is pressed, the profiler yappi will start recording data. When stopped, you will see its output on the console: Line # Hits Time Per Hit % Time Line Contents ============================================================== 7 @profiling.profile 8 def compute() -> None: 9 3 21.0 7.0 0.0 s = 0 10 3000003 433138.0 0.1 28.2 for i in range(1_000_000): 11 3000000 1098975.0 0.4 71.6 s += i**2 12 3 2151.0 717.0 0.1 ui.notify(s)","title":"Profiling"},{"location":"development/#continuous-build","text":"We run our continuous integration with GitHub Actions. For each commit the pytests are executed.","title":"Continuous Build"},{"location":"development/#releases","text":"We publish releases by creating a new version on GitHub and describe the changes. A GitHub Action then performs the following steps: If the pytests are successful, a poetry build and deployment to pypi is issued. A multi-arch docker image is built and pushed to Docker Hub .","title":"Releases"},{"location":"getting_started/","text":"Getting Started \u00b6 First install RoSys with pip or Docker. Then create a directory to host your code and put it under version control. Name your entry file main.py and add the following content: #!/usr/bin/env python3 from nicegui import ui from rosys.driving import KeyboardControl , Odometer , RobotObject , Steerer from rosys.geometry import Prism from rosys.hardware import RobotBrain , SerialCommunication , WheelsHardware , WheelsSimulation , communication # setup shape = Prism . default_robot_shape () if SerialCommunication . is_possible (): communication = SerialCommunication () robot_brain = RobotBrain ( communication ) wheels = WheelsHardware ( robot_brain ) else : wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) # ui KeyboardControl ( steerer ) with ui . scene (): RobotObject ( shape , odometer ) ui . label ( 'hold SHIFT to steer with the keyboard arrow keys' ) # start ui . run ( title = 'RoSys' ) If you launch the program, your browser will open the url http://0.0.0.0:8080/ and present a 3d view: Explanation \u00b6 Imports \u00b6 The user interface is built with NiceGUI . The individual RoSys modules come in packages driving , geometry , hardware and others. Setup \u00b6 In this example we create a Steerer which needs an Odometer . If SerialCommunication is possible, the odometer gets access \"real\" Wheels . Otherwise wheels are simulated. For visualization purposes we also need the approximate robot shape. User Interface \u00b6 The user interface consists of keyboard control with access to the steerer as well as a 3D view of the scene. The latter only contains the RobotObject with the given shape. The robot pose is constantly updated from the odometer. See NiceGUI for more details about its API. Start \u00b6 NiceGUI provides a ui.run command which launches the web server and opens the corresponding web application. If you modify the code, a reload is triggered automatically. This is very convenient, but can be deactivated by passing reload=False .","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"First install RoSys with pip or Docker. Then create a directory to host your code and put it under version control. Name your entry file main.py and add the following content: #!/usr/bin/env python3 from nicegui import ui from rosys.driving import KeyboardControl , Odometer , RobotObject , Steerer from rosys.geometry import Prism from rosys.hardware import RobotBrain , SerialCommunication , WheelsHardware , WheelsSimulation , communication # setup shape = Prism . default_robot_shape () if SerialCommunication . is_possible (): communication = SerialCommunication () robot_brain = RobotBrain ( communication ) wheels = WheelsHardware ( robot_brain ) else : wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) # ui KeyboardControl ( steerer ) with ui . scene (): RobotObject ( shape , odometer ) ui . label ( 'hold SHIFT to steer with the keyboard arrow keys' ) # start ui . run ( title = 'RoSys' ) If you launch the program, your browser will open the url http://0.0.0.0:8080/ and present a 3d view:","title":"Getting Started"},{"location":"getting_started/#explanation","text":"","title":"Explanation"},{"location":"getting_started/#imports","text":"The user interface is built with NiceGUI . The individual RoSys modules come in packages driving , geometry , hardware and others.","title":"Imports"},{"location":"getting_started/#setup","text":"In this example we create a Steerer which needs an Odometer . If SerialCommunication is possible, the odometer gets access \"real\" Wheels . Otherwise wheels are simulated. For visualization purposes we also need the approximate robot shape.","title":"Setup"},{"location":"getting_started/#user-interface","text":"The user interface consists of keyboard control with access to the steerer as well as a 3D view of the scene. The latter only contains the RobotObject with the given shape. The robot pose is constantly updated from the odometer. See NiceGUI for more details about its API.","title":"User Interface"},{"location":"getting_started/#start","text":"NiceGUI provides a ui.run command which launches the web server and opens the corresponding web application. If you modify the code, a reload is triggered automatically. This is very convenient, but can be deactivated by passing reload=False .","title":"Start"},{"location":"installation/","text":"Installation \u00b6 On Your Desktop \u00b6 python3 -m pip install rosys See Getting Started for what to do next. On The Robot \u00b6 While the above installation commands work in a well setup environment, it is often easier to run RoSys inside a docker container, especially on Nvidia Jetson devices with their old 18.04 LTS Ubuntu. Launching \u00b6 There are some specialities needed to start RoSys in different environments (Mac, Linux, Jetson, ...). To simplify the usage we wrapped this in a script called ./docker.sh which you can also use and adapt in your own project. Have a look at the examples to see how a setup of your own repository could look like. Remote Development \u00b6 You can develop quite a lot of functionality with a simulated robot on your own computer. But there comes a time when you want to run your code on a real robot. Normally you will therefore start the container on the Robot Brain and connect via WiFi to the web interface. By using VS Code Remote Containers you can continue development as if you are using your own computer. Unfortunately some robot hardware (for example Nvidia Jetson) is much much slower than your own machine. With a large code base this can result in long restart times after you change some code (30 seconds or more). By launching rosys/hardware/hardware_proxy.py on the Robot Brain you can keep developing on your computer while being connected to the hardware via WiFi. When the runtime is initialized, it will first try to find the ESP32 of the Robot Brain locally. If this does not work, it tries to reach the Robot Brain via the local WiFi connection. Only if this also fails, it will fallback on a simulated hardware system.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#on-your-desktop","text":"python3 -m pip install rosys See Getting Started for what to do next.","title":"On Your Desktop"},{"location":"installation/#on-the-robot","text":"While the above installation commands work in a well setup environment, it is often easier to run RoSys inside a docker container, especially on Nvidia Jetson devices with their old 18.04 LTS Ubuntu.","title":"On The Robot"},{"location":"installation/#launching","text":"There are some specialities needed to start RoSys in different environments (Mac, Linux, Jetson, ...). To simplify the usage we wrapped this in a script called ./docker.sh which you can also use and adapt in your own project. Have a look at the examples to see how a setup of your own repository could look like.","title":"Launching"},{"location":"installation/#remote-development","text":"You can develop quite a lot of functionality with a simulated robot on your own computer. But there comes a time when you want to run your code on a real robot. Normally you will therefore start the container on the Robot Brain and connect via WiFi to the web interface. By using VS Code Remote Containers you can continue development as if you are using your own computer. Unfortunately some robot hardware (for example Nvidia Jetson) is much much slower than your own machine. With a large code base this can result in long restart times after you change some code (30 seconds or more). By launching rosys/hardware/hardware_proxy.py on the Robot Brain you can keep developing on your computer while being connected to the hardware via WiFi. When the runtime is initialized, it will first try to find the ESP32 of the Robot Brain locally. If this does not work, it tries to reach the Robot Brain via the local WiFi connection. Only if this also fails, it will fallback on a simulated hardware system.","title":"Remote Development"},{"location":"troubleshooting/","text":"Troubleshooting \u00b6 Asyncio Warning \u00b6 While running RoSys you may see warnings similar to this one: 2021-10-31 15:08:04.040 [WARNING] asyncio: Executing <Task pending name='Task-255' coro=<handle_event() running at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:344> wait_for=<_GatheringFuture pending cb=[<TaskWakeupMethWrapper object at 0x7f7001f8e0>()] created at /usr/local/lib/python3.9/asyncio/tasks.py:705> created at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:261> took 0.238 seconds This means some coroutine is clogging the event loop for too long. In the above example it is a whopping 238 ms in which no other actor can do anything. This is an eternity when machine communication is expected to happen about every 10 ms. The warning also provides a (not so readable) hint where the time is consumed. The example above is one of the more frequent scenarios. It means some code inside a user interaction event handler (e.g. handle_event() in justpy.py ) is blocking. Try to figure out which UI event code is responsible by commenting out parts of your logic and try to reproduce the warning systematically.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"troubleshooting/#asyncio-warning","text":"While running RoSys you may see warnings similar to this one: 2021-10-31 15:08:04.040 [WARNING] asyncio: Executing <Task pending name='Task-255' coro=<handle_event() running at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:344> wait_for=<_GatheringFuture pending cb=[<TaskWakeupMethWrapper object at 0x7f7001f8e0>()] created at /usr/local/lib/python3.9/asyncio/tasks.py:705> created at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:261> took 0.238 seconds This means some coroutine is clogging the event loop for too long. In the above example it is a whopping 238 ms in which no other actor can do anything. This is an eternity when machine communication is expected to happen about every 10 ms. The warning also provides a (not so readable) hint where the time is consumed. The example above is one of the more frequent scenarios. It means some code inside a user interaction event handler (e.g. handle_event() in justpy.py ) is blocking. Try to figure out which UI event code is responsible by commenting out parts of your logic and try to reproduce the warning systematically.","title":"Asyncio Warning"},{"location":"examples/cameras/","text":"Cameras \u00b6 RoSys provides instant camera access for object detection, remote operation and similar use cases. Setup \u00b6 USB camera devices are discovered through video4linux (v4l) and accessed with openCV. Therefore the program v4l2ctl and openCV (including python bindings) must be available. We recommend to use the RoSys docker image which provides the full required software stack. Make sure the container can access the USB devices by starting it with --privileged or explicitly passing the specific --device s. Show Captured Images \u00b6 Using rosys.ui you can show the latest captured images from each camera: #!/usr/bin/env python3 from nicegui import ui from rosys.vision import CameraServer , UsbCameraProviderSimulation , camera_provider camera_provider = UsbCameraProviderSimulation () camera_provider . add_camera ( camera_provider . create_calibrated ( 'test_cam' , width = 800 , height = 600 )) CameraServer ( camera_provider ) def refresh () -> None : for uid , camera in camera_provider . cameras . items (): if uid not in feeds : feeds [ uid ] = ui . image () feeds [ uid ] . set_source ( camera . latest_image_uri ) feeds = {} ui . timer ( 0.3 , refresh ) ui . run ( title = 'RoSys' ) The ui.timer regularly updates the source property of the ui.image . The cameras latest_image_uri property provides the URI to the latest captured image. This example uses a UsbCameraProviderSimulation with a single simulated test camera. But you can replace the provider with a UsbCameraProviderHardware . Remote Operation \u00b6 A fairly often required use case is the remote operation of a robot. In a simple use case you may only need to visualize one camera and have some steering controls. Here we use the NEW_CAMERA event to display the first camera: #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Joystick , KeyboardControl , Odometer , Steerer from rosys.hardware import RobotBrain , SerialCommunication , WheelsHardware , WheelsSimulation from rosys.vision import Camera , CameraProvider , CameraServer , UsbCameraProviderHardware , UsbCameraProviderSimulation if SerialCommunication . is_possible (): communication = SerialCommunication () robot_brain = RobotBrain ( communication ) wheels = WheelsHardware ( robot_brain ) camera_provider = UsbCameraProviderHardware () else : wheels = WheelsSimulation () camera_provider = UsbCameraProviderSimulation () camera_provider . restore = lambda _ : None # NOTE: disable persistence test_cam = camera_provider . create_calibrated ( 'test_cam' , width = 800 , height = 600 ) ui . on_startup ( lambda : camera_provider . add_camera ( test_cam )) CameraServer ( camera_provider ) steerer = Steerer ( wheels ) odometer = Odometer ( wheels ) async def add_main_camera ( camera : Camera ) -> None : camera_card . clear () # remove \"seeking camera\" label with camera_card : maincam = ui . image () ui . timer ( 1 , lambda : maincam . set_source ( camera . latest_image_uri )) CameraProvider . CAMERA_ADDED . register ( add_main_camera ) with ui . card () . tight () . style ( 'width:30em' ) as camera_card : ui . label ( 'seeking main camera' ) . classes ( 'm-8 text-center' ) with ui . card () . tight () . style ( 'width:30em' ): with ui . row (): with ui . card () . tight (): Joystick ( steerer ) KeyboardControl ( steerer ) ui . markdown ( 'steer with joystick on the left or<br />SHIFT + arrow keys' ) . classes ( 'm-8 text-center' ) ui . run ( title = 'RoSys' ) By adding a Joystick and KeyboardControl the robot is ready to go for remote operation.","title":"Cameras"},{"location":"examples/cameras/#cameras","text":"RoSys provides instant camera access for object detection, remote operation and similar use cases.","title":"Cameras"},{"location":"examples/cameras/#setup","text":"USB camera devices are discovered through video4linux (v4l) and accessed with openCV. Therefore the program v4l2ctl and openCV (including python bindings) must be available. We recommend to use the RoSys docker image which provides the full required software stack. Make sure the container can access the USB devices by starting it with --privileged or explicitly passing the specific --device s.","title":"Setup"},{"location":"examples/cameras/#show-captured-images","text":"Using rosys.ui you can show the latest captured images from each camera: #!/usr/bin/env python3 from nicegui import ui from rosys.vision import CameraServer , UsbCameraProviderSimulation , camera_provider camera_provider = UsbCameraProviderSimulation () camera_provider . add_camera ( camera_provider . create_calibrated ( 'test_cam' , width = 800 , height = 600 )) CameraServer ( camera_provider ) def refresh () -> None : for uid , camera in camera_provider . cameras . items (): if uid not in feeds : feeds [ uid ] = ui . image () feeds [ uid ] . set_source ( camera . latest_image_uri ) feeds = {} ui . timer ( 0.3 , refresh ) ui . run ( title = 'RoSys' ) The ui.timer regularly updates the source property of the ui.image . The cameras latest_image_uri property provides the URI to the latest captured image. This example uses a UsbCameraProviderSimulation with a single simulated test camera. But you can replace the provider with a UsbCameraProviderHardware .","title":"Show Captured Images"},{"location":"examples/cameras/#remote-operation","text":"A fairly often required use case is the remote operation of a robot. In a simple use case you may only need to visualize one camera and have some steering controls. Here we use the NEW_CAMERA event to display the first camera: #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Joystick , KeyboardControl , Odometer , Steerer from rosys.hardware import RobotBrain , SerialCommunication , WheelsHardware , WheelsSimulation from rosys.vision import Camera , CameraProvider , CameraServer , UsbCameraProviderHardware , UsbCameraProviderSimulation if SerialCommunication . is_possible (): communication = SerialCommunication () robot_brain = RobotBrain ( communication ) wheels = WheelsHardware ( robot_brain ) camera_provider = UsbCameraProviderHardware () else : wheels = WheelsSimulation () camera_provider = UsbCameraProviderSimulation () camera_provider . restore = lambda _ : None # NOTE: disable persistence test_cam = camera_provider . create_calibrated ( 'test_cam' , width = 800 , height = 600 ) ui . on_startup ( lambda : camera_provider . add_camera ( test_cam )) CameraServer ( camera_provider ) steerer = Steerer ( wheels ) odometer = Odometer ( wheels ) async def add_main_camera ( camera : Camera ) -> None : camera_card . clear () # remove \"seeking camera\" label with camera_card : maincam = ui . image () ui . timer ( 1 , lambda : maincam . set_source ( camera . latest_image_uri )) CameraProvider . CAMERA_ADDED . register ( add_main_camera ) with ui . card () . tight () . style ( 'width:30em' ) as camera_card : ui . label ( 'seeking main camera' ) . classes ( 'm-8 text-center' ) with ui . card () . tight () . style ( 'width:30em' ): with ui . row (): with ui . card () . tight (): Joystick ( steerer ) KeyboardControl ( steerer ) ui . markdown ( 'steer with joystick on the left or<br />SHIFT + arrow keys' ) . classes ( 'm-8 text-center' ) ui . run ( title = 'RoSys' ) By adding a Joystick and KeyboardControl the robot is ready to go for remote operation.","title":"Remote Operation"},{"location":"examples/click-and-drive/","text":"Click-and-drive \u00b6 In this example we create a simulated robot that drives wherever the user clicks. #!/usr/bin/env python3 from nicegui import ui from rosys.automation import AutomationControls , Automator from rosys.driving import Driver , Odometer , RobotObject from rosys.geometry import Point , Prism from rosys.hardware import WheelsSimulation shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) driver = Driver ( wheels , odometer ) automator = Automator ( wheels , None ) async def handle_click ( msg ): for hit in msg . hits : if hit . object_id == 'ground' : target = Point ( x = hit . point . x , y = hit . point . y ) automator . start ( driver . drive_to ( target )) with ui . scene ( on_click = handle_click ): RobotObject ( shape , odometer , debug = True ) ui . label ( 'click into the scene to drive the robot' ) with ui . row (): AutomationControls ( automator ) ui . label ( 'you can also pause/resume or stop the running automation' ) ui . run ( title = 'RoSys' ) Modules Besides wheels, odometer and a robot shape we need a driver that enables the robot to drive along a given path as well as an automator to start and stop such an automated behavior. Click handler NiceGUI's 3D scene allows registering a click handler that can iterate through all hit points and find the target on the ground. Driver Among others, the driver has an async method drive_to which lets the robot follow a straight line to a given target. Automator and automation controls The automator starts the async method and allows pausing, resuming and stopping it, e.g. with the AutomationControls UI element.","title":"Click-and-drive"},{"location":"examples/click-and-drive/#click-and-drive","text":"In this example we create a simulated robot that drives wherever the user clicks. #!/usr/bin/env python3 from nicegui import ui from rosys.automation import AutomationControls , Automator from rosys.driving import Driver , Odometer , RobotObject from rosys.geometry import Point , Prism from rosys.hardware import WheelsSimulation shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) driver = Driver ( wheels , odometer ) automator = Automator ( wheels , None ) async def handle_click ( msg ): for hit in msg . hits : if hit . object_id == 'ground' : target = Point ( x = hit . point . x , y = hit . point . y ) automator . start ( driver . drive_to ( target )) with ui . scene ( on_click = handle_click ): RobotObject ( shape , odometer , debug = True ) ui . label ( 'click into the scene to drive the robot' ) with ui . row (): AutomationControls ( automator ) ui . label ( 'you can also pause/resume or stop the running automation' ) ui . run ( title = 'RoSys' ) Modules Besides wheels, odometer and a robot shape we need a driver that enables the robot to drive along a given path as well as an automator to start and stop such an automated behavior. Click handler NiceGUI's 3D scene allows registering a click handler that can iterate through all hit points and find the target on the ground. Driver Among others, the driver has an async method drive_to which lets the robot follow a straight line to a given target. Automator and automation controls The automator starts the async method and allows pausing, resuming and stopping it, e.g. with the AutomationControls UI element.","title":"Click-and-drive"},{"location":"examples/navigation/","text":"Navigation \u00b6 This example is similar to Click-and-drive but includes a PathPlanner to find a path around an obstacle. #!/usr/bin/env python3 from nicegui import ui from rosys.automation import Automator from rosys.driving import Driver , Odometer , RobotObject from rosys.geometry import Point , Pose , Prism from rosys.hardware import WheelsSimulation from rosys.pathplanning import Obstacle , ObstacleObject , PathObject , PathPlanner shape = Prism . default_robot_shape () path_planner = PathPlanner ( shape ) path_planner . restore = lambda _ : None # NOTE: disable persistence path_planner . obstacles [ '0' ] = Obstacle ( id = '0' , outline = [ Point ( x = 3 , y = 0 ), Point ( x = 0 , y = 3 ), Point ( x = 3 , y = 3 )]) wheels = WheelsSimulation () odometer = Odometer ( wheels ) driver = Driver ( wheels , odometer ) automator = Automator ( wheels , None ) async def handle_click ( msg ): for hit in msg . hits : if hit . object_id == 'ground' : yaw = odometer . prediction . point . direction ( hit . point ) goal = Pose ( x = hit . point . x , y = hit . point . y , yaw = yaw ) path = await path_planner . search ( start = odometer . prediction , goal = goal ) path3d . update ( path ) automator . start ( driver . drive_path ( path )) with ui . scene ( on_click = handle_click , width = 600 ): RobotObject ( shape , odometer ) ObstacleObject ( path_planner . obstacles ) path3d = PathObject () ui . label ( 'click into the scene to drive the robot' ) ui . run ( title = 'RoSys' ) Geometry \u00b6 When following a path, a \"carrot\" is dragged along a spline and the robot follows it like a donkey. Additionally, there is a virtual \"hook\" attached to the robot, which is pulled towards the carrot. There are three parameters: hook_offset : How far from the wheel axis (i.e. the coordinate center of the robot) is the hook, which is pulled towards the carrot. carrot_offset : How far ahead of the carrot is the robot pulled. This parameter is necessary in order to have the hook pulled a bit further, even though the carrot already reached the end of the spline. carrot_distance : How long is the \"thread\" between hook and carrot (or the offset point ahead of the carrot, respectively). In the following illustration these points are depicted as spheres: the coordinate center of the robot (blue, small), the hook (blue, large), carrot (orange, small), offset point ahead of the carrot (orange, large). Note The automation drive_spline has an optional argument flip_hook . It turns the hook 180 degrees to the back of the robot, while preserving the distance hook_offset to the robot's coordinate center. This allows the robot to drive backwards to a point behind it instead of turning around and approaching it forwards.","title":"Navigation"},{"location":"examples/navigation/#navigation","text":"This example is similar to Click-and-drive but includes a PathPlanner to find a path around an obstacle. #!/usr/bin/env python3 from nicegui import ui from rosys.automation import Automator from rosys.driving import Driver , Odometer , RobotObject from rosys.geometry import Point , Pose , Prism from rosys.hardware import WheelsSimulation from rosys.pathplanning import Obstacle , ObstacleObject , PathObject , PathPlanner shape = Prism . default_robot_shape () path_planner = PathPlanner ( shape ) path_planner . restore = lambda _ : None # NOTE: disable persistence path_planner . obstacles [ '0' ] = Obstacle ( id = '0' , outline = [ Point ( x = 3 , y = 0 ), Point ( x = 0 , y = 3 ), Point ( x = 3 , y = 3 )]) wheels = WheelsSimulation () odometer = Odometer ( wheels ) driver = Driver ( wheels , odometer ) automator = Automator ( wheels , None ) async def handle_click ( msg ): for hit in msg . hits : if hit . object_id == 'ground' : yaw = odometer . prediction . point . direction ( hit . point ) goal = Pose ( x = hit . point . x , y = hit . point . y , yaw = yaw ) path = await path_planner . search ( start = odometer . prediction , goal = goal ) path3d . update ( path ) automator . start ( driver . drive_path ( path )) with ui . scene ( on_click = handle_click , width = 600 ): RobotObject ( shape , odometer ) ObstacleObject ( path_planner . obstacles ) path3d = PathObject () ui . label ( 'click into the scene to drive the robot' ) ui . run ( title = 'RoSys' )","title":"Navigation"},{"location":"examples/navigation/#geometry","text":"When following a path, a \"carrot\" is dragged along a spline and the robot follows it like a donkey. Additionally, there is a virtual \"hook\" attached to the robot, which is pulled towards the carrot. There are three parameters: hook_offset : How far from the wheel axis (i.e. the coordinate center of the robot) is the hook, which is pulled towards the carrot. carrot_offset : How far ahead of the carrot is the robot pulled. This parameter is necessary in order to have the hook pulled a bit further, even though the carrot already reached the end of the spline. carrot_distance : How long is the \"thread\" between hook and carrot (or the offset point ahead of the carrot, respectively). In the following illustration these points are depicted as spheres: the coordinate center of the robot (blue, small), the hook (blue, large), carrot (orange, small), offset point ahead of the carrot (orange, large). Note The automation drive_spline has an optional argument flip_hook . It turns the hook 180 degrees to the back of the robot, while preserving the distance hook_offset to the robot's coordinate center. This allows the robot to drive backwards to a point behind it instead of turning around and approaching it forwards.","title":"Geometry"},{"location":"examples/steering/","text":"Steering \u00b6 The following example simulates a robot that can be steered using keyboard controls or a joystick. #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Joystick , KeyboardControl , Odometer , RobotObject , Steerer from rosys.geometry import Prism from rosys.hardware import WheelsSimulation shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) KeyboardControl ( steerer ) Joystick ( steerer , size = 50 , color = 'blue' ) with ui . scene (): RobotObject ( shape , odometer ) ui . run ( title = 'RoSys' ) Keyboard Control By adding a KeyboardControl to the user interface you enable steering the robot with the keyboard. Press the arrow keys while holding the SHIFT key to steer the robot. You can also modify the speed of the robot by pressing the a number key. Use the optional parameter default_speed to change the initial value. Joystick When operating from a mobile phone, you can use a Joystick to create a UI element with touch control. You can drive the robot by dragging the mouse inside the top left square.","title":"Steering"},{"location":"examples/steering/#steering","text":"The following example simulates a robot that can be steered using keyboard controls or a joystick. #!/usr/bin/env python3 from nicegui import ui from rosys.driving import Joystick , KeyboardControl , Odometer , RobotObject , Steerer from rosys.geometry import Prism from rosys.hardware import WheelsSimulation shape = Prism . default_robot_shape () wheels = WheelsSimulation () odometer = Odometer ( wheels ) steerer = Steerer ( wheels ) KeyboardControl ( steerer ) Joystick ( steerer , size = 50 , color = 'blue' ) with ui . scene (): RobotObject ( shape , odometer ) ui . run ( title = 'RoSys' ) Keyboard Control By adding a KeyboardControl to the user interface you enable steering the robot with the keyboard. Press the arrow keys while holding the SHIFT key to steer the robot. You can also modify the speed of the robot by pressing the a number key. Use the optional parameter default_speed to change the initial value. Joystick When operating from a mobile phone, you can use a Joystick to create a UI element with touch control. You can drive the robot by dragging the mouse inside the top left square.","title":"Steering"},{"location":"reference/SUMMARY/","text":"automation debugging driving hardware pathplanning vision","title":"SUMMARY"},{"location":"reference/rosys/automation/","text":"AppControls \u00b6 AppControls ( communication : Communication , automator : Automator ) -> None The AppControls module enables the connection with a mobile-app-based user interface. It uses a given communication object to communicate with Lizard running on a microcontroller and in turn being connected to a mobile app via Bluetooth Low Energy. It displays buttons to control a given automator. APP_CONNECTED class-attribute \u00b6 APP_CONNECTED = Event () an app connected via bluetooth (used to refresh information or similar) notify async \u00b6 notify ( msg : str ) -> None show notification as Snackbar message on mobile device set_info async \u00b6 set_info ( msg : str ) -> None replace constantly shown info text on mobile device Automation \u00b6 Automation ( coro : Coroutine , exception_handler : Optional [ Callable ] = None , on_complete : Optional [ Callable ] = None ) -> None An automation wraps a coroutine and allows pausing and resuming it. Optional exception and completion handlers are called if provided. AutomationControls \u00b6 AutomationControls ( automator : Automator ) -> None This UI element contains start/stop/pause/resume buttons for controlling a given automator. Automator \u00b6 Automator ( wheels : Optional [ Drivable ], steerer : Optional [ Steerer ], * , default_automation : Optional [ Callable ] = None ) -> None An automator allows running automations, i.e. coroutines that can be paused and resumed. A default automation can be specified, so that the automator can later be started (e.g. via an automation controls UI element) without passing an automation. Optional wheels (or any stoppable hardware representation) will be stopped when an automation pauses or stops. Manually steering the robot using an optional steerer pauses a currently running automation. AUTOMATION_COMPLETED class-attribute \u00b6 AUTOMATION_COMPLETED = Event () an automation has been completed AUTOMATION_FAILED class-attribute \u00b6 AUTOMATION_FAILED = Event () an automation has failed to complete (string argument: description of the cause) AUTOMATION_PAUSED class-attribute \u00b6 AUTOMATION_PAUSED = Event () an automation has been paused (string argument: description of the cause) AUTOMATION_RESUMED class-attribute \u00b6 AUTOMATION_RESUMED = Event () an automation has been resumed AUTOMATION_STARTED class-attribute \u00b6 AUTOMATION_STARTED = Event () an automation has been started AUTOMATION_STOPPED class-attribute \u00b6 AUTOMATION_STOPPED = Event () an automation has been stopped (string argument: description of the cause)","title":"automation"},{"location":"reference/rosys/automation/#rosys.automation.AppControls","text":"AppControls ( communication : Communication , automator : Automator ) -> None The AppControls module enables the connection with a mobile-app-based user interface. It uses a given communication object to communicate with Lizard running on a microcontroller and in turn being connected to a mobile app via Bluetooth Low Energy. It displays buttons to control a given automator.","title":"AppControls"},{"location":"reference/rosys/automation/#rosys.automation.app_controls.AppControls.APP_CONNECTED","text":"APP_CONNECTED = Event () an app connected via bluetooth (used to refresh information or similar)","title":"APP_CONNECTED"},{"location":"reference/rosys/automation/#rosys.automation.app_controls.AppControls.notify","text":"notify ( msg : str ) -> None show notification as Snackbar message on mobile device","title":"notify()"},{"location":"reference/rosys/automation/#rosys.automation.app_controls.AppControls.set_info","text":"set_info ( msg : str ) -> None replace constantly shown info text on mobile device","title":"set_info()"},{"location":"reference/rosys/automation/#rosys.automation.Automation","text":"Automation ( coro : Coroutine , exception_handler : Optional [ Callable ] = None , on_complete : Optional [ Callable ] = None ) -> None An automation wraps a coroutine and allows pausing and resuming it. Optional exception and completion handlers are called if provided.","title":"Automation"},{"location":"reference/rosys/automation/#rosys.automation.AutomationControls","text":"AutomationControls ( automator : Automator ) -> None This UI element contains start/stop/pause/resume buttons for controlling a given automator.","title":"AutomationControls"},{"location":"reference/rosys/automation/#rosys.automation.Automator","text":"Automator ( wheels : Optional [ Drivable ], steerer : Optional [ Steerer ], * , default_automation : Optional [ Callable ] = None ) -> None An automator allows running automations, i.e. coroutines that can be paused and resumed. A default automation can be specified, so that the automator can later be started (e.g. via an automation controls UI element) without passing an automation. Optional wheels (or any stoppable hardware representation) will be stopped when an automation pauses or stops. Manually steering the robot using an optional steerer pauses a currently running automation.","title":"Automator"},{"location":"reference/rosys/automation/#rosys.automation.automator.Automator.AUTOMATION_COMPLETED","text":"AUTOMATION_COMPLETED = Event () an automation has been completed","title":"AUTOMATION_COMPLETED"},{"location":"reference/rosys/automation/#rosys.automation.automator.Automator.AUTOMATION_FAILED","text":"AUTOMATION_FAILED = Event () an automation has failed to complete (string argument: description of the cause)","title":"AUTOMATION_FAILED"},{"location":"reference/rosys/automation/#rosys.automation.automator.Automator.AUTOMATION_PAUSED","text":"AUTOMATION_PAUSED = Event () an automation has been paused (string argument: description of the cause)","title":"AUTOMATION_PAUSED"},{"location":"reference/rosys/automation/#rosys.automation.automator.Automator.AUTOMATION_RESUMED","text":"AUTOMATION_RESUMED = Event () an automation has been resumed","title":"AUTOMATION_RESUMED"},{"location":"reference/rosys/automation/#rosys.automation.automator.Automator.AUTOMATION_STARTED","text":"AUTOMATION_STARTED = Event () an automation has been started","title":"AUTOMATION_STARTED"},{"location":"reference/rosys/automation/#rosys.automation.automator.Automator.AUTOMATION_STOPPED","text":"AUTOMATION_STOPPED = Event () an automation has been stopped (string argument: description of the cause)","title":"AUTOMATION_STOPPED"},{"location":"reference/rosys/debugging/","text":"ProfileButton \u00b6 ProfileButton () -> None Bases: ui . button The profile button allows starting and stopping a profiling session. Use the profiling.profile decorator for including functions or methods in the analysis. The results are shown on the console. WifiButton \u00b6 WifiButton () -> None Bases: Button The WiFi button indicates the current connectivity state and allows setting a new WiFi connection.","title":"debugging"},{"location":"reference/rosys/debugging/#rosys.debugging.ProfileButton","text":"ProfileButton () -> None Bases: ui . button The profile button allows starting and stopping a profiling session. Use the profiling.profile decorator for including functions or methods in the analysis. The results are shown on the console.","title":"ProfileButton"},{"location":"reference/rosys/debugging/#rosys.debugging.WifiButton","text":"WifiButton () -> None Bases: Button The WiFi button indicates the current connectivity state and allows setting a new WiFi connection.","title":"WifiButton"},{"location":"reference/rosys/driving/","text":"Driver \u00b6 Driver ( wheels : Drivable , odometer : Odometer ) -> None The driver module allows following a given path. It requires a wheels module (or any drivable hardware representation) to execute individual drive commands. It also requires an odometer to get a current prediction of the robot's pose. Its parameters allow controlling the specific drive behavior. Joystick \u00b6 Joystick ( steerer : Steerer , ** options ) -> None Bases: NiceGuiJoystick The Joystick UI element allows controlling a given steerer via touch events. KeyboardControl \u00b6 KeyboardControl ( steerer : Steerer , * , default_speed : float = 2.0 ) -> None The KeyboardControl UI element allows controlling a given steerer via keyboard events. Hold shift while pressing an arrow key to steer the robot. You can change the speed with the number keys 1 to 9 and the initial speed via the default_speed argument. Odometer \u00b6 Odometer ( wheels : VelocityProvider ) -> None An odometer collects velocity information from a given wheels module (or any velocity-providing hardware representation). It can also handle \"detections\", i.e. absolute pose information with timestamps. Given the history of previously received velocities, it can update its prediction of the current pose. The get_pose method provides robot poses from the within the last 10 seconds. ROBOT_MOVED class-attribute \u00b6 ROBOT_MOVED = Event () a robot movement is detected RobotObject \u00b6 RobotObject ( shape : Prism , odometer : Odometer , driver : Optional [ Driver ] = None , * , debug : bool = False ) -> None Bases: Object3D The RobotObject UI element displays the robot with its given shape in a 3D scene. The current pose is taken from a given odometer. An optional driver module shows debugging information about a current path-following process. The debug argument can be set to show a wireframe instead of a closed polygon. Steerer \u00b6 Steerer ( wheels : Drivable ) -> None The steerer module translates x-y information (e.g. from a joystick) to linear/angular velocities sent to the robot. The wheels module can be any drivable hardware representation. Changing the steering state emits events that can be used to react to manual user interaction. STEERING_STARTED class-attribute \u00b6 STEERING_STARTED = Event () steering has started STEERING_STOPPED class-attribute \u00b6 STEERING_STOPPED = Event () steering has stopped","title":"driving"},{"location":"reference/rosys/driving/#rosys.driving.Driver","text":"Driver ( wheels : Drivable , odometer : Odometer ) -> None The driver module allows following a given path. It requires a wheels module (or any drivable hardware representation) to execute individual drive commands. It also requires an odometer to get a current prediction of the robot's pose. Its parameters allow controlling the specific drive behavior.","title":"Driver"},{"location":"reference/rosys/driving/#rosys.driving.Joystick","text":"Joystick ( steerer : Steerer , ** options ) -> None Bases: NiceGuiJoystick The Joystick UI element allows controlling a given steerer via touch events.","title":"Joystick"},{"location":"reference/rosys/driving/#rosys.driving.KeyboardControl","text":"KeyboardControl ( steerer : Steerer , * , default_speed : float = 2.0 ) -> None The KeyboardControl UI element allows controlling a given steerer via keyboard events. Hold shift while pressing an arrow key to steer the robot. You can change the speed with the number keys 1 to 9 and the initial speed via the default_speed argument.","title":"KeyboardControl"},{"location":"reference/rosys/driving/#rosys.driving.Odometer","text":"Odometer ( wheels : VelocityProvider ) -> None An odometer collects velocity information from a given wheels module (or any velocity-providing hardware representation). It can also handle \"detections\", i.e. absolute pose information with timestamps. Given the history of previously received velocities, it can update its prediction of the current pose. The get_pose method provides robot poses from the within the last 10 seconds.","title":"Odometer"},{"location":"reference/rosys/driving/#rosys.driving.odometer.Odometer.ROBOT_MOVED","text":"ROBOT_MOVED = Event () a robot movement is detected","title":"ROBOT_MOVED"},{"location":"reference/rosys/driving/#rosys.driving.RobotObject","text":"RobotObject ( shape : Prism , odometer : Odometer , driver : Optional [ Driver ] = None , * , debug : bool = False ) -> None Bases: Object3D The RobotObject UI element displays the robot with its given shape in a 3D scene. The current pose is taken from a given odometer. An optional driver module shows debugging information about a current path-following process. The debug argument can be set to show a wireframe instead of a closed polygon.","title":"RobotObject"},{"location":"reference/rosys/driving/#rosys.driving.Steerer","text":"Steerer ( wheels : Drivable ) -> None The steerer module translates x-y information (e.g. from a joystick) to linear/angular velocities sent to the robot. The wheels module can be any drivable hardware representation. Changing the steering state emits events that can be used to react to manual user interaction.","title":"Steerer"},{"location":"reference/rosys/driving/#rosys.driving.steerer.Steerer.STEERING_STARTED","text":"STEERING_STARTED = Event () steering has started","title":"STEERING_STARTED"},{"location":"reference/rosys/driving/#rosys.driving.steerer.Steerer.STEERING_STOPPED","text":"STEERING_STOPPED = Event () steering has stopped","title":"STEERING_STOPPED"},{"location":"reference/rosys/hardware/","text":"Communication \u00b6 Communication () -> None Bases: abc . ABC This abstract module defines an interface for communicating with a microcontroller. Besides sending and receiving messages a communication module provides a property whether communication is possible. It can also provide a piece of debug UI. RobotBrain \u00b6 RobotBrain ( communication : Communication ) -> None This module manages the communication with a Zauberzeug Robot Brain . It expects a communication object, which is used for the actual read and write operations. Besides providing some basic methods like configuring or restarting the microcontroller, it augments and verifies checksums for each message. LINE_RECEIVED class-attribute \u00b6 LINE_RECEIVED = Event () a line has been received from the microcontroller (argument: line as string) SerialCommunication \u00b6 SerialCommunication ( baud_rate : int = 115200 ) -> None Bases: Communication This module implements a communication via a serial device with a given baud rate. It contains a list of search paths for finding the serial device. WebCommunication \u00b6 WebCommunication () -> None Bases: Communication Remote connection to the Robot Brain's ESP. This makes it possible to keep developing on your fast computer while communicating with the hardware components connected to a physical Robot Brain. Wheels \u00b6 Wheels () -> None Bases: abc . ABC The wheels module is a simple example for a representation of real or simulated robot hardware. Wheels can be moved using the drive methods and provide measured velocities as an event. VELOCITY_MEASURED class-attribute \u00b6 VELOCITY_MEASURED = Event () new velocity measurements are available for processing (argument: list of velocities) WheelsHardware \u00b6 WheelsHardware ( robot_brain : RobotBrain ) -> None Bases: Wheels This module implements wheels hardware. Drive and stop commands are forwarded to a given Robot Brain. Velocities are read and emitted regularly. WheelsSimulation \u00b6 WheelsSimulation () -> None Bases: Wheels This module simulates two wheels. Drive and stop commands impact internal velocities (linear and angular). A simulated pose is regularly updated with these velocities, while the velocities are emitted as an event.","title":"hardware"},{"location":"reference/rosys/hardware/#rosys.hardware.Communication","text":"Communication () -> None Bases: abc . ABC This abstract module defines an interface for communicating with a microcontroller. Besides sending and receiving messages a communication module provides a property whether communication is possible. It can also provide a piece of debug UI.","title":"Communication"},{"location":"reference/rosys/hardware/#rosys.hardware.RobotBrain","text":"RobotBrain ( communication : Communication ) -> None This module manages the communication with a Zauberzeug Robot Brain . It expects a communication object, which is used for the actual read and write operations. Besides providing some basic methods like configuring or restarting the microcontroller, it augments and verifies checksums for each message.","title":"RobotBrain"},{"location":"reference/rosys/hardware/#rosys.hardware.robot_brain.RobotBrain.LINE_RECEIVED","text":"LINE_RECEIVED = Event () a line has been received from the microcontroller (argument: line as string)","title":"LINE_RECEIVED"},{"location":"reference/rosys/hardware/#rosys.hardware.SerialCommunication","text":"SerialCommunication ( baud_rate : int = 115200 ) -> None Bases: Communication This module implements a communication via a serial device with a given baud rate. It contains a list of search paths for finding the serial device.","title":"SerialCommunication"},{"location":"reference/rosys/hardware/#rosys.hardware.WebCommunication","text":"WebCommunication () -> None Bases: Communication Remote connection to the Robot Brain's ESP. This makes it possible to keep developing on your fast computer while communicating with the hardware components connected to a physical Robot Brain.","title":"WebCommunication"},{"location":"reference/rosys/hardware/#rosys.hardware.Wheels","text":"Wheels () -> None Bases: abc . ABC The wheels module is a simple example for a representation of real or simulated robot hardware. Wheels can be moved using the drive methods and provide measured velocities as an event.","title":"Wheels"},{"location":"reference/rosys/hardware/#rosys.hardware.wheels.Wheels.VELOCITY_MEASURED","text":"VELOCITY_MEASURED = Event () new velocity measurements are available for processing (argument: list of velocities)","title":"VELOCITY_MEASURED"},{"location":"reference/rosys/hardware/#rosys.hardware.WheelsHardware","text":"WheelsHardware ( robot_brain : RobotBrain ) -> None Bases: Wheels This module implements wheels hardware. Drive and stop commands are forwarded to a given Robot Brain. Velocities are read and emitted regularly.","title":"WheelsHardware"},{"location":"reference/rosys/hardware/#rosys.hardware.WheelsSimulation","text":"WheelsSimulation () -> None Bases: Wheels This module simulates two wheels. Drive and stop commands impact internal velocities (linear and angular). A simulated pose is regularly updated with these velocities, while the velocities are emitted as an event.","title":"WheelsSimulation"},{"location":"reference/rosys/pathplanning/","text":"PathPlanner \u00b6 PathPlanner ( robot_shape : Prism ) -> None This module runs a path planning algorithm in a separate process. If given, the algorithm respects the given robot shape as well as a dictionary of accessible areas and a dictionary of obstacles, both of which a backed up and restored automatically. The path planner can search paths, check if a spline interferes with obstacles and get the distance of a pose to any obstacle.","title":"pathplanning"},{"location":"reference/rosys/pathplanning/#rosys.pathplanning.PathPlanner","text":"PathPlanner ( robot_shape : Prism ) -> None This module runs a path planning algorithm in a separate process. If given, the algorithm respects the given robot shape as well as a dictionary of accessible areas and a dictionary of obstacles, both of which a backed up and restored automatically. The path planner can search paths, check if a spline interferes with obstacles and get the distance of a pose to any obstacle.","title":"PathPlanner"},{"location":"reference/rosys/vision/","text":"CameraObjects \u00b6 CameraObjects ( camera_provider : CameraProvider , camera_projector : CameraProjector , * , px_per_m : float = 10000 ) -> None Bases: Group This module provides a UI element for displaying cameras in a 3D scene. It requires a camera provider as a source of cameras as well as a camera projector to show the current images projected on the ground plane. The px_per_m argument can be used to scale the camera frustums. CameraProjector \u00b6 CameraProjector ( camera_provider : CameraProvider ) -> None The camera projector computes a grid of projected image points on the ground plane. It is mainly used for visualization purposes. CameraProvider \u00b6 Bases: abc . ABC A camera provider holds a dictionary of cameras and manages additions and removals. The camera dictionary should not be modified directly but by using the camera provider's methods. This way respective events are emitted and consistency can be taken care of. CAMERA_ADDED class-attribute \u00b6 CAMERA_ADDED = Event () a new camera has been added (argument: camera) CAMERA_REMOVED class-attribute \u00b6 CAMERA_REMOVED = Event () a camera has been removed (argument: camera id) NEW_IMAGE class-attribute \u00b6 NEW_IMAGE = Event () an new image is available (argument: image) CameraServer \u00b6 CameraServer ( camera_module : CameraProvider ) -> None A camera server creates an HTTP route to access images of a given camera provider. Detector \u00b6 Bases: abc . ABC A detector allows detecting objects in images. It also holds an upload queue for sending images with uncertain results to an active learning infrastructure like the Zauberzeug Learning Loop . NEW_DETECTIONS class-attribute \u00b6 NEW_DETECTIONS = Event () detection on an image is completed (argument: image) DetectorHardware \u00b6 DetectorHardware ( * , port : int = 8004 ) -> None Bases: Detector This detector communicates with a YOLO detector via Socket.IO. It automatically connects and reconnects, submits and receives detections and sends images that should be uploaded to the Zauberzeug Learning Loop . detect async \u00b6 detect ( image : Image , autoupload : Autoupload = Autoupload . FILTERED , tags : list [ str ] = []) -> None Runs detections on the image. Afterwards the image.detections property is filled. DetectorSimulation \u00b6 DetectorSimulation ( camera_provider : CameraProvider , * , noise : float = 1.0 ) -> None Bases: Detector This detector simulates object detection. It requires a camera provider in order to check visibility using the cameras' calibrations. Individual camera IDs can be added to a set of blocked_cameras to simulate occlusions during pytests. A list of simulated_objects can be filled to define what can be detected. An optional noise parameter controls the spatial accuracy in pixels. MultiCameraProvider \u00b6 MultiCameraProvider ( * camera_providers : CameraProvider ) -> None Bases: CameraProvider A multi-camera provider combines multiple camera providers into one. This is useful if another module requires a single camera provider but the robot has multiple camera sources like USB and WiFi cameras. UsbCameraProviderHardware \u00b6 UsbCameraProviderHardware () -> None Bases: CameraProvider This module collects and provides real USB cameras. Camera devices are discovered through video4linux (v4l) and accessed with openCV. Therefore the program v4l2ctl and openCV (including python bindings) must be available. UsbCameraProviderSimulation \u00b6 UsbCameraProviderSimulation () -> None Bases: CameraProvider This module collects and simulates USB cameras and generates synthetic images. In the current implementation the images only contain the camera ID and the current time.","title":"vision"},{"location":"reference/rosys/vision/#rosys.vision.CameraObjects","text":"CameraObjects ( camera_provider : CameraProvider , camera_projector : CameraProjector , * , px_per_m : float = 10000 ) -> None Bases: Group This module provides a UI element for displaying cameras in a 3D scene. It requires a camera provider as a source of cameras as well as a camera projector to show the current images projected on the ground plane. The px_per_m argument can be used to scale the camera frustums.","title":"CameraObjects"},{"location":"reference/rosys/vision/#rosys.vision.CameraProjector","text":"CameraProjector ( camera_provider : CameraProvider ) -> None The camera projector computes a grid of projected image points on the ground plane. It is mainly used for visualization purposes.","title":"CameraProjector"},{"location":"reference/rosys/vision/#rosys.vision.CameraProvider","text":"Bases: abc . ABC A camera provider holds a dictionary of cameras and manages additions and removals. The camera dictionary should not be modified directly but by using the camera provider's methods. This way respective events are emitted and consistency can be taken care of.","title":"CameraProvider"},{"location":"reference/rosys/vision/#rosys.vision.camera_provider.CameraProvider.CAMERA_ADDED","text":"CAMERA_ADDED = Event () a new camera has been added (argument: camera)","title":"CAMERA_ADDED"},{"location":"reference/rosys/vision/#rosys.vision.camera_provider.CameraProvider.CAMERA_REMOVED","text":"CAMERA_REMOVED = Event () a camera has been removed (argument: camera id)","title":"CAMERA_REMOVED"},{"location":"reference/rosys/vision/#rosys.vision.camera_provider.CameraProvider.NEW_IMAGE","text":"NEW_IMAGE = Event () an new image is available (argument: image)","title":"NEW_IMAGE"},{"location":"reference/rosys/vision/#rosys.vision.CameraServer","text":"CameraServer ( camera_module : CameraProvider ) -> None A camera server creates an HTTP route to access images of a given camera provider.","title":"CameraServer"},{"location":"reference/rosys/vision/#rosys.vision.Detector","text":"Bases: abc . ABC A detector allows detecting objects in images. It also holds an upload queue for sending images with uncertain results to an active learning infrastructure like the Zauberzeug Learning Loop .","title":"Detector"},{"location":"reference/rosys/vision/#rosys.vision.detector.Detector.NEW_DETECTIONS","text":"NEW_DETECTIONS = Event () detection on an image is completed (argument: image)","title":"NEW_DETECTIONS"},{"location":"reference/rosys/vision/#rosys.vision.DetectorHardware","text":"DetectorHardware ( * , port : int = 8004 ) -> None Bases: Detector This detector communicates with a YOLO detector via Socket.IO. It automatically connects and reconnects, submits and receives detections and sends images that should be uploaded to the Zauberzeug Learning Loop .","title":"DetectorHardware"},{"location":"reference/rosys/vision/#rosys.vision.detector_hardware.DetectorHardware.detect","text":"detect ( image : Image , autoupload : Autoupload = Autoupload . FILTERED , tags : list [ str ] = []) -> None Runs detections on the image. Afterwards the image.detections property is filled.","title":"detect()"},{"location":"reference/rosys/vision/#rosys.vision.DetectorSimulation","text":"DetectorSimulation ( camera_provider : CameraProvider , * , noise : float = 1.0 ) -> None Bases: Detector This detector simulates object detection. It requires a camera provider in order to check visibility using the cameras' calibrations. Individual camera IDs can be added to a set of blocked_cameras to simulate occlusions during pytests. A list of simulated_objects can be filled to define what can be detected. An optional noise parameter controls the spatial accuracy in pixels.","title":"DetectorSimulation"},{"location":"reference/rosys/vision/#rosys.vision.MultiCameraProvider","text":"MultiCameraProvider ( * camera_providers : CameraProvider ) -> None Bases: CameraProvider A multi-camera provider combines multiple camera providers into one. This is useful if another module requires a single camera provider but the robot has multiple camera sources like USB and WiFi cameras.","title":"MultiCameraProvider"},{"location":"reference/rosys/vision/#rosys.vision.UsbCameraProviderHardware","text":"UsbCameraProviderHardware () -> None Bases: CameraProvider This module collects and provides real USB cameras. Camera devices are discovered through video4linux (v4l) and accessed with openCV. Therefore the program v4l2ctl and openCV (including python bindings) must be available.","title":"UsbCameraProviderHardware"},{"location":"reference/rosys/vision/#rosys.vision.UsbCameraProviderSimulation","text":"UsbCameraProviderSimulation () -> None Bases: CameraProvider This module collects and simulates USB cameras and generates synthetic images. In the current implementation the images only contain the camera ID and the current time.","title":"UsbCameraProviderSimulation"}]}